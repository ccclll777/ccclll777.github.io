<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Tensorflow中Keras 高层接口 | ccclll777's blogs</title><meta name="keywords" content="深度学习框架,python,tensorflow"><meta name="author" content="ccclll777"><meta name="copyright" content="ccclll777"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="TensorFlow中的子模块 tf.keras中快速构建神经网络的高层接口">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow中Keras 高层接口">
<meta property="og:url" content="http://yoursite.com/2020/12/04/Tensorflow/Tensorflow-Keras-high-level-interface/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="TensorFlow中的子模块 tf.keras中快速构建神经网络的高层接口">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png">
<meta property="article:published_time" content="2020-12-04T05:41:13.000Z">
<meta property="article:modified_time" content="2021-10-17T01:36:25.742Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="python">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png"><link rel="shortcut icon" href="/images/avatar.png"><link rel="canonical" href="http://yoursite.com/2020/12/04/Tensorflow/Tensorflow-Keras-high-level-interface/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Tensorflow中Keras 高层接口',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-10-17 09:36:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ccclll777's blogs</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Tensorflow中Keras 高层接口</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-12-04T05:41:13.000Z" title="发表于 2020-12-04 13:41:13">2020-12-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-17T01:36:25.742Z" title="更新于 2021-10-17 09:36:25">2021-10-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>15分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Tensorflow中Keras 高层接口"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>TensorFlow中的子模块 tf.keras中快速构建神经网络的高层接口</p>
<span id="more"></span>
<h1 id="常见功能模块"><a class="markdownIt-Anchor" href="#常见功能模块"></a> 常见功能模块</h1>
<p><strong>Keras 提供了一系列高层的神经网络相关类和函数，如经典数据集加载函数、网络层类、模型容器、损失函数类、优化器类、经典模型类。</strong></p>
<h2 id="常见网络层类"><a class="markdownIt-Anchor" href="#常见网络层类"></a> 常见网络层类</h2>
<ul>
<li>对于常见的神经网络层，可以使用张量方式的底层接口函数来实现，这些接口函数一 般在 tf.nn 模块中</li>
<li>对于常见的网络层，我们一般直接使用层方式来完成模型的 搭建，在 tf.keras.layers 命名空间中提供了大量常见网络层的类，如全连接层、激活函数层、池化层、卷积层、循环神经网络层。</li>
<li>例如以 Softmax 层为例，它既可以使用 tf.nn.softmax 函数在前向传播逻辑中完成 Softmax 运算，也可以通过 layers.Softmax(axis)类搭建 Softmax 网络层</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-comment"># 导入keras模型，不能使用import keras，它导入的是标准的Keras库 </span><br><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras<br><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers <span class="hljs-comment"># 导入常见网络层类</span><br><span class="hljs-comment">#然后创建 Softmax 层，并调用__call__方法完成前向计算</span><br>x = tf.constant([<span class="hljs-number">2.</span>,<span class="hljs-number">1.</span>,<span class="hljs-number">0.1</span>]) <span class="hljs-comment"># 创建输入张量 layer = layers.Softmax(axis=-1) # 创建Softmax层</span><br>out = layer(x) <span class="hljs-comment"># 调用softmax前向计算，输出为out</span><br><span class="hljs-comment">#可以直接通过 tf.nn.softmax()函数完成计算</span><br>out = tf.nn.softmax(x) <span class="hljs-comment"># 调用 softmax 函数完成前向计算</span><br></code></pre></td></tr></table></figure>
<h2 id="网络容器"><a class="markdownIt-Anchor" href="#网络容器"></a> 网络容器</h2>
<p>为了避免当网络层数较深时，手动调用每一层的类实例完成前向传播运算的麻烦。</p>
<ul>
<li>可以通过 Keras 提供的网络容器 Sequential 将多个<br />
网络层封装成一个大网络模型，只需要调用网络模型的实例一次即可完成数据从第一层到 最末层的顺序传播运算。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入Sequential容器</span><br><span class="hljs-keyword">from</span> tensorflow.keras <br><span class="hljs-keyword">import</span> layers, Sequential <br>network = Sequential([ <span class="hljs-comment"># 封装为一个网络</span><br>layers.Dense(<span class="hljs-number">3</span>, activation=<span class="hljs-literal">None</span>), <span class="hljs-comment"># 全连接层，此处不使用激活函数 layers.ReLU(),#激活函数层</span><br>layers.Dense(<span class="hljs-number">2</span>, activation=<span class="hljs-literal">None</span>), <span class="hljs-comment"># 全连接层，此处不使用激活函数</span><br>layers.ReLU() <span class="hljs-comment">#激活函数层 </span><br>])<br>x = tf.random.normal([<span class="hljs-number">4</span>,<span class="hljs-number">3</span>])<br>out = network(x) <span class="hljs-comment"># 输入从第一层开始，逐层传播至输出层，并返回输出层的输出</span><br></code></pre></td></tr></table></figure>
<ul>
<li>Sequential 容器也可以通过 add()方法继续追加新的网络层，实现动态创建网络的功能</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">layers_num = <span class="hljs-number">2</span> <span class="hljs-comment"># 堆叠2次</span><br>network = Sequential([]) <span class="hljs-comment"># 先创建空的网络容器 for _ in range(layers_num):</span><br>network.add(layers.Dense(<span class="hljs-number">3</span>)) <span class="hljs-comment"># 添加全连接层</span><br>network.add(layers.ReLU())<span class="hljs-comment"># 添加激活函数层 </span><br><span class="hljs-comment">#通过调用类的 build 方法并指定 输入大小，即可自动创建所有层的内部张量</span><br>network.build(input_shape=(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)) <span class="hljs-comment"># 创建网络参数  </span><br>network.summary()<br></code></pre></td></tr></table></figure>
<ul>
<li>当我们通过 Sequential 容量封装多个网络层时，每层的参数列表将会自动并入 Sequential 容器的参数列表中，不需要人为合并网络参数列表。Sequential 对象的 trainable_variables 和 variables 包含了所有层的待优化张量列表 和全部张量列表。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印网络的待优化参数名与shape</span><br><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> network.trainable_variables: <br>	print(p.name, p.shape) <span class="hljs-comment"># 参数名和形状</span><br></code></pre></td></tr></table></figure>
<h1 id="模型装配-训练与测试"><a class="markdownIt-Anchor" href="#模型装配-训练与测试"></a> 模型装配、训练与测试</h1>
<h2 id="模型装配"><a class="markdownIt-Anchor" href="#模型装配"></a> 模型装配</h2>
<ul>
<li>在 Keras 中，有 2 个比较特殊的类:keras.Model 和 keras.layers.Layer 类。</li>
<li>Layer 类是网络层的母类，定义了网络层的一些常见功能，如添加权值、管理权值列表等。</li>
<li>Model 类是网络的母类，除了具有 Layer 类的功能，还添加了保存模型、加载模型、训练与测试模型等便捷功能。</li>
<li>Sequential也是 Model 的子类，因此具有 Model 类的所有功能。</li>
</ul>
<p>-创建5层的全连接网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建5层的全连接网络</span><br>network = Sequential([layers.Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>                 layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>                 layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>layers.Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>                 layers.Dense(<span class="hljs-number">10</span>)])<br>network.build(input_shape=(<span class="hljs-number">4</span>, <span class="hljs-number">28</span>*<span class="hljs-number">28</span>))<br>network.summary()<br></code></pre></td></tr></table></figure>
<ul>
<li>Keras 中提供了 compile()和 fit()函数，创建网络后，正常的流程是循环迭代数据集多个 Epoch，每次按批产生训练数据、前向计 算，然后通过损失函数计算误差值，并反向传播自动计算梯度、更新网络参数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入优化器，损失函数模块</span><br><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> optimizers,losses <br><span class="hljs-comment"># 模型装配</span><br><span class="hljs-comment"># 采用Adam优化器，学习率为0.01;采用交叉熵损失函数，包含Softmax </span><br>network.<span class="hljs-built_in">compile</span>(optimizer=optimizers.Adam(lr=<span class="hljs-number">0.01</span>),<br>loss=losses.CategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),<br>metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>] <span class="hljs-comment"># 设置测量指标为准确率 )</span><br></code></pre></td></tr></table></figure>
<h2 id="模型训练"><a class="markdownIt-Anchor" href="#模型训练"></a> 模型训练</h2>
<ul>
<li>模型装配完成后，即可通过 fit()函数送入待训练的数据集和验证用的数据集</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 指定训练集为train_db，验证集为val_db,训练5个epochs，每2个epoch验证一次 # 返回训练轨迹信息保存在history对象中</span><br>history = network.fit(train_db, epochs=<span class="hljs-number">5</span>, validation_data=val_db,<br>validation_freq=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>
<p>train_db 为 tf.data.Dataset 对象，也可以传入 Numpy Array 类型的数据;epochs 参数指 定训练迭代的 Epoch 数量;validation_data 参数指定用于验证(测试)的数据集和验证的频率 validation_freq</p>
<ul>
<li>fit 函数会返回训练过程的数据记录 history，其中 history.history 为字典对象，包含了训练过程中的 loss、测量指标等记录项</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">history.history <span class="hljs-comment"># 打印训练记录</span><br></code></pre></td></tr></table></figure>
<p>fit()函数的运行代表了网络的训练过程，因此会消耗相当的训练时间，并在训练结束 后才返回，训练中产生的历史数据可以通过返回值对象取得。</p>
<h2 id="模型测试"><a class="markdownIt-Anchor" href="#模型测试"></a> 模型测试</h2>
<ul>
<li>通过 Model.predict(x)方法即可完成模型的预测</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载一个 batch 的测试数据</span><br>x,y = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(db_test))<br>print(<span class="hljs-string">&#x27;predict x:&#x27;</span>, x.shape) <span class="hljs-comment"># 打印当前batch的形状</span><br>out = network.predict(x) <span class="hljs-comment"># 模型预测，预测结果保存在out中</span><br>print(out)<br></code></pre></td></tr></table></figure>
<ul>
<li>如果只是简单的测试模型的性能，可以通过 Model.evaluate(db)循环测试完 db 数据集 上所有样本，并打印出性能指标</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">network.evaluate(db_test) <span class="hljs-comment"># 模型测试，测试在db_test上的性能表现</span><br></code></pre></td></tr></table></figure>
<h1 id="模型保存与加载"><a class="markdownIt-Anchor" href="#模型保存与加载"></a> 模型保存与加载</h1>
<p>模型训练完成后，需要将模型保存到文件系统上，从而方便后续的模型测试与部署工作。</p>
<h2 id="张量方式"><a class="markdownIt-Anchor" href="#张量方式"></a> 张量方式</h2>
<ul>
<li>通过调用 Model.save_weights(path)方法即可将当前的 网络参数保存到 path 文件上</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">network.save_weights(<span class="hljs-string">&#x27;weights.ckpt&#x27;</span>) <span class="hljs-comment"># 保存模型的所有张量数据</span><br></code></pre></td></tr></table></figure>
<p>将 network 模型保存到 weights.ckpt 文件上。</p>
<ul>
<li>在需要的时候，先创建好网络对象， 然后调用网络对象的 load_weights(path)方法即可将指定的模型文件中保存的张量数值写入 到当前网络参数中去</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 保存模型参数到文件上 </span><br>network.save_weights(<span class="hljs-string">&#x27;weights.ckpt&#x27;</span>) <br>print(<span class="hljs-string">&#x27;saved weights.&#x27;</span>)<br><span class="hljs-keyword">del</span> network <span class="hljs-comment"># 删除网络对象</span><br><span class="hljs-comment"># 重新创建相同的网络结构</span><br>network = Sequential([layers.Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>layers.Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>),<br>layers.Dense(<span class="hljs-number">10</span>)])<br>network.<span class="hljs-built_in">compile</span>(optimizer=optimizers.Adam(lr=<span class="hljs-number">0.01</span>),<br>        loss=tf.losses.CategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),<br>        metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>]<br>   )<br><span class="hljs-comment"># 从参数文件中读取数据并写入当前网络 </span><br>network.load_weights(<span class="hljs-string">&#x27;weights.ckpt&#x27;</span>)<br>print(<span class="hljs-string">&#x27;loaded weights!&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h2 id="网络方式"><a class="markdownIt-Anchor" href="#网络方式"></a> 网络方式</h2>
<ul>
<li>通过 Model.save(path)函数可以将模型的结构以及模型的参数保存到 path 文件上，在不 需要网络源文件的条件下，通过 keras.models.load_model(path)即可恢复网络结构和网络参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 保存模型结构与模型参数到文件</span><br>network.save(<span class="hljs-string">&#x27;model.h5&#x27;</span>)<br>print(<span class="hljs-string">&#x27;saved total model.&#x27;</span>)<br><span class="hljs-keyword">del</span> network <span class="hljs-comment"># 删除网络对象</span><br></code></pre></td></tr></table></figure>
<ul>
<li>通过 model.h5 文件即可恢复出网络的结构和状态，不需要提前创建网络对象</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"> <span class="hljs-comment"># 从文件恢复网络结构与网络参数</span><br>network = keras.models.load_model(<span class="hljs-string">&#x27;model.h5&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>model.h5 文件除了保存了模型参数外，还应保存了网络结构信息，不需要提前 创建模型即可直接从文件中恢复出网络 network 对象</p>
<h2 id="savedmodel-方式"><a class="markdownIt-Anchor" href="#savedmodel-方式"></a> SavedModel 方式</h2>
<ul>
<li>通过 tf.saved_model.save (network, path)即可将模型以 SavedModel 方式保存到 path 目录中</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 保存模型结构与模型参数到文件</span><br>tf.saved_model.save(network, <span class="hljs-string">&#x27;model-savedmodel&#x27;</span>)<br>print(<span class="hljs-string">&#x27;saving savedmodel.&#x27;</span>)<br><span class="hljs-keyword">del</span> network <span class="hljs-comment"># 删除网络对象</span><br></code></pre></td></tr></table></figure>
<ul>
<li>通过 tf.saved_model.load 函数即可恢复出模型对象，我们在恢复出模型实例后，完成测试准确率的计算</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">print(<span class="hljs-string">&#x27;load savedmodel from file.&#x27;</span>)<br><span class="hljs-comment"># 从文件恢复网络结构与网络参数</span><br>network = tf.saved_model.load(<span class="hljs-string">&#x27;model-savedmodel&#x27;</span>) <span class="hljs-comment"># 准确率计量器</span><br>acc_meter = metrics.CategoricalAccuracy()<br><span class="hljs-keyword">for</span> x,y <span class="hljs-keyword">in</span> ds_val: <span class="hljs-comment"># 遍历测试集</span><br>pred = network(x) <span class="hljs-comment"># 前向计算</span><br>acc_meter.update_state(y_true=y, y_pred=pred) <span class="hljs-comment"># 更新准确率统计 # 打印准确率</span><br>print(<span class="hljs-string">&quot;Test Accuracy:%f&quot;</span> % acc_meter.result())<br></code></pre></td></tr></table></figure>
<h1 id="自定义网络"><a class="markdownIt-Anchor" href="#自定义网络"></a> 自定义网络</h1>
<p>对于需要创建自定义逻辑的网络层，可以通过自定义类来实现。在创建自定义网络层 类时，需要继承自 layers.Layer 基类;创建自定义的网络类时，需要继承自 keras.Model 基 类，这样建立的自定义类才能够方便的利用 Layer/Model 基类提供的参数管理等功能，同 时也能够与其他的标准网络层类交互使用。</p>
<h2 id="自定义网络层"><a class="markdownIt-Anchor" href="#自定义网络层"></a> 自定义网络层</h2>
<ul>
<li>对于自定义的网络层，至少需要实现初始化__init__方法和前向传播逻辑 call 方法。</li>
<li>首先创建类，并继承自 Layer 基类。创建初始化方法，并调用母类的初始化函数，由 于是全连接层，因此需要设置两个参数:输入特征的长度 inp_dim 和输出特征的长度 outp_dim，并通过 self.add_variable(name, shape)创建 shape 大小，名字为 name 的张量𝑾， 并设置为需要优化。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyDense</span>(<span class="hljs-params">layers.Layer</span>):</span> <span class="hljs-comment"># 自定义网络层</span><br>	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, inp_dim, outp_dim</span>):</span><br>		<span class="hljs-built_in">super</span>(MyDense, self).__init__()<br>		<span class="hljs-comment"># 创建权值张量并添加到类管理列表中，设置为需要优化</span><br>		self.kernel = self.add_variable(<span class="hljs-string">&#x27;w&#x27;</span>, [inp_dim, outp_dim],<br>		trainable=<span class="hljs-literal">True</span>)<br><br><br>net = MyDense(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>) <span class="hljs-comment"># 创建输入为 4，输出为 3 节点的自定义层 </span><br>net.variables,net.trainable_variables <span class="hljs-comment"># 查看自定义层的参数列表</span><br></code></pre></td></tr></table></figure>
<ul>
<li>通过修改为 self.kernel = self.add_variable(‘w’, [inp_dim, outp_dim], trainable=False)，我们可以设置𝑾张量不需要被优化</li>
<li>通过 tf.Variable 创建的类成员也会自动加入类参数列表</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 通过 tf.Variable 创建的类成员也会自动加入类参数列表</span><br>self.kernel = tf.Variable(tf.random.normal([inp_dim, outp_dim]),<br>                trainable=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure>
<ul>
<li>自定义类的前向运算逻辑，对于这个例 子，只需要完成𝑶 = 𝑿@𝑾矩阵运算，并通过固定的 ReLU 激活函数即可</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, inputs, training=<span class="hljs-literal">None</span></span>):</span> <br>	<span class="hljs-comment"># 实现自定义类的前向计算逻辑</span><br>	<span class="hljs-comment"># X@W</span><br>	out = inputs @ self.kernel<br>	<span class="hljs-comment"># 执行激活函数运算</span><br>	out = tf.nn.relu(out) <span class="hljs-keyword">return</span> out<br> <br></code></pre></td></tr></table></figure>
<ul>
<li>自定义类的前向运算逻辑实现在 call(inputs, training=None)函数中，其中 inputs 代表输入，由用户在调用时传入;training 参数用于指定模型的状态:training 为 True 时执 行训练模式，training 为 False 时执行测试模式，默认参数为 None，即测试模式。由于全连 接层的训练模式和测试模式逻辑一致，此处不需要额外处理。对于部份测试模式和训练模 式不一致的网络层，需要根据 training 参数来设计需要执行的逻辑。</li>
</ul>
<h2 id="自定义网络-2"><a class="markdownIt-Anchor" href="#自定义网络-2"></a> 自定义网络</h2>
<ul>
<li>自定义网络类可以和其他标准类一样，通过 Sequential 容器方便地封装成一个网络模型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">network = Sequential([MyDense(<span class="hljs-number">784</span>, <span class="hljs-number">256</span>), <span class="hljs-comment"># 使用自定义的层 MyDense(256, 128),</span><br>                 MyDense(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>),<br>                 MyDense(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>),<br>                 MyDense(<span class="hljs-number">32</span>, <span class="hljs-number">10</span>)])<br>network.build(input_shape=(<span class="hljs-literal">None</span>, <span class="hljs-number">28</span>*<span class="hljs-number">28</span>))<br>network.summary()<br></code></pre></td></tr></table></figure>
<ul>
<li>创建自定义网络类，首先 创建类，并继承自 Model 基类，分别创建对应的网络层对象和前向运算逻辑</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModel</span>(<span class="hljs-params">keras.Model</span>):</span><br>	<span class="hljs-comment"># 自定义网络类，继承自 Model 基类 </span><br>	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>		<span class="hljs-built_in">super</span>(MyModel, self).__init__() <span class="hljs-comment"># 完成网络内需要的网络层的创建工作 </span><br>		self.fc1 = MyDense(<span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">256</span>) <br>		self.fc2 = MyDense(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>) <br>		self.fc3 = MyDense(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>)<br>		self.fc4 = MyDense(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>)<br>		self.fc5 = MyDense(<span class="hljs-number">32</span>, <span class="hljs-number">10</span>)<br>	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, inputs, training=<span class="hljs-literal">None</span></span>):</span> <span class="hljs-comment"># 自定义前向运算逻辑</span><br>		x = self.fc1(inputs)<br>		x = self.fc2(x)<br>		x = self.fc3(x)<br>		x = self.fc4(x)<br>		x = self.fc5(x)<br>		<span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<h1 id="常用模型"><a class="markdownIt-Anchor" href="#常用模型"></a> 常用模型</h1>
<p>对于常用的网络模型，如 ResNet、VGG 等，不需要手动创建网络，可以直接从 keras.applications 子模块中通过一行代码即可创建并使用这些经典模型，同时还可以通过设 置 weights 参数加载预训练的网络参数。</p>
<h2 id="加载模型"><a class="markdownIt-Anchor" href="#加载模型"></a> 加载模型</h2>
<ul>
<li>以 ResNet50 网络模型为例，一般将 ResNet50 去除最后一层后的网络作为新任务的特 征提取子网络，即利用在 ImageNet 数据集上预训练好的网络参数初始化，并根据自定义任 务的类别追加一个对应数据类别数的全连接分类层或子网络，从而可以在预训练网络的基 础上快速、高效地学习新任务。</li>
<li>首先利用 Keras 模型乐园加载 ImageNet 预训练好的 ResNet50 网络</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载 ImageNet 预训练网络模型，并去掉最后一层</span><br>resnet = keras.applications.ResNet50(weights=<span class="hljs-string">&#x27;imagenet&#x27;</span>,include_top=<span class="hljs-literal">False</span>) resnet.summary()<br><span class="hljs-comment"># 测试网络的输出</span><br>x = tf.random.normal([<span class="hljs-number">4</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>,<span class="hljs-number">3</span>])<br>out = resnet(x) <span class="hljs-comment"># 获得子网络的输出</span><br>out.shape<br></code></pre></td></tr></table></figure>
<ul>
<li>新建一个池化层(这里的池化层暂时 可以理解为高、宽维度下采样的功能)，将特征从[𝑏, 7,7,2048]降维到[𝑏, 2048]</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 新建池化层</span><br>global_average_layer = layers.GlobalAveragePooling2D() <span class="hljs-comment"># 利用上一层的输出作为本层的输入，测试其输出</span><br>x = tf.random.normal([<span class="hljs-number">4</span>,<span class="hljs-number">7</span>,<span class="hljs-number">7</span>,<span class="hljs-number">2048</span>])<br><span class="hljs-comment"># 池化层降维，形状由[4,7,7,2048]变为[4,1,1,2048],删减维度后变为[4,2048] </span><br>out = global_average_layer(x)<br>print(out.shape)<br></code></pre></td></tr></table></figure>
<ul>
<li>新建一个全连接层，并设置输出节点数为 100</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 新建全连接层</span><br>fc = layers.Dense(<span class="hljs-number">100</span>)<br><span class="hljs-comment"># 利用上一层的输出[4,2048]作为本层的输入，测试其输出</span><br>x = tf.random.normal([<span class="hljs-number">4</span>,<span class="hljs-number">2048</span>])<br>out = fc(x) <span class="hljs-comment"># 输出层的输出为样本属于 100 类别的概率分布</span><br>print(out.shape)<br></code></pre></td></tr></table></figure>
<h1 id="测量工具"><a class="markdownIt-Anchor" href="#测量工具"></a> 测量工具</h1>
<p>在网络的训练过程中，经常需要统计准确率、召回率等测量指标，除了可以通过手动 计算的方式获取这些统计数据外，Keras 提供了一些常用的测量工具，位于 keras.metrics 模 块中，专门用于统计训练过程中常用的指标数据。</p>
<h2 id="新建测量器"><a class="markdownIt-Anchor" href="#新建测量器"></a> 新建测量器</h2>
<p>在 keras.metrics 模块中，提供了较多的常用测量器类，如统计平均值的 Mean 类，统 计准确率的 Accuracy 类，统计余弦相似度的 CosineSimilarity 类等。下面我们以统计误差 值为例。在前向运算时，我们会得到每一个 Batch 的平均误差，但是我们希望统计每个 Step 的平均误差，因此选择使用 Mean 测量器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 新建平均测量器，适合 Loss 数据 </span><br>loss_meter = metrics.Mean()<br></code></pre></td></tr></table></figure>
<h2 id="写入数据"><a class="markdownIt-Anchor" href="#写入数据"></a> 写入数据</h2>
<p>通过测量器的 update_state 函数可以写入新的数据，测量器会根据自身逻辑记录并处理采样数据。例如，在每个 Step 结束时采集一次 loss 值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 记录采样的数据，通过 float()函数将张量转换为普通数值</span><br>loss_meter.update_state(<span class="hljs-built_in">float</span>(loss))<br></code></pre></td></tr></table></figure>
<h2 id="读取统计信息"><a class="markdownIt-Anchor" href="#读取统计信息"></a> 读取统计信息</h2>
<p>在采样多次数据后，可以选择在需要的地方调用测量器的 result()函数，来获取统计值。例如，间隔性统计 loss 均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"> <span class="hljs-comment"># 打印统计期间的平均 loss</span><br>print(step, <span class="hljs-string">&#x27;loss:&#x27;</span>, loss_meter.result())<br></code></pre></td></tr></table></figure>
<h2 id="清除状态"><a class="markdownIt-Anchor" href="#清除状态"></a> 清除状态</h2>
<p>由于测量器会统计所有历史记录的数据，因此在启动新一轮统计时，有必要清除历史 状态。通过 reset_states()即可实现清除状态功能。例如，在每次读取完平均误差后，清零统 计信息，以便下一轮统计的开始。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>	<span class="hljs-comment"># 打印统计的平均 loss</span><br>	print(step, <span class="hljs-string">&#x27;loss:&#x27;</span>, loss_meter.result())<br>	loss_meter.reset_states() <span class="hljs-comment"># 打印完后，清零测量器</span><br></code></pre></td></tr></table></figure>
<h2 id="准确率统计实战"><a class="markdownIt-Anchor" href="#准确率统计实战"></a> 准确率统计实战</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">acc_meter = metrics.Accuracy() <span class="hljs-comment"># 创建准确率测量器</span><br><span class="hljs-comment"># [b, 784] =&gt; [b, 10]，网络输出值</span><br><span class="hljs-comment">#Accuracy 类的 update_state 函数的参数为预测值和真实值，而不是当前 Batch 的准确率</span><br>out = network(x)<br><span class="hljs-comment"># [b, 10] =&gt; [b]，经过 argmax 后计算预测值 </span><br>pred = tf.argmax(out, axis=<span class="hljs-number">1</span>)<br>pred = tf.cast(pred, dtype=tf.int32) <span class="hljs-comment"># 根据预测值与真实值写入测量器</span><br>acc_meter.update_state(y, pred)<br><span class="hljs-comment"># 读取统计结果</span><br>print(step, <span class="hljs-string">&#x27;Evaluate Acc:&#x27;</span>, acc_meter.result().numpy()) <br>acc_meter.reset_states() <span class="hljs-comment"># 清零测量器</span><br></code></pre></td></tr></table></figure>
<h1 id="可视化"><a class="markdownIt-Anchor" href="#可视化"></a> 可视化</h1>
<p>TensorFlow 提供了一个专门的可视 化工具，叫做 TensorBoard，它通过 TensorFlow 将监控数据写入到文件系统，并利用 Web 后端监控对应的文件目录，从而可以允许用户从远程查看网络的监控数据。</p>
<h2 id=""><a class="markdownIt-Anchor" href="#"></a> </h2>
<ul>
<li>通过 tf.summary.create_file_writer 创建监控对象类实例，并指定监控数据的写入目录</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"> <span class="hljs-comment"># 创建监控类，监控数据将写入 log_dir 目录</span><br>summary_writer = tf.summary.create_file_writer(log_dir)<br></code></pre></td></tr></table></figure>
<ul>
<li>通过 tf.summary.scalar 函数记录监控数据，并指定时 间戳 step 参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> summary_writer.as_default(): <span class="hljs-comment"># 写入环境</span><br><span class="hljs-comment"># 当前时间戳 step 上的数据为 loss，写入到名为 train-loss 数据库中</span><br>   tf.summary.scalar(<span class="hljs-string">&#x27;train-loss&#x27;</span>, <span class="hljs-built_in">float</span>(loss), step=step)<br></code></pre></td></tr></table></figure>
<ul>
<li>通过 tf.summary.image 函数写入监控图片数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> summary_writer.as_default():<span class="hljs-comment"># 写入环境</span><br><span class="hljs-comment"># 写入测试准确率</span><br>	tf.summary.scalar(<span class="hljs-string">&#x27;test-acc&#x27;</span>, <span class="hljs-built_in">float</span>(total_correct/total),<br>	step=step)<br><span class="hljs-comment"># 可视化测试用的图片，设置最多可视化 9 张图片</span><br>	tf.summary.image(<span class="hljs-string">&quot;val-onebyone-images:&quot;</span>, val_images,<br>max_outputs=<span class="hljs-number">9</span>, step=step)<br></code></pre></td></tr></table></figure></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/tensorflow/">tensorflow</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/12/04/Tensorflow/Tensorflow-and-Convolutional-Neural-Network/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Tensorflow与卷积神经网络</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/04/Tensorflow/Tensorflow-and-Neural-Networks/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Tensorflow构建简单神经网络</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/12/08/Tensorflow/Tensorflow-and-Reinforcement-learning/" title="Tensorflow与强化学习"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-08</div><div class="title">Tensorflow与强化学习</div></div></a></div><div><a href="/2020/12/06/Tensorflow/Tensorflow-and-Encoder-Decoder/" title="Tensorflow与自编码器"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-06</div><div class="title">Tensorflow与自编码器</div></div></a></div><div><a href="/2020/12/06/Tensorflow/Tensorflow-and-Recurrent-neural-network/" title="Tensorflow与循环神经网络"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-06</div><div class="title">Tensorflow与循环神经网络</div></div></a></div><div><a href="/2020/12/04/Tensorflow/Tensorflow-and-Convolutional-Neural-Network/" title="Tensorflow与卷积神经网络"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-04</div><div class="title">Tensorflow与卷积神经网络</div></div></a></div><div><a href="/2020/12/04/Tensorflow/Tensorflow-and-Neural-Networks/" title="Tensorflow构建简单神经网络"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-04</div><div class="title">Tensorflow构建简单神经网络</div></div></a></div><div><a href="/2020/12/03/Tensorflow/Tensorflow-advanced-knowledge/" title="Tensorflow2.0进阶知识"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-03</div><div class="title">Tensorflow2.0进阶知识</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ccclll777</div><div class="author-info__description">胸怀猛虎 细嗅蔷薇</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ccclll777"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccclll777" target="_blank" title="fab fa-github"><i class="GitHub"></i></a><a class="social-icon" href="mailto:sdu945860882@gmail.com" target="_blank" title="fa fa-envelope"><i class="E-Mail"></i></a><a class="social-icon" href="https://www.weibo.com/6732062654" target="_blank" title="fab fa-weibo"><i class="Weibo"></i></a><a class="social-icon" href="https://blog.csdn.net/baidu_41871794" target="_blank" title="gratipay"><i class="CSDN"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9D%97"><span class="toc-number">1.</span> <span class="toc-text"> 常见功能模块</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E5%B1%82%E7%B1%BB"><span class="toc-number">1.1.</span> <span class="toc-text"> 常见网络层类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E5%AE%B9%E5%99%A8"><span class="toc-number">1.2.</span> <span class="toc-text"> 网络容器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%A3%85%E9%85%8D-%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95"><span class="toc-number">2.</span> <span class="toc-text"> 模型装配、训练与测试</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%A3%85%E9%85%8D"><span class="toc-number">2.1.</span> <span class="toc-text"> 模型装配</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">2.2.</span> <span class="toc-text"> 模型训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95"><span class="toc-number">2.3.</span> <span class="toc-text"> 模型测试</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">3.</span> <span class="toc-text"> 模型保存与加载</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E6%96%B9%E5%BC%8F"><span class="toc-number">3.1.</span> <span class="toc-text"> 张量方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%96%B9%E5%BC%8F"><span class="toc-number">3.2.</span> <span class="toc-text"> 网络方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#savedmodel-%E6%96%B9%E5%BC%8F"><span class="toc-number">3.3.</span> <span class="toc-text"> SavedModel 方式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C"><span class="toc-number">4.</span> <span class="toc-text"> 自定义网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="toc-number">4.1.</span> <span class="toc-text"> 自定义网络层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C-2"><span class="toc-number">4.2.</span> <span class="toc-text"> 自定义网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.</span> <span class="toc-text"> 常用模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.1.</span> <span class="toc-text"> 加载模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B5%8B%E9%87%8F%E5%B7%A5%E5%85%B7"><span class="toc-number">6.</span> <span class="toc-text"> 测量工具</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B0%E5%BB%BA%E6%B5%8B%E9%87%8F%E5%99%A8"><span class="toc-number">6.1.</span> <span class="toc-text"> 新建测量器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-number">6.2.</span> <span class="toc-text"> 写入数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF"><span class="toc-number">6.3.</span> <span class="toc-text"> 读取统计信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B8%85%E9%99%A4%E7%8A%B6%E6%80%81"><span class="toc-number">6.4.</span> <span class="toc-text"> 清除状态</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87%E7%BB%9F%E8%AE%A1%E5%AE%9E%E6%88%98"><span class="toc-number">6.5.</span> <span class="toc-text"> 准确率统计实战</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">7.</span> <span class="toc-text"> 可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">7.1.</span> <span class="toc-text"> </span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符"><img src="/2021/10/16/Arithmetic-LeetCode/282/show.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Leetcode 282. 给表达式添加运算符"/></a><div class="content"><a class="title" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符">Leetcode 282. 给表达式添加运算符</a><time datetime="2021-10-16T15:35:16.000Z" title="发表于 2021-10-16 23:35:16">2021-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch强化学习算法实现"/></a><div class="content"><a class="title" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现">Pytorch强化学习算法实现</a><time datetime="2020-12-12T02:54:37.000Z" title="发表于 2020-12-12 10:54:37">2020-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyTorch常用工具模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块">PyTorch常用工具模块</a><time datetime="2020-12-09T13:32:23.000Z" title="发表于 2020-12-09 21:32:23">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中神经网络工具箱nn模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块">Pytorch中神经网络工具箱nn模块</a><time datetime="2020-12-09T13:25:38.000Z" title="发表于 2020-12-09 21:25:38">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd"><img src="/2020/12/09/Pytorch/Pytorch-and-Autograd/torch.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中的Autograd"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd">Pytorch中的Autograd</a><time datetime="2020-12-09T13:25:19.000Z" title="发表于 2020-12-09 21:25:19">2020-12-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By ccclll777</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>