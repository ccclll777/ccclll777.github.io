<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Pytorch强化学习算法实现 | ccclll777's blogs</title><meta name="keywords" content="深度学习框架,python,pytorch"><meta name="author" content="ccclll777"><meta name="copyright" content="ccclll777"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="使用Pytorch框架实现了强化学习算法Policy Gradient&#x2F;DQN&#x2F;DDGP">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch强化学习算法实现">
<meta property="og:url" content="http://yoursite.com/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="使用Pytorch框架实现了强化学习算法Policy Gradient&#x2F;DQN&#x2F;DDGP">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/torch.png">
<meta property="article:published_time" content="2020-12-12T02:54:37.000Z">
<meta property="article:modified_time" content="2021-10-17T10:29:22.700Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="python">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/torch.png"><link rel="shortcut icon" href="/images/avatar.png"><link rel="canonical" href="http://yoursite.com/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch强化学习算法实现',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-10-17 18:29:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/top.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ccclll777's blogs</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch强化学习算法实现</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-12-12T02:54:37.000Z" title="发表于 2020-12-12 10:54:37">2020-12-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-17T10:29:22.700Z" title="更新于 2021-10-17 18:29:22">2021-10-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pytorch强化学习算法实现"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>使用Pytorch框架实现了强化学习算法Policy Gradient/DQN/DDGP</p>
<span id="more"></span>
<h1 id="policy-gradient算法实现"><a class="markdownIt-Anchor" href="#policy-gradient算法实现"></a> Policy Gradient算法实现</h1>
<p>Policy Gradient算法的思想在<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">另一篇博客</a>中有介绍了，下面是算法的具体实现。</p>
<h2 id="policy网络"><a class="markdownIt-Anchor" href="#policy网络"></a> Policy网络</h2>
<p>两个线性层，中间使用Relu激活函数连接，最后连接softmax输出每个动作的概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PolicyNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,n_states_num,n_actions_num,hidden_size</span>):</span><br>        <span class="hljs-built_in">super</span>(PolicyNet, self).__init__()<br>        self.data = [] <span class="hljs-comment">#存储轨迹</span><br>        <span class="hljs-comment">#输入为长度为4的向量 输出为向左  向右两个动作</span><br>        self.net = nn.Sequential(<br>            nn.Linear(in_features=n_states_num, out_features=hidden_size, bias=<span class="hljs-literal">False</span>),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size, out_features=n_actions_num, bias=<span class="hljs-literal">False</span>),<br>            nn.Softmax(dim=<span class="hljs-number">1</span>)<br>        )<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, inputs</span>):</span><br>        <span class="hljs-comment"># 状态输入s的shape为向量：[4]</span><br>        x = self.net(inputs)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<h2 id="将状态输入神经网络选择动作"><a class="markdownIt-Anchor" href="#将状态输入神经网络选择动作"></a> 将状态输入神经网络，选择动作</h2>
<ul>
<li>这里给出了两种实现方式，具体思想就是输入环境的状态，传入policy网络，给出每一个动作的概率，我们需要选择出现概率最大的那个动作，以及他出现的概率值。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#将状态传入神经网络 根据概率选择动作</span><br><span class="hljs-function"><span class="hljs-keyword">def</span>  <span class="hljs-title">choose_action</span>(<span class="hljs-params">self,state</span>):</span><br>    <span class="hljs-comment">#将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span><br>    s = torch.Tensor(state).unsqueeze(<span class="hljs-number">0</span>)<br>    prob = self.pi(s)  <span class="hljs-comment"># 动作分布:[1,2]</span><br>    <span class="hljs-comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span><br>    m = torch.distributions.Categorical(prob)  <span class="hljs-comment"># 生成分布</span><br>    action = m.sample()<br>    <span class="hljs-keyword">return</span> action.item() , m.log_prob(action)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">choose_action2</span>(<span class="hljs-params">self, state</span>):</span><br>    <span class="hljs-comment"># 将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span><br>    s = torch.Tensor(state).unsqueeze(<span class="hljs-number">0</span>)<br>    prob = self.pi(s)  <span class="hljs-comment"># 动作分布:[1,2]</span><br>    <span class="hljs-comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span><br>    action =np.random.choice(<span class="hljs-built_in">range</span>(prob.shape[<span class="hljs-number">1</span>]),size=<span class="hljs-number">1</span>,p = prob.view(-<span class="hljs-number">1</span>).detach().numpy())[<span class="hljs-number">0</span>]<br>    action = <span class="hljs-built_in">int</span>(action)<br>    <span class="hljs-comment">#print(torch.log(prob[0][action]).unsqueeze(0))</span><br>    <span class="hljs-keyword">return</span> action,torch.log(prob[<span class="hljs-number">0</span>][action]).unsqueeze(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<ul>
<li>（1）使用<code>torch.distributions.Categorical</code>生成分布，然后进行选择</li>
<li>（2）使用<code>np.random.choice</code>进行采样</li>
</ul>
<h2 id="模型的训练"><a class="markdownIt-Anchor" href="#模型的训练"></a> 模型的训练</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_net</span>(<span class="hljs-params">self</span>):</span><br>    <span class="hljs-comment"># 计算梯度并更新策略网络参数。tape为梯度记录器</span><br>    R = <span class="hljs-number">0</span>  <span class="hljs-comment"># 终结状态的初始回报为0</span><br>    policy_loss = []<br>    <span class="hljs-keyword">for</span> r, log_prob <span class="hljs-keyword">in</span> self.data[::-<span class="hljs-number">1</span>]:  <span class="hljs-comment"># 逆序取</span><br>        R = r + gamma * R  <span class="hljs-comment"># 计算每个时间戳上的回报</span><br>        <span class="hljs-comment"># 每个时间戳都计算一次梯度</span><br>        loss = -log_prob * R<br>        policy_loss.append(loss)<br>    self.optimizer.zero_grad()<br>    policy_loss = torch.cat(policy_loss).<span class="hljs-built_in">sum</span>()  <span class="hljs-comment"># 求和</span><br>    <span class="hljs-comment">#反向传播</span><br>    policy_loss.backward()<br>    self.optimizer.step()<br>    self.cost_his.append(policy_loss.item())<br>    self.data = []  <span class="hljs-comment"># 清空轨迹</span><br></code></pre></td></tr></table></figure>
<h2 id="完整代码"><a class="markdownIt-Anchor" href="#完整代码"></a> 完整代码</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> 	gym,os<br><span class="hljs-keyword">import</span>  numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span>  matplotlib<br><span class="hljs-comment"># Default parameters for plots</span><br>matplotlib.rcParams[<span class="hljs-string">&#x27;font.size&#x27;</span>] = <span class="hljs-number">18</span><br>matplotlib.rcParams[<span class="hljs-string">&#x27;figure.titlesize&#x27;</span>] = <span class="hljs-number">18</span><br>matplotlib.rcParams[<span class="hljs-string">&#x27;figure.figsize&#x27;</span>] = [<span class="hljs-number">9</span>, <span class="hljs-number">7</span>]<br>matplotlib.rcParams[<span class="hljs-string">&#x27;font.family&#x27;</span>] = [<span class="hljs-string">&#x27;KaiTi&#x27;</span>]<br>matplotlib.rcParams[<span class="hljs-string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="hljs-literal">False</span><br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br>env = gym.make(<span class="hljs-string">&#x27;CartPole-v1&#x27;</span>)<br>env.seed(<span class="hljs-number">2333</span>)<br>torch.manual_seed(<span class="hljs-number">2333</span>)    <span class="hljs-comment"># 策略梯度算法方差很大，设置seed以保证复现性</span><br>print(<span class="hljs-string">&#x27;observation space:&#x27;</span>,env.observation_space)<br>print(<span class="hljs-string">&#x27;action space:&#x27;</span>,env.action_space)<br><br>learning_rate = <span class="hljs-number">0.0002</span><br>gamma         = <span class="hljs-number">0.98</span><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PolicyNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,n_states_num,n_actions_num,hidden_size</span>):</span><br>        <span class="hljs-built_in">super</span>(PolicyNet, self).__init__()<br>        self.data = [] <span class="hljs-comment">#存储轨迹</span><br>        <span class="hljs-comment">#输入为长度为4的向量 输出为向左  向右两个动作</span><br>        self.net = nn.Sequential(<br>            nn.Linear(in_features=n_states_num, out_features=hidden_size, bias=<span class="hljs-literal">False</span>),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size, out_features=n_actions_num, bias=<span class="hljs-literal">False</span>),<br>            nn.Softmax(dim=<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, inputs</span>):</span><br>        <span class="hljs-comment"># 状态输入s的shape为向量：[4]</span><br>        x = self.net(inputs)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PolicyGradient</span>():</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,n_states_num,n_actions_num,learning_rate=<span class="hljs-number">0.01</span>,reward_decay=<span class="hljs-number">0.95</span> </span>):</span><br>        <span class="hljs-comment">#状态数   state是一个4维向量，分别是位置，速度，杆子的角度，加速度</span><br>        self.n_states_num = n_states_num<br>        <span class="hljs-comment">#action是二维、离散，即向左/右推杆子</span><br>        self.n_actions_num = n_actions_num<br>        <span class="hljs-comment">#学习率</span><br>        self.lr = learning_rate<br>        <span class="hljs-comment">#gamma</span><br>        self.gamma = reward_decay<br>        <span class="hljs-comment">#网络</span><br>        self.pi = PolicyNet(n_states_num, n_actions_num, <span class="hljs-number">128</span>)<br>        <span class="hljs-comment">#优化器</span><br>        self.optimizer = torch.optim.Adam(self.pi.parameters(), lr=learning_rate)<br>        <span class="hljs-comment"># 存储轨迹  存储方式为  （每一次的reward，动作的概率）</span><br>        self.data = []<br>        self.cost_his = []<br>    <span class="hljs-comment">#存储轨迹数据</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">put_data</span>(<span class="hljs-params">self, item</span>):</span><br>        <span class="hljs-comment"># 记录r,log_P(a|s)z</span><br>        self.data.append(item)<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_net</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 计算梯度并更新策略网络参数。tape为梯度记录器</span><br>        R = <span class="hljs-number">0</span>  <span class="hljs-comment"># 终结状态的初始回报为0</span><br>        policy_loss = []<br>        <span class="hljs-keyword">for</span> r, log_prob <span class="hljs-keyword">in</span> self.data[::-<span class="hljs-number">1</span>]:  <span class="hljs-comment"># 逆序取</span><br>            R = r + gamma * R  <span class="hljs-comment"># 计算每个时间戳上的回报</span><br>            <span class="hljs-comment"># 每个时间戳都计算一次梯度</span><br>            loss = -log_prob * R<br>            policy_loss.append(loss)<br>        self.optimizer.zero_grad()<br>        policy_loss = torch.cat(policy_loss).<span class="hljs-built_in">sum</span>()  <span class="hljs-comment"># 求和</span><br>        <span class="hljs-comment">#反向传播</span><br>        policy_loss.backward()<br>        self.optimizer.step()<br>        self.cost_his.append(policy_loss.item())<br>        self.data = []  <span class="hljs-comment"># 清空轨迹</span><br>    <span class="hljs-comment">#将状态传入神经网络 根据概率选择动作</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span>  <span class="hljs-title">choose_action</span>(<span class="hljs-params">self,state</span>):</span><br>        <span class="hljs-comment">#将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span><br>        s = torch.Tensor(state).unsqueeze(<span class="hljs-number">0</span>)<br>        prob = self.pi(s)  <span class="hljs-comment"># 动作分布:[1,2]</span><br>        <span class="hljs-comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span><br>        m = torch.distributions.Categorical(prob)  <span class="hljs-comment"># 生成分布</span><br>        action = m.sample()<br>        <span class="hljs-keyword">return</span> action.item() , m.log_prob(action)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">choose_action2</span>(<span class="hljs-params">self, state</span>):</span><br>        <span class="hljs-comment"># 将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span><br>        s = torch.Tensor(state).unsqueeze(<span class="hljs-number">0</span>)<br>        prob = self.pi(s)  <span class="hljs-comment"># 动作分布:[1,2]</span><br>        <span class="hljs-comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span><br>        action =np.random.choice(<span class="hljs-built_in">range</span>(prob.shape[<span class="hljs-number">1</span>]),size=<span class="hljs-number">1</span>,p = prob.view(-<span class="hljs-number">1</span>).detach().numpy())[<span class="hljs-number">0</span>]<br>        action = <span class="hljs-built_in">int</span>(action)<br>        <span class="hljs-comment">#print(torch.log(prob[0][action]).unsqueeze(0))</span><br>        <span class="hljs-keyword">return</span> action,torch.log(prob[<span class="hljs-number">0</span>][action]).unsqueeze(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_cost</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>        plt.plot(np.arange(<span class="hljs-built_in">len</span>(self.cost_his)), self.cost_his)<br>        plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>        plt.xlabel(<span class="hljs-string">&#x27;training steps&#x27;</span>)<br>        plt.show()<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    policyGradient = PolicyGradient(<span class="hljs-number">4</span>,<span class="hljs-number">2</span>)<br>    running_reward = <span class="hljs-number">10</span>  <span class="hljs-comment"># 计分</span><br>    print_interval = <span class="hljs-number">20</span>  <span class="hljs-comment"># 打印间隔</span><br>    <span class="hljs-keyword">for</span> n_epi <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):<br>        state = env.reset()  <span class="hljs-comment"># 回到游戏初始状态，返回s0</span><br>        ep_reward = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1001</span>):  <span class="hljs-comment"># CartPole-v1 forced to terminates at 1000 step.</span><br>            <span class="hljs-comment">#根据状态 传入神经网络 选择动作</span><br>            action ,log_prob  = policyGradient.choose_action2(state)<br>            <span class="hljs-comment">#与环境交互</span><br>            s_prime, reward, done, info = env.step(action)<br>            <span class="hljs-comment"># s_prime, reward, done, info = env.step(action)</span><br>            <span class="hljs-keyword">if</span> n_epi &gt; <span class="hljs-number">1000</span>:<br>                env.render()<br>            <span class="hljs-comment"># 记录动作a和动作产生的奖励r</span><br>            <span class="hljs-comment"># prob shape:[1,2]</span><br>            policyGradient.put_data((reward, log_prob))<br>            state = s_prime  <span class="hljs-comment"># 刷新状态</span><br>            ep_reward += reward<br>            <span class="hljs-keyword">if</span> done:  <span class="hljs-comment"># 当前episode终止</span><br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-comment"># episode终止后，训练一次网络</span><br>        running_reward = <span class="hljs-number">0.05</span> * ep_reward + (<span class="hljs-number">1</span> - <span class="hljs-number">0.05</span>) * running_reward<br>        <span class="hljs-comment">#交互完成后 进行学习</span><br>        policyGradient.train_net()<br>        <span class="hljs-keyword">if</span> n_epi % print_interval == <span class="hljs-number">0</span>:<br>            print(<span class="hljs-string">&#x27;Episode &#123;&#125;\tLast reward: &#123;:.2f&#125;\tAverage reward: &#123;:.2f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                n_epi, ep_reward, running_reward))<br>        <span class="hljs-keyword">if</span> running_reward &gt; env.spec.reward_threshold:  <span class="hljs-comment"># 大于游戏的最大阈值475时，退出游戏</span><br>            print(<span class="hljs-string">&quot;Solved! Running reward is now &#123;&#125; and &quot;</span><br>                  <span class="hljs-string">&quot;the last episode runs to &#123;&#125; time steps!&quot;</span>.<span class="hljs-built_in">format</span>(running_reward, t))<br>            <span class="hljs-keyword">break</span><br>    policyGradient.plot_cost()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br><br></code></pre></td></tr></table></figure>
<h1 id="dqn算法实现"><a class="markdownIt-Anchor" href="#dqn算法实现"></a> DQN算法实现</h1>
<p>DQN算法的思想在<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">另一篇博客</a>中有介绍了，下面是算法的具体实现。</p>
<h2 id="经验回放池"><a class="markdownIt-Anchor" href="#经验回放池"></a> 经验回放池</h2>
<p>这里使用python的双向队列实现了经验回放池，实现了状态的存储以及随机采样。</p>
<ul>
<li>经验回放 Experience replay：由于在强化学习中，我们得到 的观测数据是有序的，用这样的数据去更新神经网络的参数会有问题（对比监督学习，数据之间都是独立的）。因此 DQN 中使 用经验回放，即用一个 Memory 来存储经历过的数据，每次更新参数的时候从 Memory 中抽取一部分的数据来用于更新，以此来打破数据间的关联。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ReplayBuffer</span>():</span><br>    <span class="hljs-comment"># 经验回放池</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 双向队列</span><br>        self.buffer = collections.deque(maxlen=buffer_limit)<br>        <span class="hljs-comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">put</span>(<span class="hljs-params">self, transition</span>):</span><br>        self.buffer.append(transition)<br>    <span class="hljs-comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample</span>(<span class="hljs-params">self, n</span>):</span><br>        <span class="hljs-comment"># 从回放池采样n个5元组</span><br>        mini_batch = random.sample(self.buffer, n)<br>        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []<br>        <span class="hljs-comment"># 按类别进行整理</span><br>        <span class="hljs-keyword">for</span> transition <span class="hljs-keyword">in</span> mini_batch:<br>            s, a, r, s_prime = transition<br>            s_lst.append(s)<br>            a_lst.append([a])<br>            r_lst.append([r])<br>            s_prime_lst.append(s_prime)<br>        <span class="hljs-comment"># 转换成Tensor</span><br>        <span class="hljs-keyword">return</span> torch.Tensor(s_lst), \<br>               torch.Tensor(a_lst), \<br>                      torch.Tensor(r_lst), \<br>                      torch.Tensor(s_prime_lst)<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">size</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.buffer)<br></code></pre></td></tr></table></figure>
<h2 id="q网络"><a class="markdownIt-Anchor" href="#q网络"></a> Q网络</h2>
<p>DQN使用神经网络取代了Q Table去预测Q（s，a）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Qnet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,input_size,output_size,hidden_size</span>):</span><br>        <span class="hljs-comment"># 创建Q网络，输入为状态向量，输出为动作的Q值</span><br>        <span class="hljs-built_in">super</span>(Qnet, self).__init__()<br>        self.net = nn.Sequential(<br>            nn.Linear(in_features=input_size, out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size, out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size, out_features=output_size),<br>        )<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, inputs, training=<span class="hljs-literal">None</span></span>):</span><br>        x = self.net(inputs)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<h2 id="模型训练"><a class="markdownIt-Anchor" href="#模型训练"></a> 模型训练</h2>
<p>模型使用的两个Q网络，  在原来的 Q 网络的基础上又引入了一个 target Q 网络，即用来计算 target 的网络。它和 Q 网络结构一样， 初始的权重也一样，只是 Q 网络每次迭代都会更新，而 target Q 网络是每隔一段时间才会更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#训练模型</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="hljs-number">0</span>:<br>            self.q_target_net.load_state_dict(self.q_net.state_dict())<br><br>        <span class="hljs-comment"># 通过Q网络和影子网络来构造贝尔曼方程的误差，</span><br>        <span class="hljs-comment"># 并只更新Q网络，影子网络的更新会滞后Q网络</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>): <span class="hljs-comment"># 训练10次</span><br>            s, a, r, s_prime = self.memory.sample(self.batch_size)<br>            <span class="hljs-comment"># q_prime  用旧网络、动作后的环境预测，q_a 用新网络、动作前的环境；同时预测记忆中的情形</span><br>            q_next, q_eval = self.q_target_net(s),self.q_net(s_prime)<br>            <span class="hljs-comment"># 每次学习都用下一个状态的动作结合反馈作为当前动作值（这样，将未来状态的动作作为目标，有一定前瞻性）</span><br>            q_target = q_eval<br>            <span class="hljs-comment">#action的index值 它所在的位置</span><br><br>            <span class="hljs-comment">#实际Q网络的值</span><br>            act_index = np.array(a.tolist()).astype(np.int64)<br>            act_index = torch.from_numpy(act_index)<br>            q_a = q_eval.gather(<span class="hljs-number">1</span>, act_index)  <span class="hljs-comment"># 动作的概率值, [b,1]</span><br><br>            <span class="hljs-comment">#target Q网络的值</span><br>            max_q_prime, _ = torch.<span class="hljs-built_in">max</span>(q_next, dim=<span class="hljs-number">1</span>)<br>            max_q_prime = max_q_prime.unsqueeze(<span class="hljs-number">1</span>)<br><br>            q_target = r + self.gamma * max_q_prime<br>            <span class="hljs-comment">#q_out[batch_index, eval_act_index] = reward + self.gamma * np.max(q_prime, axis=1)</span><br><br><br>            loss = self.loss(q_a,q_target)<br>            self.optimizer.zero_grad()<br>            loss.backward()<br>            self.optimizer.step()<br>            cost = loss.item()<br><br>            self.cost_his.append(cost)<br><br>            self.epsilon = self.epsilon + self.epsilon_increment <span class="hljs-keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="hljs-keyword">else</span> self.epsilon_max<br>            self.learn_step_counter += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure>
<h2 id="完整代码-2"><a class="markdownIt-Anchor" href="#完整代码-2"></a> 完整代码</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> collections<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> gym,os<br><span class="hljs-keyword">import</span>  numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br>env = gym.make(<span class="hljs-string">&#x27;CartPole-v1&#x27;</span>)<br>env.seed(<span class="hljs-number">2333</span>)<br>torch.manual_seed(<span class="hljs-number">2333</span>)    <span class="hljs-comment"># 策略梯度算法方差很大，设置seed以保证复现性</span><br>print(<span class="hljs-string">&#x27;observation space:&#x27;</span>,env.observation_space)<br>print(<span class="hljs-string">&#x27;action space:&#x27;</span>,env.action_space)<br><br><span class="hljs-comment"># Hyperparameters</span><br>learning_rate = <span class="hljs-number">0.0002</span><br>gamma = <span class="hljs-number">0.99</span><br>buffer_limit = <span class="hljs-number">50000</span><br>batch_size = <span class="hljs-number">32</span><br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ReplayBuffer</span>():</span><br>    <span class="hljs-comment"># 经验回放池</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 双向队列</span><br>        self.buffer = collections.deque(maxlen=buffer_limit)<br>        <span class="hljs-comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">put</span>(<span class="hljs-params">self, transition</span>):</span><br>        self.buffer.append(transition)<br>    <span class="hljs-comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample</span>(<span class="hljs-params">self, n</span>):</span><br>        <span class="hljs-comment"># 从回放池采样n个5元组</span><br>        mini_batch = random.sample(self.buffer, n)<br>        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []<br>        <span class="hljs-comment"># 按类别进行整理</span><br>        <span class="hljs-keyword">for</span> transition <span class="hljs-keyword">in</span> mini_batch:<br>            s, a, r, s_prime = transition<br>            s_lst.append(s)<br>            a_lst.append([a])<br>            r_lst.append([r])<br>            s_prime_lst.append(s_prime)<br>        <span class="hljs-comment"># 转换成Tensor</span><br>        <span class="hljs-keyword">return</span> torch.Tensor(s_lst), \<br>               torch.Tensor(a_lst), \<br>                      torch.Tensor(r_lst), \<br>                      torch.Tensor(s_prime_lst)<br><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">size</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.buffer)<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Qnet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,input_size,output_size,hidden_size</span>):</span><br>        <span class="hljs-comment"># 创建Q网络，输入为状态向量，输出为动作的Q值</span><br>        <span class="hljs-built_in">super</span>(Qnet, self).__init__()<br>        self.net = nn.Sequential(<br>            nn.Linear(in_features=input_size, out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size, out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size, out_features=output_size),<br>        )<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, inputs, training=<span class="hljs-literal">None</span></span>):</span><br>        x = self.net(inputs)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DQN</span>():</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,input_size,output_size,hidden_size,learning_rate,reward_decay,epsilon,e_greedy_increment,e_greedy</span>):</span><br>        self.n_actions = output_size<br>        self.n_features = input_size<br>        self.learning_rate = learning_rate<br>        self.gamma = reward_decay<br>        self.epsilon = epsilon <span class="hljs-comment">#e - 贪心方式 参数</span><br>        self.q_net = Qnet(input_size,output_size,hidden_size)<br>        self.q_target_net = Qnet(input_size,output_size,hidden_size)  <span class="hljs-comment"># 创建影子网络</span><br>        self.q_target_net.load_state_dict(self.q_net.state_dict()) <span class="hljs-comment"># 影子网络权值来自Q</span><br>        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)<span class="hljs-comment">#优化器</span><br>        self.buffer_limit = <span class="hljs-number">50000</span><br>        self.batch_size = <span class="hljs-number">32</span><br>        self.memory = ReplayBuffer()  <span class="hljs-comment">#创建回放池</span><br>        <span class="hljs-comment"># Huber Loss常用于回归问题，其最大的特点是对离群点（outliers）、噪声不敏感，具有较强的鲁棒性</span><br>        self.loss  = torch.nn.SmoothL1Loss()<span class="hljs-comment">#损失函数</span><br>        self.cost_his = []<br>        self.epsilon_increment =e_greedy_increment<br>        self.learn_step_counter = <span class="hljs-number">0</span><br>        self.epsilon_max = e_greedy<br><br>        self.replace_target_iter = <span class="hljs-number">300</span><br>    <span class="hljs-comment"># 送入状态向量，获取策略: [4]</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">choose_action</span>(<span class="hljs-params">self,state</span>):</span><br>        <span class="hljs-comment"># 将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span><br>        state = torch.Tensor(state).unsqueeze(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 策略改进：e-贪心方式</span><br>        <span class="hljs-keyword">if</span> np.random.uniform() &lt; self.epsilon:<br>            actions_value = self.q_net(state)<br>            action = np.argmax(actions_value.detach().numpy())<br>        <span class="hljs-keyword">else</span>:<br>            action = np.random.randint(<span class="hljs-number">0</span>, self.n_actions)<br>        <span class="hljs-keyword">return</span> action<br>    <span class="hljs-comment">#训练模型</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="hljs-number">0</span>:<br>            self.q_target_net.load_state_dict(self.q_net.state_dict())<br>        <span class="hljs-comment"># 通过Q网络和影子网络来构造贝尔曼方程的误差，</span><br>        <span class="hljs-comment"># 并只更新Q网络，影子网络的更新会滞后Q网络</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>): <span class="hljs-comment"># 训练10次</span><br>            s, a, r, s_prime = self.memory.sample(self.batch_size)<br>            <span class="hljs-comment"># q_prime  用旧网络、动作后的环境预测，q_a 用新网络、动作前的环境；同时预测记忆中的情形</span><br>            q_next, q_eval = self.q_target_net(s),self.q_net(s_prime)<br>            <span class="hljs-comment"># 每次学习都用下一个状态的动作结合反馈作为当前动作值（这样，将未来状态的动作作为目标，有一定前瞻性）</span><br>            q_target = q_eval<br>            <span class="hljs-comment">#action的index值 它所在的位置</span><br>            <span class="hljs-comment">#实际Q网络的值</span><br>            act_index = np.array(a.tolist()).astype(np.int64)<br>            act_index = torch.from_numpy(act_index)<br>            q_a = q_eval.gather(<span class="hljs-number">1</span>, act_index)  <span class="hljs-comment"># 动作的概率值, [b,1]</span><br>            <span class="hljs-comment">#target Q网络的值</span><br>            max_q_prime, _ = torch.<span class="hljs-built_in">max</span>(q_next, dim=<span class="hljs-number">1</span>)<br>            max_q_prime = max_q_prime.unsqueeze(<span class="hljs-number">1</span>)<br>            q_target = r + self.gamma * max_q_prime<br>            loss = self.loss(q_a,q_target)<br>            self.optimizer.zero_grad()<br>            loss.backward()<br>            self.optimizer.step()<br>            cost = loss.item()<br><br>            self.cost_his.append(cost)<br><br>            self.epsilon = self.epsilon + self.epsilon_increment <span class="hljs-keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="hljs-keyword">else</span> self.epsilon_max<br>            self.learn_step_counter += <span class="hljs-number">1</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_cost</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>        plt.plot(np.arange(<span class="hljs-built_in">len</span>(self.cost_his)), self.cost_his)<br>        plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>        plt.xlabel(<span class="hljs-string">&#x27;training steps&#x27;</span>)<br>        plt.show()<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    env = gym.make(<span class="hljs-string">&#x27;CartPole-v1&#x27;</span>)  <span class="hljs-comment"># 创建环境</span><br>    dqn = DQN(input_size = <span class="hljs-number">4</span>,<br>              output_size = <span class="hljs-number">2</span>,<br>              hidden_size = <span class="hljs-number">10</span>,<br>              learning_rate = <span class="hljs-number">0.01</span>,<br>              reward_decay=<span class="hljs-number">0.9</span>,epsilon=<span class="hljs-number">0.9</span>,<br>              e_greedy_increment=<span class="hljs-number">0.001</span>,<br>              e_greedy=<span class="hljs-number">0.9</span>,<br>              )<br><br>    print_interval = <span class="hljs-number">20</span><br>    reword = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> n_epi <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2000</span>):  <span class="hljs-comment"># 训练次数</span><br>        <span class="hljs-comment"># epsilon概率也会8%到1%衰减，越到后面越使用Q值最大的动作</span><br>        dqn.epsilon = <span class="hljs-built_in">max</span>(<span class="hljs-number">0.01</span>, <span class="hljs-number">0.08</span> - <span class="hljs-number">0.01</span> * (n_epi / <span class="hljs-number">200</span>))<br>        state = env.reset()  <span class="hljs-comment"># 复位环境</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>: <span class="hljs-comment"># 一个回合最大时间戳</span><br>            <span class="hljs-comment"># 根据当前Q网络提取策略，并改进策略</span><br>            action = dqn.choose_action(state)<br>            <span class="hljs-comment"># 使用改进的策略与环境交互</span><br>            s_prime, r, done, info = env.step(action)<br>            <span class="hljs-comment">#https://blog.csdn.net/u012465304/article/details/81172759</span><br>            <span class="hljs-comment">#由于CartPole这个游戏的reward是只要杆子是立起来的，他reward就是1，失败就是0，</span><br>            <span class="hljs-comment"># 显然这个reward对于连续性变量是不可以接受的，所以我们通过observation修改这个值。</span><br>            <span class="hljs-comment"># 点击pycharm右上角的搜索符号搜索CartPole进入他环境的源代码中，再进入step函数，</span><br>            <span class="hljs-comment"># 看到里面返回值state的定义</span><br>            <span class="hljs-comment">#通过这四个值定义新的reward是</span><br>            x, x_dot, theta, theta_dot = s_prime<br>            r1 = (env.x_threshold - <span class="hljs-built_in">abs</span>(x)) / env.x_threshold - <span class="hljs-number">0.8</span><br>            r2 = (env.theta_threshold_radians - <span class="hljs-built_in">abs</span>(theta)) / env.theta_threshold_radians - <span class="hljs-number">0.5</span><br>            reward = r1 + r2<br><br>            <span class="hljs-comment"># 保存四元组</span><br>            dqn.memory.put((state, action,reward,s_prime))<br>            state = s_prime<br>            reword +=reward<br>            <span class="hljs-keyword">if</span> done:  <span class="hljs-comment"># 回合结束</span><br>                <span class="hljs-keyword">break</span><br><br>            <span class="hljs-keyword">if</span> dqn.memory.size() &gt; <span class="hljs-number">1000</span>:  <span class="hljs-comment"># 缓冲池只有大于2000就可以训练</span><br>                dqn.train()<br>        <span class="hljs-keyword">if</span> n_epi % print_interval == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> n_epi != <span class="hljs-number">0</span>:<br>            print(<span class="hljs-string">&quot;# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, &quot;</span> \<br>                  <span class="hljs-string">&quot;epsilon : &#123;:.1f&#125;%&quot;</span> \<br>                  .<span class="hljs-built_in">format</span>(n_epi, reword / print_interval, dqn.memory.size(), dqn.epsilon * <span class="hljs-number">100</span>))<br>            reword = <span class="hljs-number">0.0</span><br>    env.close()<br>    dqn.plot_cost()<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br><br><br><br></code></pre></td></tr></table></figure>
<h1 id="ddpg算法实现"><a class="markdownIt-Anchor" href="#ddpg算法实现"></a> DDPG算法实现</h1>
<p>DDPG算法的思想在<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">另一篇博客</a>中有介绍了，下面是算法的具体实现。</p>
<p>DDPG 可以解决连续动作空间问题，并且是actor-critic方法，即既有值函数网络(critic)，又有策略网络(actor)。</p>
<h2 id="经验回放池-2"><a class="markdownIt-Anchor" href="#经验回放池-2"></a> 经验回放池</h2>
<p>与DQN中的经验回放池的实现相同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ReplayBuffer</span>():</span><br>    <span class="hljs-comment"># 经验回放池</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 双向队列</span><br>        buffer_limit = <span class="hljs-number">50000</span><br>        self.buffer = collections.deque(maxlen=buffer_limit)<br>        <span class="hljs-comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">put</span>(<span class="hljs-params">self, transition</span>):</span><br>        self.buffer.append(transition)<br>    <span class="hljs-comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample</span>(<span class="hljs-params">self, n</span>):</span><br>        <span class="hljs-comment"># 从回放池采样n个5元组</span><br>        mini_batch = random.sample(self.buffer, n)<br>        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []<br>        <span class="hljs-comment"># 按类别进行整理</span><br>        <span class="hljs-keyword">for</span> transition <span class="hljs-keyword">in</span> mini_batch:<br>            s, a, r, s_prime = transition<br>            s_lst.append(s)<br>            a_lst.append([a])<br>            r_lst.append([r])<br>            s_prime_lst.append(s_prime)<br>        <span class="hljs-comment"># 转换成Tensor</span><br>        <span class="hljs-keyword">return</span> torch.Tensor(s_lst), \<br>               torch.Tensor(a_lst), \<br>                      torch.Tensor(r_lst), \<br>                      torch.Tensor(s_prime_lst)<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">size</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.buffer)<br></code></pre></td></tr></table></figure>
<h2 id="策略网络actor网络"><a class="markdownIt-Anchor" href="#策略网络actor网络"></a> 策略网络（Actor网络）</h2>
<p>输入为state  输出为概率分布pi(a|s)（每个动作出现的概率）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 策略网络，也叫Actor网络，输入为state  输出为概率分布pi(a|s)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Actor</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,input_size,hidden_size,output_size</span>):</span><br>        <span class="hljs-built_in">super</span>(Actor, self).__init__()<br>        <span class="hljs-comment"># self.linear  = nn.Linear(hidden_size, output_size)</span><br>        self.actor_net = nn.Sequential(<br>            nn.Linear(in_features=input_size,out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size,out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size,out_features=output_size)<br>        )<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,state</span>):</span><br>        x = self.actor_net(state)<br>        x = torch.tanh(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<h2 id="值函数网络-输入是stateaction输出是qsa"><a class="markdownIt-Anchor" href="#值函数网络-输入是stateaction输出是qsa"></a> 值函数网络  输入是state，action输出是Q(s,a)</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#值函数网络  输入是state，action输出是Q(s,a)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Critic</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):</span><br>        <span class="hljs-built_in">super</span>(Critic, self).__init__()<br>        self.critic_net = nn.Sequential(<br>            nn.Linear(in_features=input_size, out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size, out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size, out_features=output_size)<br>        )<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, state,action</span>):</span><br>        inputs = torch.cat([state,action],<span class="hljs-number">1</span>)<br>        x = self.critic_net(inputs)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<h2 id="整体实现"><a class="markdownIt-Anchor" href="#整体实现"></a> 整体实现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gym<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span>  collections<br><br>env = gym.make(<span class="hljs-string">&#x27;Pendulum-v0&#x27;</span>)<br>env.seed(<span class="hljs-number">2333</span>)<br>torch.manual_seed(<span class="hljs-number">2333</span>)    <span class="hljs-comment"># 策略梯度算法方差很大，设置seed以保证复现性</span><br>env.reset()<br>env.render()<br>print(<span class="hljs-string">&#x27;observation space:&#x27;</span>,env.observation_space)<br>print(<span class="hljs-string">&#x27;action space:&#x27;</span>,env.action_space)<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ReplayBuffer</span>():</span><br>    <span class="hljs-comment"># 经验回放池</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 双向队列</span><br>        buffer_limit = <span class="hljs-number">50000</span><br>        self.buffer = collections.deque(maxlen=buffer_limit)<br>        <span class="hljs-comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">put</span>(<span class="hljs-params">self, transition</span>):</span><br>        self.buffer.append(transition)<br>    <span class="hljs-comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample</span>(<span class="hljs-params">self, n</span>):</span><br>        <span class="hljs-comment"># 从回放池采样n个5元组</span><br>        mini_batch = random.sample(self.buffer, n)<br>        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []<br>        <span class="hljs-comment"># 按类别进行整理</span><br>        <span class="hljs-keyword">for</span> transition <span class="hljs-keyword">in</span> mini_batch:<br>            s, a, r, s_prime = transition<br>            s_lst.append(s)<br>            a_lst.append([a])<br>            r_lst.append([r])<br>            s_prime_lst.append(s_prime)<br>        <span class="hljs-comment"># 转换成Tensor</span><br>        <span class="hljs-keyword">return</span> torch.Tensor(s_lst), \<br>               torch.Tensor(a_lst), \<br>                      torch.Tensor(r_lst), \<br>                      torch.Tensor(s_prime_lst)<br><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">size</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.buffer)<br><br><br><span class="hljs-comment"># 策略网络，也叫Actor网络，输入为state  输出为概率分布pi(a|s)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Actor</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,input_size,hidden_size,output_size</span>):</span><br>        <span class="hljs-built_in">super</span>(Actor, self).__init__()<br>        <span class="hljs-comment"># self.linear  = nn.Linear(hidden_size, output_size)</span><br>        self.actor_net = nn.Sequential(<br>            nn.Linear(in_features=input_size,out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size,out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size,out_features=output_size)<br>        )<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self,state</span>):</span><br>        x = self.actor_net(state)<br>        x = torch.tanh(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment">#值函数网络  输入是state，action输出是Q(s,a)</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Critic</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):</span><br>        <span class="hljs-built_in">super</span>(Critic, self).__init__()<br>        self.critic_net = nn.Sequential(<br>            nn.Linear(in_features=input_size, out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size, out_features=hidden_size),<br>            nn.ReLU(),<br>            nn.Linear(in_features=hidden_size, out_features=output_size)<br>        )<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, state,action</span>):</span><br>        inputs = torch.cat([state,action],<span class="hljs-number">1</span>)<br>        x = self.critic_net(inputs)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DDPG</span>():</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,state_size,action_size,hidden_size = <span class="hljs-number">256</span>,actor_lr = <span class="hljs-number">0.001</span>,ctitic_lr = <span class="hljs-number">0.001</span>,batch_size = <span class="hljs-number">32</span></span>):</span><br><br>        self.state_size = state_size<br>        self.action_size = action_size<br>        self.hidden_size = hidden_size<br>        self.actor_lr = actor_lr <span class="hljs-comment">#actor网络学习率</span><br>        self.critic_lr = ctitic_lr<span class="hljs-comment">#critic网络学习率</span><br>        <span class="hljs-comment"># 策略网络，也叫Actor网络，输入为state  输出为概率分布pi(a|s)</span><br>        self.actor = Actor(self.state_size, self.hidden_size, self.action_size)<br>        <span class="hljs-comment">#target actor网络 延迟更新</span><br>        self.actor_target = Actor(self.state_size, self.hidden_size, self.action_size)<br>        <span class="hljs-comment"># 值函数网络  输入是state，action输出是Q(s,a)</span><br>        self.critic = Critic(self.state_size + self.action_size, self.hidden_size, self.action_size)<br>        self.critic_target = Critic(self.state_size + self.action_size, self.hidden_size, self.action_size)<br><br>        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.actor_lr)<br>        self.critic_optim = optim.Adam(self.critic.parameters(), lr=self.critic_lr)<br>        self.buffer = []<br>        <span class="hljs-comment"># 影子网络权值来自原网络，只不过延迟更新</span><br>        self.actor_target.load_state_dict(self.actor.state_dict())<br>        self.critic_target.load_state_dict(self.critic.state_dict())<br>        self.gamma = <span class="hljs-number">0.99</span><br>        self.batch_size = batch_size<br>        self.memory = ReplayBuffer()  <span class="hljs-comment"># 创建回放池</span><br><br>        self.memory2 = []<br>        self.learn_step_counter = <span class="hljs-number">0</span> <span class="hljs-comment">#学习轮数 与影子网络的更新有关</span><br>        self.replace_target_iter = <span class="hljs-number">200</span> <span class="hljs-comment">#影子网络迭代多少轮更新一次</span><br>        self.cost_his_actor = []<span class="hljs-comment"># 存储cost 准备画图</span><br>        self.cost_his_critic = []<br><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">choose_action</span>(<span class="hljs-params">self,state</span>):</span><br>        <span class="hljs-comment"># 将state转化成tensor 并且维度转化为[3]-&gt;[1,3]  unsqueeze(0)在第0个维度上田间</span><br>        state = torch.Tensor(state).unsqueeze(<span class="hljs-number">0</span>)<br>        action = self.actor(state).squeeze(<span class="hljs-number">0</span>).detach().numpy()<br>        <span class="hljs-keyword">return</span> action<br>    <span class="hljs-comment">#critic网络的学习</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">critic_learn</span>(<span class="hljs-params">self,s0,a0,r1,s1</span>):</span><br>        <span class="hljs-comment">#从actor_target通过状态获取对应的动作  detach()将tensor从计算图上剥离</span><br>        a1 = self.actor_target(s0).detach()<br>        <span class="hljs-comment">#删减一个维度  [b,1,1]变成[b,1]</span><br>        a0 = a0.squeeze(<span class="hljs-number">2</span>)<br>        y_pred = self.critic(s0,a0)<br>        y_target = r1 +self.gamma *self.critic_target(s1,a1).detach()<br>        loss_fn = nn.MSELoss()<br>        loss = loss_fn(y_pred, y_target)<br>        self.critic_optim.zero_grad()<br>        loss.backward()<br>        self.critic_optim.step()<br>        self.cost_his_critic.append(loss.item())<br>    <span class="hljs-comment">#actor网络的学习</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">actor_learn</span>(<span class="hljs-params">self,s0,a0,r1,s1</span>):</span><br>        loss = -torch.mean(self.critic(s0, self.actor(s0)))<br>        self.actor_optim.zero_grad()<br>        loss.backward()<br>        self.actor_optim.step()<br>        self.cost_his_actor.append(loss.item())<br>    <span class="hljs-comment">#模型的训练</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="hljs-number">0</span>:<br>            self.actor_target.load_state_dict(self.actor.state_dict())<br>            self.critic_target.load_state_dict(self.critic.state_dict())<br>        <span class="hljs-comment">#随机采样出 batch_size 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span><br>        s0, a0, r, s_prime = self.memory.sample(self.batch_size)<br>        self.critic_learn(s0, a0, r, s_prime)<br>        self.actor_learn(s0, a0, r, s_prime)<br><br>        self.soft_update(self.critic_target, self.critic, <span class="hljs-number">0.02</span>)<br>        self.soft_update(self.actor_target, self.actor, <span class="hljs-number">0.02</span>)<br>    <span class="hljs-comment">#target网络的更新</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">soft_update</span>(<span class="hljs-params">self,net_target, net, tau</span>):</span><br>        <span class="hljs-keyword">for</span> target_param, param <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(net_target.parameters(), net.parameters()):<br>            target_param.data.copy_(target_param.data * (<span class="hljs-number">1.0</span> - tau) + param.data * tau)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_cost</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>        plt.plot(np.arange(<span class="hljs-built_in">len</span>(self.cost_his_critic)), self.cost_his_critic)<br>        plt.ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>        plt.xlabel(<span class="hljs-string">&#x27;training steps&#x27;</span>)<br>        plt.show()<br><br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    print(env.observation_space.shape[<span class="hljs-number">0</span>])<br>    print(env.action_space.shape[<span class="hljs-number">0</span>])<br>    ddgp = DDPG(state_size=env.observation_space.shape[<span class="hljs-number">0</span>],<br>                action_size=env.action_space.shape[<span class="hljs-number">0</span>],<br>                hidden_size=<span class="hljs-number">256</span>,<br>                actor_lr=<span class="hljs-number">0.001</span>,<br>                ctitic_lr=  <span class="hljs-number">0.001</span>,<br>                batch_size=<span class="hljs-number">32</span>)<br><br>    print_interval = <span class="hljs-number">4</span><br><br>    <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>        state = env.reset()<br>        episode_reward = <span class="hljs-number">0</span><br><br>        <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">500</span>):<br>            env.render()<br>            action0 = ddgp.choose_action(state)<br>            s_prime, r, done, info = env.step(action0)<br><br>            <span class="hljs-comment"># 保存四元组</span><br>            ddgp.memory.put((state, action0, r, s_prime))<br>            episode_reward += r<br>            state = s_prime<br><br>            <span class="hljs-keyword">if</span> done:  <span class="hljs-comment"># 回合结束</span><br>                <span class="hljs-keyword">break</span><br><br>            <span class="hljs-keyword">if</span> ddgp.memory.size() &gt; <span class="hljs-number">32</span>:  <span class="hljs-comment"># 缓冲池只有大于500就可以训练</span><br>                ddgp.train()<br><br>        <span class="hljs-keyword">if</span> episode % print_interval == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> episode != <span class="hljs-number">0</span>:<br>            print(<span class="hljs-string">&quot;# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, &quot;</span><br>                  .<span class="hljs-built_in">format</span>(episode, episode_reward / print_interval, ddgp.memory.size()))<br>    env.close()<br>    ddgp.plot_cost()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/torch.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/10/16/Arithmetic-LeetCode/282/"><img class="prev-cover" src="/2021/10/16/Arithmetic-LeetCode/282/show.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Leetcode 282. 给表达式添加运算符</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/"><img class="next-cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">PyTorch常用工具模块</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">PyTorch常用工具模块</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块"><img class="cover" src="/2020/12/09/Pytorch/Pytorch-and-torch-nn/torch.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorch中神经网络工具箱nn模块</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd"><img class="cover" src="/2020/12/09/Pytorch/Pytorch-and-Autograd/torch.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorch中的Autograd</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-Tensor/" title="Pytorch中的Tensor"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorch中的Tensor</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-Introductory-knowledge/" title="PyTorch入门知识"><img class="cover" src="/2020/12/09/Pytorch/Pytorch-Introductory-knowledge/torch.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">PyTorch入门知识</div></div></a></div><div><a href="/2020/12/08/Tensorflow/Tensorflow-and-Reinforcement-learning/" title="Tensorflow与强化学习"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-08</div><div class="title">Tensorflow与强化学习</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ccclll777</div><div class="author-info__description">胸怀猛虎 细嗅蔷薇</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ccclll777"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccclll777" target="_blank" title="fab fa-github"><i class="GitHub"></i></a><a class="social-icon" href="mailto:sdu945860882@gmail.com" target="_blank" title="fa fa-envelope"><i class="E-Mail"></i></a><a class="social-icon" href="https://www.weibo.com/6732062654" target="_blank" title="fab fa-weibo"><i class="Weibo"></i></a><a class="social-icon" href="https://blog.csdn.net/baidu_41871794" target="_blank" title="gratipay"><i class="CSDN"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#policy-gradient%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.</span> <span class="toc-text"> Policy Gradient算法实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#policy%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.</span> <span class="toc-text"> Policy网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E7%8A%B6%E6%80%81%E8%BE%93%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%80%89%E6%8B%A9%E5%8A%A8%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text"> 将状态输入神经网络，选择动作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.3.</span> <span class="toc-text"> 模型的训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-number">1.4.</span> <span class="toc-text"> 完整代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#dqn%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text"> DQN算法实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%E6%B1%A0"><span class="toc-number">2.1.</span> <span class="toc-text"> 经验回放池</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#q%E7%BD%91%E7%BB%9C"><span class="toc-number">2.2.</span> <span class="toc-text"> Q网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">2.3.</span> <span class="toc-text"> 模型训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81-2"><span class="toc-number">2.4.</span> <span class="toc-text"> 完整代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ddpg%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text"> DDPG算法实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%E6%B1%A0-2"><span class="toc-number">3.1.</span> <span class="toc-text"> 经验回放池</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9Cactor%E7%BD%91%E7%BB%9C"><span class="toc-number">3.2.</span> <span class="toc-text"> 策略网络（Actor网络）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0%E7%BD%91%E7%BB%9C-%E8%BE%93%E5%85%A5%E6%98%AFstateaction%E8%BE%93%E5%87%BA%E6%98%AFqsa"><span class="toc-number">3.3.</span> <span class="toc-text"> 值函数网络  输入是state，action输出是Q(s,a)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.4.</span> <span class="toc-text"> 整体实现</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符"><img src="/2021/10/16/Arithmetic-LeetCode/282/show.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Leetcode 282. 给表达式添加运算符"/></a><div class="content"><a class="title" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符">Leetcode 282. 给表达式添加运算符</a><time datetime="2021-10-16T15:35:16.000Z" title="发表于 2021-10-16 23:35:16">2021-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现"><img src="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/torch.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch强化学习算法实现"/></a><div class="content"><a class="title" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现">Pytorch强化学习算法实现</a><time datetime="2020-12-12T02:54:37.000Z" title="发表于 2020-12-12 10:54:37">2020-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块"><img src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyTorch常用工具模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块">PyTorch常用工具模块</a><time datetime="2020-12-09T13:32:23.000Z" title="发表于 2020-12-09 21:32:23">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块"><img src="/2020/12/09/Pytorch/Pytorch-and-torch-nn/torch.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中神经网络工具箱nn模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块">Pytorch中神经网络工具箱nn模块</a><time datetime="2020-12-09T13:25:38.000Z" title="发表于 2020-12-09 21:25:38">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd"><img src="/2020/12/09/Pytorch/Pytorch-and-Autograd/torch.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中的Autograd"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd">Pytorch中的Autograd</a><time datetime="2020-12-09T13:25:19.000Z" title="发表于 2020-12-09 21:25:19">2020-12-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/top.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By ccclll777</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>