<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Pytorch强化学习算法实现 | ccclll777&#39;s blogs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="使用Pytorch框架实现了强化学习算法Policy Gradient&#x2F;DQN&#x2F;DDGP">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch强化学习算法实现">
<meta property="og:url" content="http://yoursite.com/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="使用Pytorch框架实现了强化学习算法Policy Gradient&#x2F;DQN&#x2F;DDGP">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-12-12T02:54:37.000Z">
<meta property="article:modified_time" content="2021-10-16T09:33:23.234Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="python">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="ccclll777&#39;s blogs" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">ccclll777&#39;s blogs</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Pytorch/Pytorch-Reinforcement-Learning-Algorithm" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" class="article-date">
  <time datetime="2020-12-12T02:54:37.000Z" itemprop="datePublished">2020-12-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Pytorch强化学习算法实现
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>使用Pytorch框架实现了强化学习算法Policy Gradient/DQN/DDGP<br><span id="more"></span></p>
<h1 id="Policy-Gradient算法实现"><a href="#Policy-Gradient算法实现" class="headerlink" title="Policy Gradient算法实现"></a>Policy Gradient算法实现</h1><p>Policy Gradient算法的思想在<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">另一篇博客</a>中有介绍了，下面是算法的具体实现。</p>
<h2 id="Policy网络"><a href="#Policy网络" class="headerlink" title="Policy网络"></a>Policy网络</h2><p>两个线性层，中间使用Relu激活函数连接，最后连接softmax输出每个动作的概率。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,n_states_num,n_actions_num,hidden_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PolicyNet, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment">#存储轨迹</span></span><br><span class="line">        <span class="comment">#输入为长度为4的向量 输出为向左  向右两个动作</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=n_states_num, out_features=hidden_size, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=n_actions_num, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># 状态输入s的shape为向量：[4]</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h2 id="将状态输入神经网络，选择动作"><a href="#将状态输入神经网络，选择动作" class="headerlink" title="将状态输入神经网络，选择动作"></a>将状态输入神经网络，选择动作</h2><ul>
<li>这里给出了两种实现方式，具体思想就是输入环境的状态，传入policy网络，给出每一个动作的概率，我们需要选择出现概率最大的那个动作，以及他出现的概率值。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将状态传入神经网络 根据概率选择动作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span>  <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">    <span class="comment">#将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">    s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    prob = self.pi(s)  <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">    <span class="comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span></span><br><span class="line">    m = torch.distributions.Categorical(prob)  <span class="comment"># 生成分布</span></span><br><span class="line">    action = m.sample()</span><br><span class="line">    <span class="keyword">return</span> action.item() , m.log_prob(action)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action2</span>(<span class="params">self, state</span>):</span></span><br><span class="line">    <span class="comment"># 将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">    s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    prob = self.pi(s)  <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">    <span class="comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span></span><br><span class="line">    action =np.random.choice(<span class="built_in">range</span>(prob.shape[<span class="number">1</span>]),size=<span class="number">1</span>,p = prob.view(-<span class="number">1</span>).detach().numpy())[<span class="number">0</span>]</span><br><span class="line">    action = <span class="built_in">int</span>(action)</span><br><span class="line">    <span class="comment">#print(torch.log(prob[0][action]).unsqueeze(0))</span></span><br><span class="line">    <span class="keyword">return</span> action,torch.log(prob[<span class="number">0</span>][action]).unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>（1）使用<code>torch.distributions.Categorical</code>生成分布，然后进行选择</li>
<li>（2）使用<code>np.random.choice</code>进行采样<h2 id="模型的训练"><a href="#模型的训练" class="headerlink" title="模型的训练"></a>模型的训练</h2></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_net</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="comment"># 计算梯度并更新策略网络参数。tape为梯度记录器</span></span><br><span class="line">    R = <span class="number">0</span>  <span class="comment"># 终结状态的初始回报为0</span></span><br><span class="line">    policy_loss = []</span><br><span class="line">    <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::-<span class="number">1</span>]:  <span class="comment"># 逆序取</span></span><br><span class="line">        R = r + gamma * R  <span class="comment"># 计算每个时间戳上的回报</span></span><br><span class="line">        <span class="comment"># 每个时间戳都计算一次梯度</span></span><br><span class="line">        loss = -log_prob * R</span><br><span class="line">        policy_loss.append(loss)</span><br><span class="line">    self.optimizer.zero_grad()</span><br><span class="line">    policy_loss = torch.cat(policy_loss).<span class="built_in">sum</span>()  <span class="comment"># 求和</span></span><br><span class="line">    <span class="comment">#反向传播</span></span><br><span class="line">    policy_loss.backward()</span><br><span class="line">    self.optimizer.step()</span><br><span class="line">    self.cost_his.append(policy_loss.item())</span><br><span class="line">    self.data = []  <span class="comment"># 清空轨迹</span></span><br></pre></td></tr></table></figure>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> 	gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="comment"># Default parameters for plots</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.titlesize&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;KaiTi&#x27;</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">torch.manual_seed(<span class="number">2333</span>)    <span class="comment"># 策略梯度算法方差很大，设置seed以保证复现性</span></span><br><span class="line">print(<span class="string">&#x27;observation space:&#x27;</span>,env.observation_space)</span><br><span class="line">print(<span class="string">&#x27;action space:&#x27;</span>,env.action_space)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma         = <span class="number">0.98</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,n_states_num,n_actions_num,hidden_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PolicyNet, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment">#存储轨迹</span></span><br><span class="line">        <span class="comment">#输入为长度为4的向量 输出为向左  向右两个动作</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=n_states_num, out_features=hidden_size, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=n_actions_num, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># 状态输入s的shape为向量：[4]</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyGradient</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,n_states_num,n_actions_num,learning_rate=<span class="number">0.01</span>,reward_decay=<span class="number">0.95</span> </span>):</span></span><br><span class="line">        <span class="comment">#状态数   state是一个4维向量，分别是位置，速度，杆子的角度，加速度</span></span><br><span class="line">        self.n_states_num = n_states_num</span><br><span class="line">        <span class="comment">#action是二维、离散，即向左/右推杆子</span></span><br><span class="line">        self.n_actions_num = n_actions_num</span><br><span class="line">        <span class="comment">#学习率</span></span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        <span class="comment">#gamma</span></span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        <span class="comment">#网络</span></span><br><span class="line">        self.pi = PolicyNet(n_states_num, n_actions_num, <span class="number">128</span>)</span><br><span class="line">        <span class="comment">#优化器</span></span><br><span class="line">        self.optimizer = torch.optim.Adam(self.pi.parameters(), lr=learning_rate)</span><br><span class="line">        <span class="comment"># 存储轨迹  存储方式为  （每一次的reward，动作的概率）</span></span><br><span class="line">        self.data = []</span><br><span class="line">        self.cost_his = []</span><br><span class="line">    <span class="comment">#存储轨迹数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put_data</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        <span class="comment"># 记录r,log_P(a|s)z</span></span><br><span class="line">        self.data.append(item)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_net</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 计算梯度并更新策略网络参数。tape为梯度记录器</span></span><br><span class="line">        R = <span class="number">0</span>  <span class="comment"># 终结状态的初始回报为0</span></span><br><span class="line">        policy_loss = []</span><br><span class="line">        <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::-<span class="number">1</span>]:  <span class="comment"># 逆序取</span></span><br><span class="line">            R = r + gamma * R  <span class="comment"># 计算每个时间戳上的回报</span></span><br><span class="line">            <span class="comment"># 每个时间戳都计算一次梯度</span></span><br><span class="line">            loss = -log_prob * R</span><br><span class="line">            policy_loss.append(loss)</span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        policy_loss = torch.cat(policy_loss).<span class="built_in">sum</span>()  <span class="comment"># 求和</span></span><br><span class="line">        <span class="comment">#反向传播</span></span><br><span class="line">        policy_loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        self.cost_his.append(policy_loss.item())</span><br><span class="line">        self.data = []  <span class="comment"># 清空轨迹</span></span><br><span class="line">    <span class="comment">#将状态传入神经网络 根据概率选择动作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        <span class="comment">#将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">        s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        prob = self.pi(s)  <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">        <span class="comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span></span><br><span class="line">        m = torch.distributions.Categorical(prob)  <span class="comment"># 生成分布</span></span><br><span class="line">        action = m.sample()</span><br><span class="line">        <span class="keyword">return</span> action.item() , m.log_prob(action)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action2</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="comment"># 将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">        s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        prob = self.pi(s)  <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">        <span class="comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span></span><br><span class="line">        action =np.random.choice(<span class="built_in">range</span>(prob.shape[<span class="number">1</span>]),size=<span class="number">1</span>,p = prob.view(-<span class="number">1</span>).detach().numpy())[<span class="number">0</span>]</span><br><span class="line">        action = <span class="built_in">int</span>(action)</span><br><span class="line">        <span class="comment">#print(torch.log(prob[0][action]).unsqueeze(0))</span></span><br><span class="line">        <span class="keyword">return</span> action,torch.log(prob[<span class="number">0</span>][action]).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his)), self.cost_his)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    policyGradient = PolicyGradient(<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">    running_reward = <span class="number">10</span>  <span class="comment"># 计分</span></span><br><span class="line">    print_interval = <span class="number">20</span>  <span class="comment"># 打印间隔</span></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">        state = env.reset()  <span class="comment"># 回到游戏初始状态，返回s0</span></span><br><span class="line">        ep_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1001</span>):  <span class="comment"># CartPole-v1 forced to terminates at 1000 step.</span></span><br><span class="line">            <span class="comment">#根据状态 传入神经网络 选择动作</span></span><br><span class="line">            action ,log_prob  = policyGradient.choose_action2(state)</span><br><span class="line">            <span class="comment">#与环境交互</span></span><br><span class="line">            s_prime, reward, done, info = env.step(action)</span><br><span class="line">            <span class="comment"># s_prime, reward, done, info = env.step(action)</span></span><br><span class="line">            <span class="keyword">if</span> n_epi &gt; <span class="number">1000</span>:</span><br><span class="line">                env.render()</span><br><span class="line">            <span class="comment"># 记录动作a和动作产生的奖励r</span></span><br><span class="line">            <span class="comment"># prob shape:[1,2]</span></span><br><span class="line">            policyGradient.put_data((reward, log_prob))</span><br><span class="line">            state = s_prime  <span class="comment"># 刷新状态</span></span><br><span class="line">            ep_reward += reward</span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># 当前episode终止</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># episode终止后，训练一次网络</span></span><br><span class="line">        running_reward = <span class="number">0.05</span> * ep_reward + (<span class="number">1</span> - <span class="number">0.05</span>) * running_reward</span><br><span class="line">        <span class="comment">#交互完成后 进行学习</span></span><br><span class="line">        policyGradient.train_net()</span><br><span class="line">        <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&#x27;Episode &#123;&#125;\tLast reward: &#123;:.2f&#125;\tAverage reward: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                n_epi, ep_reward, running_reward))</span><br><span class="line">        <span class="keyword">if</span> running_reward &gt; env.spec.reward_threshold:  <span class="comment"># 大于游戏的最大阈值475时，退出游戏</span></span><br><span class="line">            print(<span class="string">&quot;Solved! Running reward is now &#123;&#125; and &quot;</span></span><br><span class="line">                  <span class="string">&quot;the last episode runs to &#123;&#125; time steps!&quot;</span>.<span class="built_in">format</span>(running_reward, t))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    policyGradient.plot_cost()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="DQN算法实现"><a href="#DQN算法实现" class="headerlink" title="DQN算法实现"></a>DQN算法实现</h1><p>DQN算法的思想在<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">另一篇博客</a>中有介绍了，下面是算法的具体实现。</p>
<h2 id="经验回放池"><a href="#经验回放池" class="headerlink" title="经验回放池"></a>经验回放池</h2><p>这里使用python的双向队列实现了经验回放池，实现了状态的存储以及随机采样。</p>
<ul>
<li>经验回放 Experience replay：由于在强化学习中，我们得到 的观测数据是有序的，用这样的数据去更新神经网络的参数会有问题（对比监督学习，数据之间都是独立的）。因此 DQN 中使 用经验回放，即用一个 Memory 来存储经历过的数据，每次更新参数的时候从 Memory 中抽取一部分的数据来用于更新，以此来打破数据间的关联。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></table></figure>
<h2 id="Q网络"><a href="#Q网络" class="headerlink" title="Q网络"></a>Q网络</h2><p>DQN使用神经网络取代了Q Table去预测Q（s，a）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,output_size,hidden_size</span>):</span></span><br><span class="line">        <span class="comment"># 创建Q网络，输入为状态向量，输出为动作的Q值</span></span><br><span class="line">        <span class="built_in">super</span>(Qnet, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size),</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>模型使用的两个Q网络，  在原来的 Q 网络的基础上又引入了一个 target Q 网络，即用来计算 target 的网络。它和 Q 网络结构一样， 初始的权重也一样，只是 Q 网络每次迭代都会更新，而 target Q 网络是每隔一段时间才会更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.q_target_net.load_state_dict(self.q_net.state_dict())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过Q网络和影子网络来构造贝尔曼方程的误差，</span></span><br><span class="line">        <span class="comment"># 并只更新Q网络，影子网络的更新会滞后Q网络</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>): <span class="comment"># 训练10次</span></span><br><span class="line">            s, a, r, s_prime = self.memory.sample(self.batch_size)</span><br><span class="line">            <span class="comment"># q_prime  用旧网络、动作后的环境预测，q_a 用新网络、动作前的环境；同时预测记忆中的情形</span></span><br><span class="line">            q_next, q_eval = self.q_target_net(s),self.q_net(s_prime)</span><br><span class="line">            <span class="comment"># 每次学习都用下一个状态的动作结合反馈作为当前动作值（这样，将未来状态的动作作为目标，有一定前瞻性）</span></span><br><span class="line">            q_target = q_eval</span><br><span class="line">            <span class="comment">#action的index值 它所在的位置</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#实际Q网络的值</span></span><br><span class="line">            act_index = np.array(a.tolist()).astype(np.int64)</span><br><span class="line">            act_index = torch.from_numpy(act_index)</span><br><span class="line">            q_a = q_eval.gather(<span class="number">1</span>, act_index)  <span class="comment"># 动作的概率值, [b,1]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#target Q网络的值</span></span><br><span class="line">            max_q_prime, _ = torch.<span class="built_in">max</span>(q_next, dim=<span class="number">1</span>)</span><br><span class="line">            max_q_prime = max_q_prime.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            q_target = r + self.gamma * max_q_prime</span><br><span class="line">            <span class="comment">#q_out[batch_index, eval_act_index] = reward + self.gamma * np.max(q_prime, axis=1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            loss = self.loss(q_a,q_target)</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line">            cost = loss.item()</span><br><span class="line"></span><br><span class="line">            self.cost_his.append(cost)</span><br><span class="line"></span><br><span class="line">            self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">            self.learn_step_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="完整代码-1"><a href="#完整代码-1" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">torch.manual_seed(<span class="number">2333</span>)    <span class="comment"># 策略梯度算法方差很大，设置seed以保证复现性</span></span><br><span class="line">print(<span class="string">&#x27;observation space:&#x27;</span>,env.observation_space)</span><br><span class="line">print(<span class="string">&#x27;action space:&#x27;</span>,env.action_space)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma = <span class="number">0.99</span></span><br><span class="line">buffer_limit = <span class="number">50000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,output_size,hidden_size</span>):</span></span><br><span class="line">        <span class="comment"># 创建Q网络，输入为状态向量，输出为动作的Q值</span></span><br><span class="line">        <span class="built_in">super</span>(Qnet, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,output_size,hidden_size,learning_rate,reward_decay,epsilon,e_greedy_increment,e_greedy</span>):</span></span><br><span class="line">        self.n_actions = output_size</span><br><span class="line">        self.n_features = input_size</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        self.epsilon = epsilon <span class="comment">#e - 贪心方式 参数</span></span><br><span class="line">        self.q_net = Qnet(input_size,output_size,hidden_size)</span><br><span class="line">        self.q_target_net = Qnet(input_size,output_size,hidden_size)  <span class="comment"># 创建影子网络</span></span><br><span class="line">        self.q_target_net.load_state_dict(self.q_net.state_dict()) <span class="comment"># 影子网络权值来自Q</span></span><br><span class="line">        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)<span class="comment">#优化器</span></span><br><span class="line">        self.buffer_limit = <span class="number">50000</span></span><br><span class="line">        self.batch_size = <span class="number">32</span></span><br><span class="line">        self.memory = ReplayBuffer()  <span class="comment">#创建回放池</span></span><br><span class="line">        <span class="comment"># Huber Loss常用于回归问题，其最大的特点是对离群点（outliers）、噪声不敏感，具有较强的鲁棒性</span></span><br><span class="line">        self.loss  = torch.nn.SmoothL1Loss()<span class="comment">#损失函数</span></span><br><span class="line">        self.cost_his = []</span><br><span class="line">        self.epsilon_increment =e_greedy_increment</span><br><span class="line">        self.learn_step_counter = <span class="number">0</span></span><br><span class="line">        self.epsilon_max = e_greedy</span><br><span class="line"></span><br><span class="line">        self.replace_target_iter = <span class="number">300</span></span><br><span class="line">    <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        <span class="comment"># 将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">        state = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 策略改进：e-贪心方式</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            actions_value = self.q_net(state)</span><br><span class="line">            action = np.argmax(actions_value.detach().numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    <span class="comment">#训练模型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.q_target_net.load_state_dict(self.q_net.state_dict())</span><br><span class="line">        <span class="comment"># 通过Q网络和影子网络来构造贝尔曼方程的误差，</span></span><br><span class="line">        <span class="comment"># 并只更新Q网络，影子网络的更新会滞后Q网络</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>): <span class="comment"># 训练10次</span></span><br><span class="line">            s, a, r, s_prime = self.memory.sample(self.batch_size)</span><br><span class="line">            <span class="comment"># q_prime  用旧网络、动作后的环境预测，q_a 用新网络、动作前的环境；同时预测记忆中的情形</span></span><br><span class="line">            q_next, q_eval = self.q_target_net(s),self.q_net(s_prime)</span><br><span class="line">            <span class="comment"># 每次学习都用下一个状态的动作结合反馈作为当前动作值（这样，将未来状态的动作作为目标，有一定前瞻性）</span></span><br><span class="line">            q_target = q_eval</span><br><span class="line">            <span class="comment">#action的index值 它所在的位置</span></span><br><span class="line">            <span class="comment">#实际Q网络的值</span></span><br><span class="line">            act_index = np.array(a.tolist()).astype(np.int64)</span><br><span class="line">            act_index = torch.from_numpy(act_index)</span><br><span class="line">            q_a = q_eval.gather(<span class="number">1</span>, act_index)  <span class="comment"># 动作的概率值, [b,1]</span></span><br><span class="line">            <span class="comment">#target Q网络的值</span></span><br><span class="line">            max_q_prime, _ = torch.<span class="built_in">max</span>(q_next, dim=<span class="number">1</span>)</span><br><span class="line">            max_q_prime = max_q_prime.unsqueeze(<span class="number">1</span>)</span><br><span class="line">            q_target = r + self.gamma * max_q_prime</span><br><span class="line">            loss = self.loss(q_a,q_target)</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line">            cost = loss.item()</span><br><span class="line"></span><br><span class="line">            self.cost_his.append(cost)</span><br><span class="line"></span><br><span class="line">            self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">            self.learn_step_counter += <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his)), self.cost_his)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># 创建环境</span></span><br><span class="line">    dqn = DQN(input_size = <span class="number">4</span>,</span><br><span class="line">              output_size = <span class="number">2</span>,</span><br><span class="line">              hidden_size = <span class="number">10</span>,</span><br><span class="line">              learning_rate = <span class="number">0.01</span>,</span><br><span class="line">              reward_decay=<span class="number">0.9</span>,epsilon=<span class="number">0.9</span>,</span><br><span class="line">              e_greedy_increment=<span class="number">0.001</span>,</span><br><span class="line">              e_greedy=<span class="number">0.9</span>,</span><br><span class="line">              )</span><br><span class="line"></span><br><span class="line">    print_interval = <span class="number">20</span></span><br><span class="line">    reword = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2000</span>):  <span class="comment"># 训练次数</span></span><br><span class="line">        <span class="comment"># epsilon概率也会8%到1%衰减，越到后面越使用Q值最大的动作</span></span><br><span class="line">        dqn.epsilon = <span class="built_in">max</span>(<span class="number">0.01</span>, <span class="number">0.08</span> - <span class="number">0.01</span> * (n_epi / <span class="number">200</span>))</span><br><span class="line">        state = env.reset()  <span class="comment"># 复位环境</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>: <span class="comment"># 一个回合最大时间戳</span></span><br><span class="line">            <span class="comment"># 根据当前Q网络提取策略，并改进策略</span></span><br><span class="line">            action = dqn.choose_action(state)</span><br><span class="line">            <span class="comment"># 使用改进的策略与环境交互</span></span><br><span class="line">            s_prime, r, done, info = env.step(action)</span><br><span class="line">            <span class="comment">#https://blog.csdn.net/u012465304/article/details/81172759</span></span><br><span class="line">            <span class="comment">#由于CartPole这个游戏的reward是只要杆子是立起来的，他reward就是1，失败就是0，</span></span><br><span class="line">            <span class="comment"># 显然这个reward对于连续性变量是不可以接受的，所以我们通过observation修改这个值。</span></span><br><span class="line">            <span class="comment"># 点击pycharm右上角的搜索符号搜索CartPole进入他环境的源代码中，再进入step函数，</span></span><br><span class="line">            <span class="comment"># 看到里面返回值state的定义</span></span><br><span class="line">            <span class="comment">#通过这四个值定义新的reward是</span></span><br><span class="line">            x, x_dot, theta, theta_dot = s_prime</span><br><span class="line">            r1 = (env.x_threshold - <span class="built_in">abs</span>(x)) / env.x_threshold - <span class="number">0.8</span></span><br><span class="line">            r2 = (env.theta_threshold_radians - <span class="built_in">abs</span>(theta)) / env.theta_threshold_radians - <span class="number">0.5</span></span><br><span class="line">            reward = r1 + r2</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 保存四元组</span></span><br><span class="line">            dqn.memory.put((state, action,reward,s_prime))</span><br><span class="line">            state = s_prime</span><br><span class="line">            reword +=reward</span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># 回合结束</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> dqn.memory.size() &gt; <span class="number">1000</span>:  <span class="comment"># 缓冲池只有大于2000就可以训练</span></span><br><span class="line">                dqn.train()</span><br><span class="line">        <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span> <span class="keyword">and</span> n_epi != <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&quot;# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, &quot;</span> \</span><br><span class="line">                  <span class="string">&quot;epsilon : &#123;:.1f&#125;%&quot;</span> \</span><br><span class="line">                  .<span class="built_in">format</span>(n_epi, reword / print_interval, dqn.memory.size(), dqn.epsilon * <span class="number">100</span>))</span><br><span class="line">            reword = <span class="number">0.0</span></span><br><span class="line">    env.close()</span><br><span class="line">    dqn.plot_cost()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="DDPG算法实现"><a href="#DDPG算法实现" class="headerlink" title="DDPG算法实现"></a>DDPG算法实现</h1><p>DDPG算法的思想在<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">另一篇博客</a>中有介绍了，下面是算法的具体实现。</p>
<p>DDPG 可以解决连续动作空间问题，并且是actor-critic方法，即既有值函数网络(critic)，又有策略网络(actor)。</p>
<h2 id="经验回放池-1"><a href="#经验回放池-1" class="headerlink" title="经验回放池"></a>经验回放池</h2><p>与DQN中的经验回放池的实现相同<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        buffer_limit = <span class="number">50000</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></table></figure></p>
<h2 id="策略网络（Actor网络）"><a href="#策略网络（Actor网络）" class="headerlink" title="策略网络（Actor网络）"></a>策略网络（Actor网络）</h2><p>输入为state  输出为概率分布pi(a|s)（每个动作出现的概率）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 策略网络，也叫Actor网络，输入为state  输出为概率分布pi(a|s)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,hidden_size,output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># self.linear  = nn.Linear(hidden_size, output_size)</span></span><br><span class="line">        self.actor_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=output_size)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        x = self.actor_net(state)</span><br><span class="line">        x = torch.tanh(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="值函数网络-输入是state，action输出是Q-s-a"><a href="#值函数网络-输入是state，action输出是Q-s-a" class="headerlink" title="值函数网络  输入是state，action输出是Q(s,a)"></a>值函数网络  输入是state，action输出是Q(s,a)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#值函数网络  输入是state，action输出是Q(s,a)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        self.critic_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state,action</span>):</span></span><br><span class="line">        inputs = torch.cat([state,action],<span class="number">1</span>)</span><br><span class="line">        x = self.critic_net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="整体实现"><a href="#整体实现" class="headerlink" title="整体实现"></a>整体实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span>  collections</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;Pendulum-v0&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">torch.manual_seed(<span class="number">2333</span>)    <span class="comment"># 策略梯度算法方差很大，设置seed以保证复现性</span></span><br><span class="line">env.reset()</span><br><span class="line">env.render()</span><br><span class="line">print(<span class="string">&#x27;observation space:&#x27;</span>,env.observation_space)</span><br><span class="line">print(<span class="string">&#x27;action space:&#x27;</span>,env.action_space)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        buffer_limit = <span class="number">50000</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 策略网络，也叫Actor网络，输入为state  输出为概率分布pi(a|s)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,hidden_size,output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># self.linear  = nn.Linear(hidden_size, output_size)</span></span><br><span class="line">        self.actor_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=output_size)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        x = self.actor_net(state)</span><br><span class="line">        x = torch.tanh(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#值函数网络  输入是state，action输出是Q(s,a)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        self.critic_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state,action</span>):</span></span><br><span class="line">        inputs = torch.cat([state,action],<span class="number">1</span>)</span><br><span class="line">        x = self.critic_net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPG</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,state_size,action_size,hidden_size = <span class="number">256</span>,actor_lr = <span class="number">0.001</span>,ctitic_lr = <span class="number">0.001</span>,batch_size = <span class="number">32</span></span>):</span></span><br><span class="line"></span><br><span class="line">        self.state_size = state_size</span><br><span class="line">        self.action_size = action_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.actor_lr = actor_lr <span class="comment">#actor网络学习率</span></span><br><span class="line">        self.critic_lr = ctitic_lr<span class="comment">#critic网络学习率</span></span><br><span class="line">        <span class="comment"># 策略网络，也叫Actor网络，输入为state  输出为概率分布pi(a|s)</span></span><br><span class="line">        self.actor = Actor(self.state_size, self.hidden_size, self.action_size)</span><br><span class="line">        <span class="comment">#target actor网络 延迟更新</span></span><br><span class="line">        self.actor_target = Actor(self.state_size, self.hidden_size, self.action_size)</span><br><span class="line">        <span class="comment"># 值函数网络  输入是state，action输出是Q(s,a)</span></span><br><span class="line">        self.critic = Critic(self.state_size + self.action_size, self.hidden_size, self.action_size)</span><br><span class="line">        self.critic_target = Critic(self.state_size + self.action_size, self.hidden_size, self.action_size)</span><br><span class="line"></span><br><span class="line">        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.actor_lr)</span><br><span class="line">        self.critic_optim = optim.Adam(self.critic.parameters(), lr=self.critic_lr)</span><br><span class="line">        self.buffer = []</span><br><span class="line">        <span class="comment"># 影子网络权值来自原网络，只不过延迟更新</span></span><br><span class="line">        self.actor_target.load_state_dict(self.actor.state_dict())</span><br><span class="line">        self.critic_target.load_state_dict(self.critic.state_dict())</span><br><span class="line">        self.gamma = <span class="number">0.99</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.memory = ReplayBuffer()  <span class="comment"># 创建回放池</span></span><br><span class="line"></span><br><span class="line">        self.memory2 = []</span><br><span class="line">        self.learn_step_counter = <span class="number">0</span> <span class="comment">#学习轮数 与影子网络的更新有关</span></span><br><span class="line">        self.replace_target_iter = <span class="number">200</span> <span class="comment">#影子网络迭代多少轮更新一次</span></span><br><span class="line">        self.cost_his_actor = []<span class="comment"># 存储cost 准备画图</span></span><br><span class="line">        self.cost_his_critic = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        <span class="comment"># 将state转化成tensor 并且维度转化为[3]-&gt;[1,3]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">        state = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        action = self.actor(state).squeeze(<span class="number">0</span>).detach().numpy()</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    <span class="comment">#critic网络的学习</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">critic_learn</span>(<span class="params">self,s0,a0,r1,s1</span>):</span></span><br><span class="line">        <span class="comment">#从actor_target通过状态获取对应的动作  detach()将tensor从计算图上剥离</span></span><br><span class="line">        a1 = self.actor_target(s0).detach()</span><br><span class="line">        <span class="comment">#删减一个维度  [b,1,1]变成[b,1]</span></span><br><span class="line">        a0 = a0.squeeze(<span class="number">2</span>)</span><br><span class="line">        y_pred = self.critic(s0,a0)</span><br><span class="line">        y_target = r1 +self.gamma *self.critic_target(s1,a1).detach()</span><br><span class="line">        loss_fn = nn.MSELoss()</span><br><span class="line">        loss = loss_fn(y_pred, y_target)</span><br><span class="line">        self.critic_optim.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.critic_optim.step()</span><br><span class="line">        self.cost_his_critic.append(loss.item())</span><br><span class="line">    <span class="comment">#actor网络的学习</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">actor_learn</span>(<span class="params">self,s0,a0,r1,s1</span>):</span></span><br><span class="line">        loss = -torch.mean(self.critic(s0, self.actor(s0)))</span><br><span class="line">        self.actor_optim.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.actor_optim.step()</span><br><span class="line">        self.cost_his_actor.append(loss.item())</span><br><span class="line">    <span class="comment">#模型的训练</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.actor_target.load_state_dict(self.actor.state_dict())</span><br><span class="line">            self.critic_target.load_state_dict(self.critic.state_dict())</span><br><span class="line">        <span class="comment">#随机采样出 batch_size 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">        s0, a0, r, s_prime = self.memory.sample(self.batch_size)</span><br><span class="line">        self.critic_learn(s0, a0, r, s_prime)</span><br><span class="line">        self.actor_learn(s0, a0, r, s_prime)</span><br><span class="line"></span><br><span class="line">        self.soft_update(self.critic_target, self.critic, <span class="number">0.02</span>)</span><br><span class="line">        self.soft_update(self.actor_target, self.actor, <span class="number">0.02</span>)</span><br><span class="line">    <span class="comment">#target网络的更新</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">soft_update</span>(<span class="params">self,net_target, net, tau</span>):</span></span><br><span class="line">        <span class="keyword">for</span> target_param, param <span class="keyword">in</span> <span class="built_in">zip</span>(net_target.parameters(), net.parameters()):</span><br><span class="line">            target_param.data.copy_(target_param.data * (<span class="number">1.0</span> - tau) + param.data * tau)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his_critic)), self.cost_his_critic)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    print(env.observation_space.shape[<span class="number">0</span>])</span><br><span class="line">    print(env.action_space.shape[<span class="number">0</span>])</span><br><span class="line">    ddgp = DDPG(state_size=env.observation_space.shape[<span class="number">0</span>],</span><br><span class="line">                action_size=env.action_space.shape[<span class="number">0</span>],</span><br><span class="line">                hidden_size=<span class="number">256</span>,</span><br><span class="line">                actor_lr=<span class="number">0.001</span>,</span><br><span class="line">                ctitic_lr=  <span class="number">0.001</span>,</span><br><span class="line">                batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">    print_interval = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        state = env.reset()</span><br><span class="line">        episode_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">            env.render()</span><br><span class="line">            action0 = ddgp.choose_action(state)</span><br><span class="line">            s_prime, r, done, info = env.step(action0)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 保存四元组</span></span><br><span class="line">            ddgp.memory.put((state, action0, r, s_prime))</span><br><span class="line">            episode_reward += r</span><br><span class="line">            state = s_prime</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># 回合结束</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ddgp.memory.size() &gt; <span class="number">32</span>:  <span class="comment"># 缓冲池只有大于500就可以训练</span></span><br><span class="line">                ddgp.train()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> episode % print_interval == <span class="number">0</span> <span class="keyword">and</span> episode != <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&quot;# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, &quot;</span></span><br><span class="line">                  .<span class="built_in">format</span>(episode, episode_reward / print_interval, ddgp.memory.size()))</span><br><span class="line">    env.close()</span><br><span class="line">    ddgp.plot_cost()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" data-id="ckutyt6pa006hav0ae5ke6om4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="tag">深度学习框架</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/10/16/Arithmetic-LeetCode/282/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          282
        
      </div>
    </a>
  
  
    <a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">PyTorch常用工具模块</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/PTA%E7%94%B2%E7%BA%A7%E5%88%B7%E9%A2%98/">PTA甲级刷题</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode%E5%88%B7%E9%A2%98/">leetcode刷题</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/">爬虫学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/">论文复现</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/">课程设计</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/" rel="tag">c++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/django/" rel="tag">django</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyquery/" rel="tag">pyquery</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/springboot/" rel="tag">springboot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vue/" rel="tag">vue</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" rel="tag">动态规划</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E6%BA%AF/" rel="tag">回溯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE/" rel="tag">图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/" rel="tag">多文档摘要</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" rel="tag">字符串</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" rel="tag">操作系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%91/" rel="tag">树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E6%8B%9F/" rel="tag">模拟</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82/" rel="tag">模拟网络请求</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="tag">深度学习框架</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90/" rel="tag">网页解析</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/c/" style="font-size: 17.78px;">c++</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/java/" style="font-size: 14.44px;">java</a> <a href="/tags/pyquery/" style="font-size: 10px;">pyquery</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/pytorch/" style="font-size: 15.56px;">pytorch</a> <a href="/tags/springboot/" style="font-size: 10px;">springboot</a> <a href="/tags/tensorflow/" style="font-size: 16.67px;">tensorflow</a> <a href="/tags/vue/" style="font-size: 11.11px;">vue</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 10px;">动态规划</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size: 10px;">回溯</a> <a href="/tags/%E5%9B%BE/" style="font-size: 10px;">图</a> <a href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/" style="font-size: 10px;">多文档摘要</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 10px;">字符串</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">强化学习</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 13.33px;">操作系统</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 11.11px;">数据库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%A0%91/" style="font-size: 12.22px;">树</a> <a href="/tags/%E6%A8%A1%E6%8B%9F/" style="font-size: 10px;">模拟</a> <a href="/tags/%E6%A8%A1%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82/" style="font-size: 11.11px;">模拟网络请求</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 14.44px;">深度学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" style="font-size: 18.89px;">深度学习框架</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 15.56px;">爬虫</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 17.78px;">算法</a> <a href="/tags/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90/" style="font-size: 10px;">网页解析</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/10/16/Arithmetic-LeetCode/282/">282</a>
          </li>
        
          <li>
            <a href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/">Pytorch强化学习算法实现</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/">PyTorch常用工具模块</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/">Pytorch中神经网络工具箱nn模块</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/Pytorch-and-Autograd/">Pytorch中的Autograd</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 ccclll777<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>