<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç° | ccclll777's blogs</title><meta name="keywords" content="python,æ·±åº¦å­¦ä¹ æ¡†æ¶,pytorch"><meta name="author" content="ccclll777"><meta name="copyright" content="ccclll777"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ä½¿ç”¨Pytorchæ¡†æ¶å®ç°äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•Policy Gradient&#x2F;DQN&#x2F;DDGP">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°">
<meta property="og:url" content="http://yoursite.com/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="ä½¿ç”¨Pytorchæ¡†æ¶å®ç°äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•Policy Gradient&#x2F;DQN&#x2F;DDGP">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png">
<meta property="article:published_time" content="2020-12-12T02:54:37.000Z">
<meta property="article:modified_time" content="2021-10-17T01:36:25.739Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="python">
<meta property="article:tag" content="æ·±åº¦å­¦ä¹ æ¡†æ¶">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png"><link rel="shortcut icon" href="/images/avatar.png"><link rel="canonical" href="http://yoursite.com/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"ç¹","msgToSimplifiedChinese":"ç°¡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'å¤©',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-10-17 09:36:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">45</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">26</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">9</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ccclll777's blogs</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2020-12-12T02:54:37.000Z" title="å‘è¡¨äº 2020-12-12 10:54:37">2020-12-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2021-10-17T01:36:25.739Z" title="æ›´æ–°äº 2021-10-17 09:36:25">2021-10-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">å¼ºåŒ–å­¦ä¹ </a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>23åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>ä½¿ç”¨Pytorchæ¡†æ¶å®ç°äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•Policy Gradient/DQN/DDGP</p>
<span id="more"></span>
<h1 id="policy-gradientç®—æ³•å®ç°"><a class="markdownIt-Anchor" href="#policy-gradientç®—æ³•å®ç°"></a> Policy Gradientç®—æ³•å®ç°</h1>
<p>Policy Gradientç®—æ³•çš„æ€æƒ³åœ¨<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">å¦ä¸€ç¯‡åšå®¢</a>ä¸­æœ‰ä»‹ç»äº†ï¼Œä¸‹é¢æ˜¯ç®—æ³•çš„å…·ä½“å®ç°ã€‚</p>
<h2 id="policyç½‘ç»œ"><a class="markdownIt-Anchor" href="#policyç½‘ç»œ"></a> Policyç½‘ç»œ</h2>
<p>ä¸¤ä¸ªçº¿æ€§å±‚ï¼Œä¸­é—´ä½¿ç”¨Reluæ¿€æ´»å‡½æ•°è¿æ¥ï¼Œæœ€åè¿æ¥softmaxè¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,n_states_num,n_actions_num,hidden_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PolicyNet, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment">#å­˜å‚¨è½¨è¿¹</span></span><br><span class="line">        <span class="comment">#è¾“å…¥ä¸ºé•¿åº¦ä¸º4çš„å‘é‡ è¾“å‡ºä¸ºå‘å·¦  å‘å³ä¸¤ä¸ªåŠ¨ä½œ</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=n_states_num, out_features=hidden_size, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=n_actions_num, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># çŠ¶æ€è¾“å…¥sçš„shapeä¸ºå‘é‡ï¼š[4]</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="å°†çŠ¶æ€è¾“å…¥ç¥ç»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ"><a class="markdownIt-Anchor" href="#å°†çŠ¶æ€è¾“å…¥ç¥ç»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ"></a> å°†çŠ¶æ€è¾“å…¥ç¥ç»ç½‘ç»œï¼Œé€‰æ‹©åŠ¨ä½œ</h2>
<ul>
<li>è¿™é‡Œç»™å‡ºäº†ä¸¤ç§å®ç°æ–¹å¼ï¼Œå…·ä½“æ€æƒ³å°±æ˜¯è¾“å…¥ç¯å¢ƒçš„çŠ¶æ€ï¼Œä¼ å…¥policyç½‘ç»œï¼Œç»™å‡ºæ¯ä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©å‡ºç°æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªåŠ¨ä½œï¼Œä»¥åŠä»–å‡ºç°çš„æ¦‚ç‡å€¼ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#å°†çŠ¶æ€ä¼ å…¥ç¥ç»ç½‘ç»œ æ ¹æ®æ¦‚ç‡é€‰æ‹©åŠ¨ä½œ</span></span><br><span class="line"><span class="function"><span class="keyword">def</span>  <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">    <span class="comment">#å°†stateè½¬åŒ–æˆtensor å¹¶ä¸”ç»´åº¦è½¬åŒ–ä¸º[4]-&gt;[1,4]  unsqueeze(0)åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šç”°é—´</span></span><br><span class="line">    s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    prob = self.pi(s)  <span class="comment"># åŠ¨ä½œåˆ†å¸ƒ:[1,2]</span></span><br><span class="line">    <span class="comment"># ä»ç±»åˆ«åˆ†å¸ƒä¸­é‡‡æ ·1ä¸ªåŠ¨ä½œ, shape: [1] torch.log(prob), 1</span></span><br><span class="line">    m = torch.distributions.Categorical(prob)  <span class="comment"># ç”Ÿæˆåˆ†å¸ƒ</span></span><br><span class="line">    action = m.sample()</span><br><span class="line">    <span class="keyword">return</span> action.item() , m.log_prob(action)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action2</span>(<span class="params">self, state</span>):</span></span><br><span class="line">    <span class="comment"># å°†stateè½¬åŒ–æˆtensor å¹¶ä¸”ç»´åº¦è½¬åŒ–ä¸º[4]-&gt;[1,4]  unsqueeze(0)åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šç”°é—´</span></span><br><span class="line">    s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    prob = self.pi(s)  <span class="comment"># åŠ¨ä½œåˆ†å¸ƒ:[1,2]</span></span><br><span class="line">    <span class="comment"># ä»ç±»åˆ«åˆ†å¸ƒä¸­é‡‡æ ·1ä¸ªåŠ¨ä½œ, shape: [1] torch.log(prob), 1</span></span><br><span class="line">    action =np.random.choice(<span class="built_in">range</span>(prob.shape[<span class="number">1</span>]),size=<span class="number">1</span>,p = prob.view(-<span class="number">1</span>).detach().numpy())[<span class="number">0</span>]</span><br><span class="line">    action = <span class="built_in">int</span>(action)</span><br><span class="line">    <span class="comment">#print(torch.log(prob[0][action]).unsqueeze(0))</span></span><br><span class="line">    <span class="keyword">return</span> action,torch.log(prob[<span class="number">0</span>][action]).unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>ï¼ˆ1ï¼‰ä½¿ç”¨<code>torch.distributions.Categorical</code>ç”Ÿæˆåˆ†å¸ƒï¼Œç„¶åè¿›è¡Œé€‰æ‹©</li>
<li>ï¼ˆ2ï¼‰ä½¿ç”¨<code>np.random.choice</code>è¿›è¡Œé‡‡æ ·</li>
</ul>
<h2 id="æ¨¡å‹çš„è®­ç»ƒ"><a class="markdownIt-Anchor" href="#æ¨¡å‹çš„è®­ç»ƒ"></a> æ¨¡å‹çš„è®­ç»ƒ</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_net</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="comment"># è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç­–ç•¥ç½‘ç»œå‚æ•°ã€‚tapeä¸ºæ¢¯åº¦è®°å½•å™¨</span></span><br><span class="line">    R = <span class="number">0</span>  <span class="comment"># ç»ˆç»“çŠ¶æ€çš„åˆå§‹å›æŠ¥ä¸º0</span></span><br><span class="line">    policy_loss = []</span><br><span class="line">    <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::-<span class="number">1</span>]:  <span class="comment"># é€†åºå–</span></span><br><span class="line">        R = r + gamma * R  <span class="comment"># è®¡ç®—æ¯ä¸ªæ—¶é—´æˆ³ä¸Šçš„å›æŠ¥</span></span><br><span class="line">        <span class="comment"># æ¯ä¸ªæ—¶é—´æˆ³éƒ½è®¡ç®—ä¸€æ¬¡æ¢¯åº¦</span></span><br><span class="line">        loss = -log_prob * R</span><br><span class="line">        policy_loss.append(loss)</span><br><span class="line">    self.optimizer.zero_grad()</span><br><span class="line">    policy_loss = torch.cat(policy_loss).<span class="built_in">sum</span>()  <span class="comment"># æ±‚å’Œ</span></span><br><span class="line">    <span class="comment">#åå‘ä¼ æ’­</span></span><br><span class="line">    policy_loss.backward()</span><br><span class="line">    self.optimizer.step()</span><br><span class="line">    self.cost_his.append(policy_loss.item())</span><br><span class="line">    self.data = []  <span class="comment"># æ¸…ç©ºè½¨è¿¹</span></span><br></pre></td></tr></table></figure>
<h2 id="å®Œæ•´ä»£ç "><a class="markdownIt-Anchor" href="#å®Œæ•´ä»£ç "></a> å®Œæ•´ä»£ç </h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> 	gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="comment"># Default parameters for plots</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.titlesize&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;KaiTi&#x27;</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">torch.manual_seed(<span class="number">2333</span>)    <span class="comment"># ç­–ç•¥æ¢¯åº¦ç®—æ³•æ–¹å·®å¾ˆå¤§ï¼Œè®¾ç½®seedä»¥ä¿è¯å¤ç°æ€§</span></span><br><span class="line">print(<span class="string">&#x27;observation space:&#x27;</span>,env.observation_space)</span><br><span class="line">print(<span class="string">&#x27;action space:&#x27;</span>,env.action_space)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma         = <span class="number">0.98</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,n_states_num,n_actions_num,hidden_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PolicyNet, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment">#å­˜å‚¨è½¨è¿¹</span></span><br><span class="line">        <span class="comment">#è¾“å…¥ä¸ºé•¿åº¦ä¸º4çš„å‘é‡ è¾“å‡ºä¸ºå‘å·¦  å‘å³ä¸¤ä¸ªåŠ¨ä½œ</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=n_states_num, out_features=hidden_size, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=n_actions_num, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># çŠ¶æ€è¾“å…¥sçš„shapeä¸ºå‘é‡ï¼š[4]</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyGradient</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,n_states_num,n_actions_num,learning_rate=<span class="number">0.01</span>,reward_decay=<span class="number">0.95</span> </span>):</span></span><br><span class="line">        <span class="comment">#çŠ¶æ€æ•°   stateæ˜¯ä¸€ä¸ª4ç»´å‘é‡ï¼Œåˆ†åˆ«æ˜¯ä½ç½®ï¼Œé€Ÿåº¦ï¼Œæ†å­çš„è§’åº¦ï¼ŒåŠ é€Ÿåº¦</span></span><br><span class="line">        self.n_states_num = n_states_num</span><br><span class="line">        <span class="comment">#actionæ˜¯äºŒç»´ã€ç¦»æ•£ï¼Œå³å‘å·¦/å³æ¨æ†å­</span></span><br><span class="line">        self.n_actions_num = n_actions_num</span><br><span class="line">        <span class="comment">#å­¦ä¹ ç‡</span></span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        <span class="comment">#gamma</span></span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        <span class="comment">#ç½‘ç»œ</span></span><br><span class="line">        self.pi = PolicyNet(n_states_num, n_actions_num, <span class="number">128</span>)</span><br><span class="line">        <span class="comment">#ä¼˜åŒ–å™¨</span></span><br><span class="line">        self.optimizer = torch.optim.Adam(self.pi.parameters(), lr=learning_rate)</span><br><span class="line">        <span class="comment"># å­˜å‚¨è½¨è¿¹  å­˜å‚¨æ–¹å¼ä¸º  ï¼ˆæ¯ä¸€æ¬¡çš„rewardï¼ŒåŠ¨ä½œçš„æ¦‚ç‡ï¼‰</span></span><br><span class="line">        self.data = []</span><br><span class="line">        self.cost_his = []</span><br><span class="line">    <span class="comment">#å­˜å‚¨è½¨è¿¹æ•°æ®</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put_data</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        <span class="comment"># è®°å½•r,log_P(a|s)z</span></span><br><span class="line">        self.data.append(item)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_net</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç­–ç•¥ç½‘ç»œå‚æ•°ã€‚tapeä¸ºæ¢¯åº¦è®°å½•å™¨</span></span><br><span class="line">        R = <span class="number">0</span>  <span class="comment"># ç»ˆç»“çŠ¶æ€çš„åˆå§‹å›æŠ¥ä¸º0</span></span><br><span class="line">        policy_loss = []</span><br><span class="line">        <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::-<span class="number">1</span>]:  <span class="comment"># é€†åºå–</span></span><br><span class="line">            R = r + gamma * R  <span class="comment"># è®¡ç®—æ¯ä¸ªæ—¶é—´æˆ³ä¸Šçš„å›æŠ¥</span></span><br><span class="line">            <span class="comment"># æ¯ä¸ªæ—¶é—´æˆ³éƒ½è®¡ç®—ä¸€æ¬¡æ¢¯åº¦</span></span><br><span class="line">            loss = -log_prob * R</span><br><span class="line">            policy_loss.append(loss)</span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        policy_loss = torch.cat(policy_loss).<span class="built_in">sum</span>()  <span class="comment"># æ±‚å’Œ</span></span><br><span class="line">        <span class="comment">#åå‘ä¼ æ’­</span></span><br><span class="line">        policy_loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        self.cost_his.append(policy_loss.item())</span><br><span class="line">        self.data = []  <span class="comment"># æ¸…ç©ºè½¨è¿¹</span></span><br><span class="line">    <span class="comment">#å°†çŠ¶æ€ä¼ å…¥ç¥ç»ç½‘ç»œ æ ¹æ®æ¦‚ç‡é€‰æ‹©åŠ¨ä½œ</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        <span class="comment">#å°†stateè½¬åŒ–æˆtensor å¹¶ä¸”ç»´åº¦è½¬åŒ–ä¸º[4]-&gt;[1,4]  unsqueeze(0)åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šç”°é—´</span></span><br><span class="line">        s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        prob = self.pi(s)  <span class="comment"># åŠ¨ä½œåˆ†å¸ƒ:[1,2]</span></span><br><span class="line">        <span class="comment"># ä»ç±»åˆ«åˆ†å¸ƒä¸­é‡‡æ ·1ä¸ªåŠ¨ä½œ, shape: [1] torch.log(prob), 1</span></span><br><span class="line">        m = torch.distributions.Categorical(prob)  <span class="comment"># ç”Ÿæˆåˆ†å¸ƒ</span></span><br><span class="line">        action = m.sample()</span><br><span class="line">        <span class="keyword">return</span> action.item() , m.log_prob(action)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action2</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="comment"># å°†stateè½¬åŒ–æˆtensor å¹¶ä¸”ç»´åº¦è½¬åŒ–ä¸º[4]-&gt;[1,4]  unsqueeze(0)åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šç”°é—´</span></span><br><span class="line">        s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        prob = self.pi(s)  <span class="comment"># åŠ¨ä½œåˆ†å¸ƒ:[1,2]</span></span><br><span class="line">        <span class="comment"># ä»ç±»åˆ«åˆ†å¸ƒä¸­é‡‡æ ·1ä¸ªåŠ¨ä½œ, shape: [1] torch.log(prob), 1</span></span><br><span class="line">        action =np.random.choice(<span class="built_in">range</span>(prob.shape[<span class="number">1</span>]),size=<span class="number">1</span>,p = prob.view(-<span class="number">1</span>).detach().numpy())[<span class="number">0</span>]</span><br><span class="line">        action = <span class="built_in">int</span>(action)</span><br><span class="line">        <span class="comment">#print(torch.log(prob[0][action]).unsqueeze(0))</span></span><br><span class="line">        <span class="keyword">return</span> action,torch.log(prob[<span class="number">0</span>][action]).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his)), self.cost_his)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    policyGradient = PolicyGradient(<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">    running_reward = <span class="number">10</span>  <span class="comment"># è®¡åˆ†</span></span><br><span class="line">    print_interval = <span class="number">20</span>  <span class="comment"># æ‰“å°é—´éš”</span></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">        state = env.reset()  <span class="comment"># å›åˆ°æ¸¸æˆåˆå§‹çŠ¶æ€ï¼Œè¿”å›s0</span></span><br><span class="line">        ep_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1001</span>):  <span class="comment"># CartPole-v1 forced to terminates at 1000 step.</span></span><br><span class="line">            <span class="comment">#æ ¹æ®çŠ¶æ€ ä¼ å…¥ç¥ç»ç½‘ç»œ é€‰æ‹©åŠ¨ä½œ</span></span><br><span class="line">            action ,log_prob  = policyGradient.choose_action2(state)</span><br><span class="line">            <span class="comment">#ä¸ç¯å¢ƒäº¤äº’</span></span><br><span class="line">            s_prime, reward, done, info = env.step(action)</span><br><span class="line">            <span class="comment"># s_prime, reward, done, info = env.step(action)</span></span><br><span class="line">            <span class="keyword">if</span> n_epi &gt; <span class="number">1000</span>:</span><br><span class="line">                env.render()</span><br><span class="line">            <span class="comment"># è®°å½•åŠ¨ä½œaå’ŒåŠ¨ä½œäº§ç”Ÿçš„å¥–åŠ±r</span></span><br><span class="line">            <span class="comment"># prob shape:[1,2]</span></span><br><span class="line">            policyGradient.put_data((reward, log_prob))</span><br><span class="line">            state = s_prime  <span class="comment"># åˆ·æ–°çŠ¶æ€</span></span><br><span class="line">            ep_reward += reward</span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># å½“å‰episodeç»ˆæ­¢</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># episodeç»ˆæ­¢åï¼Œè®­ç»ƒä¸€æ¬¡ç½‘ç»œ</span></span><br><span class="line">        running_reward = <span class="number">0.05</span> * ep_reward + (<span class="number">1</span> - <span class="number">0.05</span>) * running_reward</span><br><span class="line">        <span class="comment">#äº¤äº’å®Œæˆå è¿›è¡Œå­¦ä¹ </span></span><br><span class="line">        policyGradient.train_net()</span><br><span class="line">        <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&#x27;Episode &#123;&#125;\tLast reward: &#123;:.2f&#125;\tAverage reward: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                n_epi, ep_reward, running_reward))</span><br><span class="line">        <span class="keyword">if</span> running_reward &gt; env.spec.reward_threshold:  <span class="comment"># å¤§äºæ¸¸æˆçš„æœ€å¤§é˜ˆå€¼475æ—¶ï¼Œé€€å‡ºæ¸¸æˆ</span></span><br><span class="line">            print(<span class="string">&quot;Solved! Running reward is now &#123;&#125; and &quot;</span></span><br><span class="line">                  <span class="string">&quot;the last episode runs to &#123;&#125; time steps!&quot;</span>.<span class="built_in">format</span>(running_reward, t))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    policyGradient.plot_cost()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="dqnç®—æ³•å®ç°"><a class="markdownIt-Anchor" href="#dqnç®—æ³•å®ç°"></a> DQNç®—æ³•å®ç°</h1>
<p>DQNç®—æ³•çš„æ€æƒ³åœ¨<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">å¦ä¸€ç¯‡åšå®¢</a>ä¸­æœ‰ä»‹ç»äº†ï¼Œä¸‹é¢æ˜¯ç®—æ³•çš„å…·ä½“å®ç°ã€‚</p>
<h2 id="ç»éªŒå›æ”¾æ± "><a class="markdownIt-Anchor" href="#ç»éªŒå›æ”¾æ± "></a> ç»éªŒå›æ”¾æ± </h2>
<p>è¿™é‡Œä½¿ç”¨pythonçš„åŒå‘é˜Ÿåˆ—å®ç°äº†ç»éªŒå›æ”¾æ± ï¼Œå®ç°äº†çŠ¶æ€çš„å­˜å‚¨ä»¥åŠéšæœºé‡‡æ ·ã€‚</p>
<ul>
<li>ç»éªŒå›æ”¾ Experience replayï¼šç”±äºåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ° çš„è§‚æµ‹æ•°æ®æ˜¯æœ‰åºçš„ï¼Œç”¨è¿™æ ·çš„æ•°æ®å»æ›´æ–°ç¥ç»ç½‘ç»œçš„å‚æ•°ä¼šæœ‰é—®é¢˜ï¼ˆå¯¹æ¯”ç›‘ç£å­¦ä¹ ï¼Œæ•°æ®ä¹‹é—´éƒ½æ˜¯ç‹¬ç«‹çš„ï¼‰ã€‚å› æ­¤ DQN ä¸­ä½¿ ç”¨ç»éªŒå›æ”¾ï¼Œå³ç”¨ä¸€ä¸ª Memory æ¥å­˜å‚¨ç»å†è¿‡çš„æ•°æ®ï¼Œæ¯æ¬¡æ›´æ–°å‚æ•°çš„æ—¶å€™ä» Memory ä¸­æŠ½å–ä¸€éƒ¨åˆ†çš„æ•°æ®æ¥ç”¨äºæ›´æ–°ï¼Œä»¥æ­¤æ¥æ‰“ç ´æ•°æ®é—´çš„å…³è”ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># ç»éªŒå›æ”¾æ± </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># åŒå‘é˜Ÿåˆ—</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#é€šè¿‡ put(transition)æ–¹æ³• å°†æœ€æ–°çš„(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®å­˜å…¥ Deque å¯¹è±¡</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#é€šè¿‡ sample(n)æ–¹æ³•ä» Deque å¯¹è±¡ä¸­éšæœºé‡‡æ ·å‡º n ä¸ª(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># ä»å›æ”¾æ± é‡‡æ ·nä¸ª5å…ƒç»„</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># æŒ‰ç±»åˆ«è¿›è¡Œæ•´ç†</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># è½¬æ¢æˆTensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></table></figure>
<h2 id="qç½‘ç»œ"><a class="markdownIt-Anchor" href="#qç½‘ç»œ"></a> Qç½‘ç»œ</h2>
<p>DQNä½¿ç”¨ç¥ç»ç½‘ç»œå–ä»£äº†Q Tableå»é¢„æµ‹Qï¼ˆsï¼Œaï¼‰</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,output_size,hidden_size</span>):</span></span><br><span class="line">        <span class="comment"># åˆ›å»ºQç½‘ç»œï¼Œè¾“å…¥ä¸ºçŠ¶æ€å‘é‡ï¼Œè¾“å‡ºä¸ºåŠ¨ä½œçš„Qå€¼</span></span><br><span class="line">        <span class="built_in">super</span>(Qnet, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size),</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="æ¨¡å‹è®­ç»ƒ"><a class="markdownIt-Anchor" href="#æ¨¡å‹è®­ç»ƒ"></a> æ¨¡å‹è®­ç»ƒ</h2>
<p>æ¨¡å‹ä½¿ç”¨çš„ä¸¤ä¸ªQç½‘ç»œï¼Œ  åœ¨åŸæ¥çš„ Q ç½‘ç»œçš„åŸºç¡€ä¸Šåˆå¼•å…¥äº†ä¸€ä¸ª target Q ç½‘ç»œï¼Œå³ç”¨æ¥è®¡ç®— target çš„ç½‘ç»œã€‚å®ƒå’Œ Q ç½‘ç»œç»“æ„ä¸€æ ·ï¼Œ åˆå§‹çš„æƒé‡ä¹Ÿä¸€æ ·ï¼Œåªæ˜¯ Q ç½‘ç»œæ¯æ¬¡è¿­ä»£éƒ½ä¼šæ›´æ–°ï¼Œè€Œ target Q ç½‘ç»œæ˜¯æ¯éš”ä¸€æ®µæ—¶é—´æ‰ä¼šæ›´æ–°ã€‚</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.q_target_net.load_state_dict(self.q_net.state_dict())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># é€šè¿‡Qç½‘ç»œå’Œå½±å­ç½‘ç»œæ¥æ„é€ è´å°”æ›¼æ–¹ç¨‹çš„è¯¯å·®ï¼Œ</span></span><br><span class="line">        <span class="comment"># å¹¶åªæ›´æ–°Qç½‘ç»œï¼Œå½±å­ç½‘ç»œçš„æ›´æ–°ä¼šæ»åQç½‘ç»œ</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>): <span class="comment"># è®­ç»ƒ10æ¬¡</span></span><br><span class="line">            s, a, r, s_prime = self.memory.sample(self.batch_size)</span><br><span class="line">            <span class="comment"># q_prime  ç”¨æ—§ç½‘ç»œã€åŠ¨ä½œåçš„ç¯å¢ƒé¢„æµ‹ï¼Œq_a ç”¨æ–°ç½‘ç»œã€åŠ¨ä½œå‰çš„ç¯å¢ƒï¼›åŒæ—¶é¢„æµ‹è®°å¿†ä¸­çš„æƒ…å½¢</span></span><br><span class="line">            q_next, q_eval = self.q_target_net(s),self.q_net(s_prime)</span><br><span class="line">            <span class="comment"># æ¯æ¬¡å­¦ä¹ éƒ½ç”¨ä¸‹ä¸€ä¸ªçŠ¶æ€çš„åŠ¨ä½œç»“åˆåé¦ˆä½œä¸ºå½“å‰åŠ¨ä½œå€¼ï¼ˆè¿™æ ·ï¼Œå°†æœªæ¥çŠ¶æ€çš„åŠ¨ä½œä½œä¸ºç›®æ ‡ï¼Œæœ‰ä¸€å®šå‰ç»æ€§ï¼‰</span></span><br><span class="line">            q_target = q_eval</span><br><span class="line">            <span class="comment">#actionçš„indexå€¼ å®ƒæ‰€åœ¨çš„ä½ç½®</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#å®é™…Qç½‘ç»œçš„å€¼</span></span><br><span class="line">            act_index = np.array(a.tolist()).astype(np.int64)</span><br><span class="line">            act_index = torch.from_numpy(act_index)</span><br><span class="line">            q_a = q_eval.gather(<span class="number">1</span>, act_index)  <span class="comment"># åŠ¨ä½œçš„æ¦‚ç‡å€¼, [b,1]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#target Qç½‘ç»œçš„å€¼</span></span><br><span class="line">            max_q_prime, _ = torch.<span class="built_in">max</span>(q_next, dim=<span class="number">1</span>)</span><br><span class="line">            max_q_prime = max_q_prime.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            q_target = r + self.gamma * max_q_prime</span><br><span class="line">            <span class="comment">#q_out[batch_index, eval_act_index] = reward + self.gamma * np.max(q_prime, axis=1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            loss = self.loss(q_a,q_target)</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line">            cost = loss.item()</span><br><span class="line"></span><br><span class="line">            self.cost_his.append(cost)</span><br><span class="line"></span><br><span class="line">            self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">            self.learn_step_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="å®Œæ•´ä»£ç -2"><a class="markdownIt-Anchor" href="#å®Œæ•´ä»£ç -2"></a> å®Œæ•´ä»£ç </h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">torch.manual_seed(<span class="number">2333</span>)    <span class="comment"># ç­–ç•¥æ¢¯åº¦ç®—æ³•æ–¹å·®å¾ˆå¤§ï¼Œè®¾ç½®seedä»¥ä¿è¯å¤ç°æ€§</span></span><br><span class="line">print(<span class="string">&#x27;observation space:&#x27;</span>,env.observation_space)</span><br><span class="line">print(<span class="string">&#x27;action space:&#x27;</span>,env.action_space)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma = <span class="number">0.99</span></span><br><span class="line">buffer_limit = <span class="number">50000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># ç»éªŒå›æ”¾æ± </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># åŒå‘é˜Ÿåˆ—</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#é€šè¿‡ put(transition)æ–¹æ³• å°†æœ€æ–°çš„(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®å­˜å…¥ Deque å¯¹è±¡</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#é€šè¿‡ sample(n)æ–¹æ³•ä» Deque å¯¹è±¡ä¸­éšæœºé‡‡æ ·å‡º n ä¸ª(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># ä»å›æ”¾æ± é‡‡æ ·nä¸ª5å…ƒç»„</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># æŒ‰ç±»åˆ«è¿›è¡Œæ•´ç†</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># è½¬æ¢æˆTensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,output_size,hidden_size</span>):</span></span><br><span class="line">        <span class="comment"># åˆ›å»ºQç½‘ç»œï¼Œè¾“å…¥ä¸ºçŠ¶æ€å‘é‡ï¼Œè¾“å‡ºä¸ºåŠ¨ä½œçš„Qå€¼</span></span><br><span class="line">        <span class="built_in">super</span>(Qnet, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,output_size,hidden_size,learning_rate,reward_decay,epsilon,e_greedy_increment,e_greedy</span>):</span></span><br><span class="line">        self.n_actions = output_size</span><br><span class="line">        self.n_features = input_size</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        self.epsilon = epsilon <span class="comment">#e - è´ªå¿ƒæ–¹å¼ å‚æ•°</span></span><br><span class="line">        self.q_net = Qnet(input_size,output_size,hidden_size)</span><br><span class="line">        self.q_target_net = Qnet(input_size,output_size,hidden_size)  <span class="comment"># åˆ›å»ºå½±å­ç½‘ç»œ</span></span><br><span class="line">        self.q_target_net.load_state_dict(self.q_net.state_dict()) <span class="comment"># å½±å­ç½‘ç»œæƒå€¼æ¥è‡ªQ</span></span><br><span class="line">        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)<span class="comment">#ä¼˜åŒ–å™¨</span></span><br><span class="line">        self.buffer_limit = <span class="number">50000</span></span><br><span class="line">        self.batch_size = <span class="number">32</span></span><br><span class="line">        self.memory = ReplayBuffer()  <span class="comment">#åˆ›å»ºå›æ”¾æ± </span></span><br><span class="line">        <span class="comment"># Huber Losså¸¸ç”¨äºå›å½’é—®é¢˜ï¼Œå…¶æœ€å¤§çš„ç‰¹ç‚¹æ˜¯å¯¹ç¦»ç¾¤ç‚¹ï¼ˆoutliersï¼‰ã€å™ªå£°ä¸æ•æ„Ÿï¼Œå…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§</span></span><br><span class="line">        self.loss  = torch.nn.SmoothL1Loss()<span class="comment">#æŸå¤±å‡½æ•°</span></span><br><span class="line">        self.cost_his = []</span><br><span class="line">        self.epsilon_increment =e_greedy_increment</span><br><span class="line">        self.learn_step_counter = <span class="number">0</span></span><br><span class="line">        self.epsilon_max = e_greedy</span><br><span class="line"></span><br><span class="line">        self.replace_target_iter = <span class="number">300</span></span><br><span class="line">    <span class="comment"># é€å…¥çŠ¶æ€å‘é‡ï¼Œè·å–ç­–ç•¥: [4]</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        <span class="comment"># å°†stateè½¬åŒ–æˆtensor å¹¶ä¸”ç»´åº¦è½¬åŒ–ä¸º[4]-&gt;[1,4]  unsqueeze(0)åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šç”°é—´</span></span><br><span class="line">        state = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ç­–ç•¥æ”¹è¿›ï¼še-è´ªå¿ƒæ–¹å¼</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            actions_value = self.q_net(state)</span><br><span class="line">            action = np.argmax(actions_value.detach().numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    <span class="comment">#è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.q_target_net.load_state_dict(self.q_net.state_dict())</span><br><span class="line">        <span class="comment"># é€šè¿‡Qç½‘ç»œå’Œå½±å­ç½‘ç»œæ¥æ„é€ è´å°”æ›¼æ–¹ç¨‹çš„è¯¯å·®ï¼Œ</span></span><br><span class="line">        <span class="comment"># å¹¶åªæ›´æ–°Qç½‘ç»œï¼Œå½±å­ç½‘ç»œçš„æ›´æ–°ä¼šæ»åQç½‘ç»œ</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>): <span class="comment"># è®­ç»ƒ10æ¬¡</span></span><br><span class="line">            s, a, r, s_prime = self.memory.sample(self.batch_size)</span><br><span class="line">            <span class="comment"># q_prime  ç”¨æ—§ç½‘ç»œã€åŠ¨ä½œåçš„ç¯å¢ƒé¢„æµ‹ï¼Œq_a ç”¨æ–°ç½‘ç»œã€åŠ¨ä½œå‰çš„ç¯å¢ƒï¼›åŒæ—¶é¢„æµ‹è®°å¿†ä¸­çš„æƒ…å½¢</span></span><br><span class="line">            q_next, q_eval = self.q_target_net(s),self.q_net(s_prime)</span><br><span class="line">            <span class="comment"># æ¯æ¬¡å­¦ä¹ éƒ½ç”¨ä¸‹ä¸€ä¸ªçŠ¶æ€çš„åŠ¨ä½œç»“åˆåé¦ˆä½œä¸ºå½“å‰åŠ¨ä½œå€¼ï¼ˆè¿™æ ·ï¼Œå°†æœªæ¥çŠ¶æ€çš„åŠ¨ä½œä½œä¸ºç›®æ ‡ï¼Œæœ‰ä¸€å®šå‰ç»æ€§ï¼‰</span></span><br><span class="line">            q_target = q_eval</span><br><span class="line">            <span class="comment">#actionçš„indexå€¼ å®ƒæ‰€åœ¨çš„ä½ç½®</span></span><br><span class="line">            <span class="comment">#å®é™…Qç½‘ç»œçš„å€¼</span></span><br><span class="line">            act_index = np.array(a.tolist()).astype(np.int64)</span><br><span class="line">            act_index = torch.from_numpy(act_index)</span><br><span class="line">            q_a = q_eval.gather(<span class="number">1</span>, act_index)  <span class="comment"># åŠ¨ä½œçš„æ¦‚ç‡å€¼, [b,1]</span></span><br><span class="line">            <span class="comment">#target Qç½‘ç»œçš„å€¼</span></span><br><span class="line">            max_q_prime, _ = torch.<span class="built_in">max</span>(q_next, dim=<span class="number">1</span>)</span><br><span class="line">            max_q_prime = max_q_prime.unsqueeze(<span class="number">1</span>)</span><br><span class="line">            q_target = r + self.gamma * max_q_prime</span><br><span class="line">            loss = self.loss(q_a,q_target)</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line">            cost = loss.item()</span><br><span class="line"></span><br><span class="line">            self.cost_his.append(cost)</span><br><span class="line"></span><br><span class="line">            self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">            self.learn_step_counter += <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his)), self.cost_his)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># åˆ›å»ºç¯å¢ƒ</span></span><br><span class="line">    dqn = DQN(input_size = <span class="number">4</span>,</span><br><span class="line">              output_size = <span class="number">2</span>,</span><br><span class="line">              hidden_size = <span class="number">10</span>,</span><br><span class="line">              learning_rate = <span class="number">0.01</span>,</span><br><span class="line">              reward_decay=<span class="number">0.9</span>,epsilon=<span class="number">0.9</span>,</span><br><span class="line">              e_greedy_increment=<span class="number">0.001</span>,</span><br><span class="line">              e_greedy=<span class="number">0.9</span>,</span><br><span class="line">              )</span><br><span class="line"></span><br><span class="line">    print_interval = <span class="number">20</span></span><br><span class="line">    reword = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2000</span>):  <span class="comment"># è®­ç»ƒæ¬¡æ•°</span></span><br><span class="line">        <span class="comment"># epsilonæ¦‚ç‡ä¹Ÿä¼š8%åˆ°1%è¡°å‡ï¼Œè¶Šåˆ°åé¢è¶Šä½¿ç”¨Qå€¼æœ€å¤§çš„åŠ¨ä½œ</span></span><br><span class="line">        dqn.epsilon = <span class="built_in">max</span>(<span class="number">0.01</span>, <span class="number">0.08</span> - <span class="number">0.01</span> * (n_epi / <span class="number">200</span>))</span><br><span class="line">        state = env.reset()  <span class="comment"># å¤ä½ç¯å¢ƒ</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>: <span class="comment"># ä¸€ä¸ªå›åˆæœ€å¤§æ—¶é—´æˆ³</span></span><br><span class="line">            <span class="comment"># æ ¹æ®å½“å‰Qç½‘ç»œæå–ç­–ç•¥ï¼Œå¹¶æ”¹è¿›ç­–ç•¥</span></span><br><span class="line">            action = dqn.choose_action(state)</span><br><span class="line">            <span class="comment"># ä½¿ç”¨æ”¹è¿›çš„ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’</span></span><br><span class="line">            s_prime, r, done, info = env.step(action)</span><br><span class="line">            <span class="comment">#https://blog.csdn.net/u012465304/article/details/81172759</span></span><br><span class="line">            <span class="comment">#ç”±äºCartPoleè¿™ä¸ªæ¸¸æˆçš„rewardæ˜¯åªè¦æ†å­æ˜¯ç«‹èµ·æ¥çš„ï¼Œä»–rewardå°±æ˜¯1ï¼Œå¤±è´¥å°±æ˜¯0ï¼Œ</span></span><br><span class="line">            <span class="comment"># æ˜¾ç„¶è¿™ä¸ªrewardå¯¹äºè¿ç»­æ€§å˜é‡æ˜¯ä¸å¯ä»¥æ¥å—çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬é€šè¿‡observationä¿®æ”¹è¿™ä¸ªå€¼ã€‚</span></span><br><span class="line">            <span class="comment"># ç‚¹å‡»pycharmå³ä¸Šè§’çš„æœç´¢ç¬¦å·æœç´¢CartPoleè¿›å…¥ä»–ç¯å¢ƒçš„æºä»£ç ä¸­ï¼Œå†è¿›å…¥stepå‡½æ•°ï¼Œ</span></span><br><span class="line">            <span class="comment"># çœ‹åˆ°é‡Œé¢è¿”å›å€¼stateçš„å®šä¹‰</span></span><br><span class="line">            <span class="comment">#é€šè¿‡è¿™å››ä¸ªå€¼å®šä¹‰æ–°çš„rewardæ˜¯</span></span><br><span class="line">            x, x_dot, theta, theta_dot = s_prime</span><br><span class="line">            r1 = (env.x_threshold - <span class="built_in">abs</span>(x)) / env.x_threshold - <span class="number">0.8</span></span><br><span class="line">            r2 = (env.theta_threshold_radians - <span class="built_in">abs</span>(theta)) / env.theta_threshold_radians - <span class="number">0.5</span></span><br><span class="line">            reward = r1 + r2</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ä¿å­˜å››å…ƒç»„</span></span><br><span class="line">            dqn.memory.put((state, action,reward,s_prime))</span><br><span class="line">            state = s_prime</span><br><span class="line">            reword +=reward</span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># å›åˆç»“æŸ</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> dqn.memory.size() &gt; <span class="number">1000</span>:  <span class="comment"># ç¼“å†²æ± åªæœ‰å¤§äº2000å°±å¯ä»¥è®­ç»ƒ</span></span><br><span class="line">                dqn.train()</span><br><span class="line">        <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span> <span class="keyword">and</span> n_epi != <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&quot;# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, &quot;</span> \</span><br><span class="line">                  <span class="string">&quot;epsilon : &#123;:.1f&#125;%&quot;</span> \</span><br><span class="line">                  .<span class="built_in">format</span>(n_epi, reword / print_interval, dqn.memory.size(), dqn.epsilon * <span class="number">100</span>))</span><br><span class="line">            reword = <span class="number">0.0</span></span><br><span class="line">    env.close()</span><br><span class="line">    dqn.plot_cost()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="ddpgç®—æ³•å®ç°"><a class="markdownIt-Anchor" href="#ddpgç®—æ³•å®ç°"></a> DDPGç®—æ³•å®ç°</h1>
<p>DDPGç®—æ³•çš„æ€æƒ³åœ¨<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">å¦ä¸€ç¯‡åšå®¢</a>ä¸­æœ‰ä»‹ç»äº†ï¼Œä¸‹é¢æ˜¯ç®—æ³•çš„å…·ä½“å®ç°ã€‚</p>
<p>DDPG å¯ä»¥è§£å†³è¿ç»­åŠ¨ä½œç©ºé—´é—®é¢˜ï¼Œå¹¶ä¸”æ˜¯actor-criticæ–¹æ³•ï¼Œå³æ—¢æœ‰å€¼å‡½æ•°ç½‘ç»œ(critic)ï¼Œåˆæœ‰ç­–ç•¥ç½‘ç»œ(actor)ã€‚</p>
<h2 id="ç»éªŒå›æ”¾æ± -2"><a class="markdownIt-Anchor" href="#ç»éªŒå›æ”¾æ± -2"></a> ç»éªŒå›æ”¾æ± </h2>
<p>ä¸DQNä¸­çš„ç»éªŒå›æ”¾æ± çš„å®ç°ç›¸åŒ</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># ç»éªŒå›æ”¾æ± </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># åŒå‘é˜Ÿåˆ—</span></span><br><span class="line">        buffer_limit = <span class="number">50000</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#é€šè¿‡ put(transition)æ–¹æ³• å°†æœ€æ–°çš„(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®å­˜å…¥ Deque å¯¹è±¡</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#é€šè¿‡ sample(n)æ–¹æ³•ä» Deque å¯¹è±¡ä¸­éšæœºé‡‡æ ·å‡º n ä¸ª(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># ä»å›æ”¾æ± é‡‡æ ·nä¸ª5å…ƒç»„</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># æŒ‰ç±»åˆ«è¿›è¡Œæ•´ç†</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># è½¬æ¢æˆTensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></table></figure>
<h2 id="ç­–ç•¥ç½‘ç»œactorç½‘ç»œ"><a class="markdownIt-Anchor" href="#ç­–ç•¥ç½‘ç»œactorç½‘ç»œ"></a> ç­–ç•¥ç½‘ç»œï¼ˆActorç½‘ç»œï¼‰</h2>
<p>è¾“å…¥ä¸ºstate  è¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒpi(a|s)ï¼ˆæ¯ä¸ªåŠ¨ä½œå‡ºç°çš„æ¦‚ç‡ï¼‰</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ç­–ç•¥ç½‘ç»œï¼Œä¹Ÿå«Actorç½‘ç»œï¼Œè¾“å…¥ä¸ºstate  è¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒpi(a|s)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,hidden_size,output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># self.linear  = nn.Linear(hidden_size, output_size)</span></span><br><span class="line">        self.actor_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=output_size)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        x = self.actor_net(state)</span><br><span class="line">        x = torch.tanh(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="å€¼å‡½æ•°ç½‘ç»œ-è¾“å…¥æ˜¯stateactionè¾“å‡ºæ˜¯qsa"><a class="markdownIt-Anchor" href="#å€¼å‡½æ•°ç½‘ç»œ-è¾“å…¥æ˜¯stateactionè¾“å‡ºæ˜¯qsa"></a> å€¼å‡½æ•°ç½‘ç»œ  è¾“å…¥æ˜¯stateï¼Œactionè¾“å‡ºæ˜¯Q(s,a)</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#å€¼å‡½æ•°ç½‘ç»œ  è¾“å…¥æ˜¯stateï¼Œactionè¾“å‡ºæ˜¯Q(s,a)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        self.critic_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state,action</span>):</span></span><br><span class="line">        inputs = torch.cat([state,action],<span class="number">1</span>)</span><br><span class="line">        x = self.critic_net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="æ•´ä½“å®ç°"><a class="markdownIt-Anchor" href="#æ•´ä½“å®ç°"></a> æ•´ä½“å®ç°</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span>  collections</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;Pendulum-v0&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">torch.manual_seed(<span class="number">2333</span>)    <span class="comment"># ç­–ç•¥æ¢¯åº¦ç®—æ³•æ–¹å·®å¾ˆå¤§ï¼Œè®¾ç½®seedä»¥ä¿è¯å¤ç°æ€§</span></span><br><span class="line">env.reset()</span><br><span class="line">env.render()</span><br><span class="line">print(<span class="string">&#x27;observation space:&#x27;</span>,env.observation_space)</span><br><span class="line">print(<span class="string">&#x27;action space:&#x27;</span>,env.action_space)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># ç»éªŒå›æ”¾æ± </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># åŒå‘é˜Ÿåˆ—</span></span><br><span class="line">        buffer_limit = <span class="number">50000</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#é€šè¿‡ put(transition)æ–¹æ³• å°†æœ€æ–°çš„(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®å­˜å…¥ Deque å¯¹è±¡</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#é€šè¿‡ sample(n)æ–¹æ³•ä» Deque å¯¹è±¡ä¸­éšæœºé‡‡æ ·å‡º n ä¸ª(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># ä»å›æ”¾æ± é‡‡æ ·nä¸ª5å…ƒç»„</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># æŒ‰ç±»åˆ«è¿›è¡Œæ•´ç†</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># è½¬æ¢æˆTensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ç­–ç•¥ç½‘ç»œï¼Œä¹Ÿå«Actorç½‘ç»œï¼Œè¾“å…¥ä¸ºstate  è¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒpi(a|s)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,hidden_size,output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># self.linear  = nn.Linear(hidden_size, output_size)</span></span><br><span class="line">        self.actor_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=output_size)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        x = self.actor_net(state)</span><br><span class="line">        x = torch.tanh(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#å€¼å‡½æ•°ç½‘ç»œ  è¾“å…¥æ˜¯stateï¼Œactionè¾“å‡ºæ˜¯Q(s,a)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        self.critic_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state,action</span>):</span></span><br><span class="line">        inputs = torch.cat([state,action],<span class="number">1</span>)</span><br><span class="line">        x = self.critic_net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPG</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,state_size,action_size,hidden_size = <span class="number">256</span>,actor_lr = <span class="number">0.001</span>,ctitic_lr = <span class="number">0.001</span>,batch_size = <span class="number">32</span></span>):</span></span><br><span class="line"></span><br><span class="line">        self.state_size = state_size</span><br><span class="line">        self.action_size = action_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.actor_lr = actor_lr <span class="comment">#actorç½‘ç»œå­¦ä¹ ç‡</span></span><br><span class="line">        self.critic_lr = ctitic_lr<span class="comment">#criticç½‘ç»œå­¦ä¹ ç‡</span></span><br><span class="line">        <span class="comment"># ç­–ç•¥ç½‘ç»œï¼Œä¹Ÿå«Actorç½‘ç»œï¼Œè¾“å…¥ä¸ºstate  è¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒpi(a|s)</span></span><br><span class="line">        self.actor = Actor(self.state_size, self.hidden_size, self.action_size)</span><br><span class="line">        <span class="comment">#target actorç½‘ç»œ å»¶è¿Ÿæ›´æ–°</span></span><br><span class="line">        self.actor_target = Actor(self.state_size, self.hidden_size, self.action_size)</span><br><span class="line">        <span class="comment"># å€¼å‡½æ•°ç½‘ç»œ  è¾“å…¥æ˜¯stateï¼Œactionè¾“å‡ºæ˜¯Q(s,a)</span></span><br><span class="line">        self.critic = Critic(self.state_size + self.action_size, self.hidden_size, self.action_size)</span><br><span class="line">        self.critic_target = Critic(self.state_size + self.action_size, self.hidden_size, self.action_size)</span><br><span class="line"></span><br><span class="line">        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.actor_lr)</span><br><span class="line">        self.critic_optim = optim.Adam(self.critic.parameters(), lr=self.critic_lr)</span><br><span class="line">        self.buffer = []</span><br><span class="line">        <span class="comment"># å½±å­ç½‘ç»œæƒå€¼æ¥è‡ªåŸç½‘ç»œï¼Œåªä¸è¿‡å»¶è¿Ÿæ›´æ–°</span></span><br><span class="line">        self.actor_target.load_state_dict(self.actor.state_dict())</span><br><span class="line">        self.critic_target.load_state_dict(self.critic.state_dict())</span><br><span class="line">        self.gamma = <span class="number">0.99</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.memory = ReplayBuffer()  <span class="comment"># åˆ›å»ºå›æ”¾æ± </span></span><br><span class="line"></span><br><span class="line">        self.memory2 = []</span><br><span class="line">        self.learn_step_counter = <span class="number">0</span> <span class="comment">#å­¦ä¹ è½®æ•° ä¸å½±å­ç½‘ç»œçš„æ›´æ–°æœ‰å…³</span></span><br><span class="line">        self.replace_target_iter = <span class="number">200</span> <span class="comment">#å½±å­ç½‘ç»œè¿­ä»£å¤šå°‘è½®æ›´æ–°ä¸€æ¬¡</span></span><br><span class="line">        self.cost_his_actor = []<span class="comment"># å­˜å‚¨cost å‡†å¤‡ç”»å›¾</span></span><br><span class="line">        self.cost_his_critic = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        <span class="comment"># å°†stateè½¬åŒ–æˆtensor å¹¶ä¸”ç»´åº¦è½¬åŒ–ä¸º[3]-&gt;[1,3]  unsqueeze(0)åœ¨ç¬¬0ä¸ªç»´åº¦ä¸Šç”°é—´</span></span><br><span class="line">        state = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        action = self.actor(state).squeeze(<span class="number">0</span>).detach().numpy()</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    <span class="comment">#criticç½‘ç»œçš„å­¦ä¹ </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">critic_learn</span>(<span class="params">self,s0,a0,r1,s1</span>):</span></span><br><span class="line">        <span class="comment">#ä»actor_targeté€šè¿‡çŠ¶æ€è·å–å¯¹åº”çš„åŠ¨ä½œ  detach()å°†tensorä»è®¡ç®—å›¾ä¸Šå‰¥ç¦»</span></span><br><span class="line">        a1 = self.actor_target(s0).detach()</span><br><span class="line">        <span class="comment">#åˆ å‡ä¸€ä¸ªç»´åº¦  [b,1,1]å˜æˆ[b,1]</span></span><br><span class="line">        a0 = a0.squeeze(<span class="number">2</span>)</span><br><span class="line">        y_pred = self.critic(s0,a0)</span><br><span class="line">        y_target = r1 +self.gamma *self.critic_target(s1,a1).detach()</span><br><span class="line">        loss_fn = nn.MSELoss()</span><br><span class="line">        loss = loss_fn(y_pred, y_target)</span><br><span class="line">        self.critic_optim.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.critic_optim.step()</span><br><span class="line">        self.cost_his_critic.append(loss.item())</span><br><span class="line">    <span class="comment">#actorç½‘ç»œçš„å­¦ä¹ </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">actor_learn</span>(<span class="params">self,s0,a0,r1,s1</span>):</span></span><br><span class="line">        loss = -torch.mean(self.critic(s0, self.actor(s0)))</span><br><span class="line">        self.actor_optim.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.actor_optim.step()</span><br><span class="line">        self.cost_his_actor.append(loss.item())</span><br><span class="line">    <span class="comment">#æ¨¡å‹çš„è®­ç»ƒ</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.actor_target.load_state_dict(self.actor.state_dict())</span><br><span class="line">            self.critic_target.load_state_dict(self.critic.state_dict())</span><br><span class="line">        <span class="comment">#éšæœºé‡‡æ ·å‡º batch_size ä¸ª(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®</span></span><br><span class="line">        s0, a0, r, s_prime = self.memory.sample(self.batch_size)</span><br><span class="line">        self.critic_learn(s0, a0, r, s_prime)</span><br><span class="line">        self.actor_learn(s0, a0, r, s_prime)</span><br><span class="line"></span><br><span class="line">        self.soft_update(self.critic_target, self.critic, <span class="number">0.02</span>)</span><br><span class="line">        self.soft_update(self.actor_target, self.actor, <span class="number">0.02</span>)</span><br><span class="line">    <span class="comment">#targetç½‘ç»œçš„æ›´æ–°</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">soft_update</span>(<span class="params">self,net_target, net, tau</span>):</span></span><br><span class="line">        <span class="keyword">for</span> target_param, param <span class="keyword">in</span> <span class="built_in">zip</span>(net_target.parameters(), net.parameters()):</span><br><span class="line">            target_param.data.copy_(target_param.data * (<span class="number">1.0</span> - tau) + param.data * tau)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his_critic)), self.cost_his_critic)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    print(env.observation_space.shape[<span class="number">0</span>])</span><br><span class="line">    print(env.action_space.shape[<span class="number">0</span>])</span><br><span class="line">    ddgp = DDPG(state_size=env.observation_space.shape[<span class="number">0</span>],</span><br><span class="line">                action_size=env.action_space.shape[<span class="number">0</span>],</span><br><span class="line">                hidden_size=<span class="number">256</span>,</span><br><span class="line">                actor_lr=<span class="number">0.001</span>,</span><br><span class="line">                ctitic_lr=  <span class="number">0.001</span>,</span><br><span class="line">                batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">    print_interval = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        state = env.reset()</span><br><span class="line">        episode_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">            env.render()</span><br><span class="line">            action0 = ddgp.choose_action(state)</span><br><span class="line">            s_prime, r, done, info = env.step(action0)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ä¿å­˜å››å…ƒç»„</span></span><br><span class="line">            ddgp.memory.put((state, action0, r, s_prime))</span><br><span class="line">            episode_reward += r</span><br><span class="line">            state = s_prime</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># å›åˆç»“æŸ</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ddgp.memory.size() &gt; <span class="number">32</span>:  <span class="comment"># ç¼“å†²æ± åªæœ‰å¤§äº500å°±å¯ä»¥è®­ç»ƒ</span></span><br><span class="line">                ddgp.train()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> episode % print_interval == <span class="number">0</span> <span class="keyword">and</span> episode != <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&quot;# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, &quot;</span></span><br><span class="line">                  .<span class="built_in">format</span>(episode, episode_reward / print_interval, ddgp.memory.size()))</span><br><span class="line">    env.close()</span><br><span class="line">    ddgp.plot_cost()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">æ·±åº¦å­¦ä¹ æ¡†æ¶</a><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/10/16/Arithmetic-LeetCode/282/"><img class="prev-cover" src="/2021/10/16/Arithmetic-LeetCode/282/show.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">Leetcode 282. ç»™è¡¨è¾¾å¼æ·»åŠ è¿ç®—ç¬¦</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorchä¸­ç¥ç»ç½‘ç»œå·¥å…·ç®±nnæ¨¡å—"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorchä¸­ç¥ç»ç½‘ç»œå·¥å…·ç®±nnæ¨¡å—</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorchä¸­çš„Autograd"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorchä¸­çš„Autograd</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-Tensor/" title="Pytorchä¸­çš„Tensor"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorchä¸­çš„Tensor</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-Introductory-knowledge/" title="PyTorchå…¥é—¨çŸ¥è¯†"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">PyTorchå…¥é—¨çŸ¥è¯†</div></div></a></div><div><a href="/2020/12/08/Tensorflow/Tensorflow-and-Reinforcement-learning/" title="Tensorflowä¸å¼ºåŒ–å­¦ä¹ "><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-08</div><div class="title">Tensorflowä¸å¼ºåŒ–å­¦ä¹ </div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ccclll777</div><div class="author-info__description">èƒ¸æ€€çŒ›è™ ç»†å—…è”·è–‡</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">45</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">26</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">9</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ccclll777"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccclll777" target="_blank" title="fab fa-github"><i class="GitHub"></i></a><a class="social-icon" href="mailto:sdu945860882@gmail.com" target="_blank" title="fa fa-envelope"><i class="E-Mail"></i></a><a class="social-icon" href="https://www.weibo.com/6732062654" target="_blank" title="fab fa-weibo"><i class="Weibo"></i></a><a class="social-icon" href="https://blog.csdn.net/baidu_41871794" target="_blank" title="gratipay"><i class="CSDN"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#policy-gradient%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.</span> <span class="toc-text"> Policy Gradientç®—æ³•å®ç°</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#policy%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.</span> <span class="toc-text"> Policyç½‘ç»œ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E7%8A%B6%E6%80%81%E8%BE%93%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%80%89%E6%8B%A9%E5%8A%A8%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text"> å°†çŠ¶æ€è¾“å…¥ç¥ç»ç½‘ç»œï¼Œé€‰æ‹©åŠ¨ä½œ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.3.</span> <span class="toc-text"> æ¨¡å‹çš„è®­ç»ƒ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-number">1.4.</span> <span class="toc-text"> å®Œæ•´ä»£ç </span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#dqn%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text"> DQNç®—æ³•å®ç°</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%E6%B1%A0"><span class="toc-number">2.1.</span> <span class="toc-text"> ç»éªŒå›æ”¾æ± </span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#q%E7%BD%91%E7%BB%9C"><span class="toc-number">2.2.</span> <span class="toc-text"> Qç½‘ç»œ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">2.3.</span> <span class="toc-text"> æ¨¡å‹è®­ç»ƒ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81-2"><span class="toc-number">2.4.</span> <span class="toc-text"> å®Œæ•´ä»£ç </span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ddpg%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text"> DDPGç®—æ³•å®ç°</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%E6%B1%A0-2"><span class="toc-number">3.1.</span> <span class="toc-text"> ç»éªŒå›æ”¾æ± </span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9Cactor%E7%BD%91%E7%BB%9C"><span class="toc-number">3.2.</span> <span class="toc-text"> ç­–ç•¥ç½‘ç»œï¼ˆActorç½‘ç»œï¼‰</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0%E7%BD%91%E7%BB%9C-%E8%BE%93%E5%85%A5%E6%98%AFstateaction%E8%BE%93%E5%87%BA%E6%98%AFqsa"><span class="toc-number">3.3.</span> <span class="toc-text"> å€¼å‡½æ•°ç½‘ç»œ  è¾“å…¥æ˜¯stateï¼Œactionè¾“å‡ºæ˜¯Q(s,a)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.4.</span> <span class="toc-text"> æ•´ä½“å®ç°</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>æœ€æ–°æ–‡ç« </span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. ç»™è¡¨è¾¾å¼æ·»åŠ è¿ç®—ç¬¦"><img src="/2021/10/16/Arithmetic-LeetCode/282/show.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Leetcode 282. ç»™è¡¨è¾¾å¼æ·»åŠ è¿ç®—ç¬¦"/></a><div class="content"><a class="title" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. ç»™è¡¨è¾¾å¼æ·»åŠ è¿ç®—ç¬¦">Leetcode 282. ç»™è¡¨è¾¾å¼æ·»åŠ è¿ç®—ç¬¦</a><time datetime="2021-10-16T15:35:16.000Z" title="å‘è¡¨äº 2021-10-16 23:35:16">2021-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°"/></a><div class="content"><a class="title" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°">Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°</a><time datetime="2020-12-12T02:54:37.000Z" title="å‘è¡¨äº 2020-12-12 10:54:37">2020-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—">PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—</a><time datetime="2020-12-09T13:32:23.000Z" title="å‘è¡¨äº 2020-12-09 21:32:23">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorchä¸­ç¥ç»ç½‘ç»œå·¥å…·ç®±nnæ¨¡å—"><img src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorchä¸­ç¥ç»ç½‘ç»œå·¥å…·ç®±nnæ¨¡å—"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorchä¸­ç¥ç»ç½‘ç»œå·¥å…·ç®±nnæ¨¡å—">Pytorchä¸­ç¥ç»ç½‘ç»œå·¥å…·ç®±nnæ¨¡å—</a><time datetime="2020-12-09T13:25:38.000Z" title="å‘è¡¨äº 2020-12-09 21:25:38">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorchä¸­çš„Autograd"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorchä¸­çš„Autograd"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorchä¸­çš„Autograd">Pytorchä¸­çš„Autograd</a><time datetime="2020-12-09T13:25:19.000Z" title="å‘è¡¨äº 2020-12-09 21:25:19">2020-12-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By ccclll777</div><div class="framework-info"><span>æ¡†æ¶ </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>ä¸»é¢˜ </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="ç®€ç¹è½¬æ¢">ç®€</button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>