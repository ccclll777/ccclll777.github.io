<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Pytorch强化学习算法实现 | ccclll777's blogs</title><meta name="keywords" content="python,深度学习框架,pytorch"><meta name="author" content="ccclll777"><meta name="copyright" content="ccclll777"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="使用Pytorch框架实现了强化学习算法Policy Gradient&#x2F;DQN&#x2F;DDGP">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch强化学习算法实现">
<meta property="og:url" content="http://yoursite.com/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="使用Pytorch框架实现了强化学习算法Policy Gradient&#x2F;DQN&#x2F;DDGP">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png">
<meta property="article:published_time" content="2020-12-12T02:54:37.000Z">
<meta property="article:modified_time" content="2021-10-17T01:36:25.739Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="python">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png"><link rel="shortcut icon" href="/images/avatar.png"><link rel="canonical" href="http://yoursite.com/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch强化学习算法实现',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-10-17 09:36:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ccclll777's blogs</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch强化学习算法实现</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-12-12T02:54:37.000Z" title="发表于 2020-12-12 10:54:37">2020-12-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-17T01:36:25.739Z" title="更新于 2021-10-17 09:36:25">2021-10-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pytorch强化学习算法实现"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>使用Pytorch框架实现了强化学习算法Policy Gradient/DQN/DDGP</p>
<span id="more"></span>
<h1 id="policy-gradient算法实现"><a class="markdownIt-Anchor" href="#policy-gradient算法实现"></a> Policy Gradient算法实现</h1>
<p>Policy Gradient算法的思想在<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">另一篇博客</a>中有介绍了，下面是算法的具体实现。</p>
<h2 id="policy网络"><a class="markdownIt-Anchor" href="#policy网络"></a> Policy网络</h2>
<p>两个线性层，中间使用Relu激活函数连接，最后连接softmax输出每个动作的概率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,n_states_num,n_actions_num,hidden_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PolicyNet, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment">#存储轨迹</span></span><br><span class="line">        <span class="comment">#输入为长度为4的向量 输出为向左  向右两个动作</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=n_states_num, out_features=hidden_size, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=n_actions_num, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># 状态输入s的shape为向量：[4]</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="将状态输入神经网络选择动作"><a class="markdownIt-Anchor" href="#将状态输入神经网络选择动作"></a> 将状态输入神经网络，选择动作</h2>
<ul>
<li>这里给出了两种实现方式，具体思想就是输入环境的状态，传入policy网络，给出每一个动作的概率，我们需要选择出现概率最大的那个动作，以及他出现的概率值。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#将状态传入神经网络 根据概率选择动作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span>  <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">    <span class="comment">#将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">    s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    prob = self.pi(s)  <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">    <span class="comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span></span><br><span class="line">    m = torch.distributions.Categorical(prob)  <span class="comment"># 生成分布</span></span><br><span class="line">    action = m.sample()</span><br><span class="line">    <span class="keyword">return</span> action.item() , m.log_prob(action)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action2</span>(<span class="params">self, state</span>):</span></span><br><span class="line">    <span class="comment"># 将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">    s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    prob = self.pi(s)  <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">    <span class="comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span></span><br><span class="line">    action =np.random.choice(<span class="built_in">range</span>(prob.shape[<span class="number">1</span>]),size=<span class="number">1</span>,p = prob.view(-<span class="number">1</span>).detach().numpy())[<span class="number">0</span>]</span><br><span class="line">    action = <span class="built_in">int</span>(action)</span><br><span class="line">    <span class="comment">#print(torch.log(prob[0][action]).unsqueeze(0))</span></span><br><span class="line">    <span class="keyword">return</span> action,torch.log(prob[<span class="number">0</span>][action]).unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>（1）使用<code>torch.distributions.Categorical</code>生成分布，然后进行选择</li>
<li>（2）使用<code>np.random.choice</code>进行采样</li>
</ul>
<h2 id="模型的训练"><a class="markdownIt-Anchor" href="#模型的训练"></a> 模型的训练</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_net</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="comment"># 计算梯度并更新策略网络参数。tape为梯度记录器</span></span><br><span class="line">    R = <span class="number">0</span>  <span class="comment"># 终结状态的初始回报为0</span></span><br><span class="line">    policy_loss = []</span><br><span class="line">    <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::-<span class="number">1</span>]:  <span class="comment"># 逆序取</span></span><br><span class="line">        R = r + gamma * R  <span class="comment"># 计算每个时间戳上的回报</span></span><br><span class="line">        <span class="comment"># 每个时间戳都计算一次梯度</span></span><br><span class="line">        loss = -log_prob * R</span><br><span class="line">        policy_loss.append(loss)</span><br><span class="line">    self.optimizer.zero_grad()</span><br><span class="line">    policy_loss = torch.cat(policy_loss).<span class="built_in">sum</span>()  <span class="comment"># 求和</span></span><br><span class="line">    <span class="comment">#反向传播</span></span><br><span class="line">    policy_loss.backward()</span><br><span class="line">    self.optimizer.step()</span><br><span class="line">    self.cost_his.append(policy_loss.item())</span><br><span class="line">    self.data = []  <span class="comment"># 清空轨迹</span></span><br></pre></td></tr></table></figure>
<h2 id="完整代码"><a class="markdownIt-Anchor" href="#完整代码"></a> 完整代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> 	gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="comment"># Default parameters for plots</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.titlesize&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;KaiTi&#x27;</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">torch.manual_seed(<span class="number">2333</span>)    <span class="comment"># 策略梯度算法方差很大，设置seed以保证复现性</span></span><br><span class="line">print(<span class="string">&#x27;observation space:&#x27;</span>,env.observation_space)</span><br><span class="line">print(<span class="string">&#x27;action space:&#x27;</span>,env.action_space)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma         = <span class="number">0.98</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,n_states_num,n_actions_num,hidden_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PolicyNet, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment">#存储轨迹</span></span><br><span class="line">        <span class="comment">#输入为长度为4的向量 输出为向左  向右两个动作</span></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=n_states_num, out_features=hidden_size, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=n_actions_num, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># 状态输入s的shape为向量：[4]</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyGradient</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,n_states_num,n_actions_num,learning_rate=<span class="number">0.01</span>,reward_decay=<span class="number">0.95</span> </span>):</span></span><br><span class="line">        <span class="comment">#状态数   state是一个4维向量，分别是位置，速度，杆子的角度，加速度</span></span><br><span class="line">        self.n_states_num = n_states_num</span><br><span class="line">        <span class="comment">#action是二维、离散，即向左/右推杆子</span></span><br><span class="line">        self.n_actions_num = n_actions_num</span><br><span class="line">        <span class="comment">#学习率</span></span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        <span class="comment">#gamma</span></span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        <span class="comment">#网络</span></span><br><span class="line">        self.pi = PolicyNet(n_states_num, n_actions_num, <span class="number">128</span>)</span><br><span class="line">        <span class="comment">#优化器</span></span><br><span class="line">        self.optimizer = torch.optim.Adam(self.pi.parameters(), lr=learning_rate)</span><br><span class="line">        <span class="comment"># 存储轨迹  存储方式为  （每一次的reward，动作的概率）</span></span><br><span class="line">        self.data = []</span><br><span class="line">        self.cost_his = []</span><br><span class="line">    <span class="comment">#存储轨迹数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put_data</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        <span class="comment"># 记录r,log_P(a|s)z</span></span><br><span class="line">        self.data.append(item)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_net</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 计算梯度并更新策略网络参数。tape为梯度记录器</span></span><br><span class="line">        R = <span class="number">0</span>  <span class="comment"># 终结状态的初始回报为0</span></span><br><span class="line">        policy_loss = []</span><br><span class="line">        <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::-<span class="number">1</span>]:  <span class="comment"># 逆序取</span></span><br><span class="line">            R = r + gamma * R  <span class="comment"># 计算每个时间戳上的回报</span></span><br><span class="line">            <span class="comment"># 每个时间戳都计算一次梯度</span></span><br><span class="line">            loss = -log_prob * R</span><br><span class="line">            policy_loss.append(loss)</span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        policy_loss = torch.cat(policy_loss).<span class="built_in">sum</span>()  <span class="comment"># 求和</span></span><br><span class="line">        <span class="comment">#反向传播</span></span><br><span class="line">        policy_loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        self.cost_his.append(policy_loss.item())</span><br><span class="line">        self.data = []  <span class="comment"># 清空轨迹</span></span><br><span class="line">    <span class="comment">#将状态传入神经网络 根据概率选择动作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        <span class="comment">#将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">        s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        prob = self.pi(s)  <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">        <span class="comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span></span><br><span class="line">        m = torch.distributions.Categorical(prob)  <span class="comment"># 生成分布</span></span><br><span class="line">        action = m.sample()</span><br><span class="line">        <span class="keyword">return</span> action.item() , m.log_prob(action)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action2</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="comment"># 将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">        s = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        prob = self.pi(s)  <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">        <span class="comment"># 从类别分布中采样1个动作, shape: [1] torch.log(prob), 1</span></span><br><span class="line">        action =np.random.choice(<span class="built_in">range</span>(prob.shape[<span class="number">1</span>]),size=<span class="number">1</span>,p = prob.view(-<span class="number">1</span>).detach().numpy())[<span class="number">0</span>]</span><br><span class="line">        action = <span class="built_in">int</span>(action)</span><br><span class="line">        <span class="comment">#print(torch.log(prob[0][action]).unsqueeze(0))</span></span><br><span class="line">        <span class="keyword">return</span> action,torch.log(prob[<span class="number">0</span>][action]).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his)), self.cost_his)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    policyGradient = PolicyGradient(<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">    running_reward = <span class="number">10</span>  <span class="comment"># 计分</span></span><br><span class="line">    print_interval = <span class="number">20</span>  <span class="comment"># 打印间隔</span></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">        state = env.reset()  <span class="comment"># 回到游戏初始状态，返回s0</span></span><br><span class="line">        ep_reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1001</span>):  <span class="comment"># CartPole-v1 forced to terminates at 1000 step.</span></span><br><span class="line">            <span class="comment">#根据状态 传入神经网络 选择动作</span></span><br><span class="line">            action ,log_prob  = policyGradient.choose_action2(state)</span><br><span class="line">            <span class="comment">#与环境交互</span></span><br><span class="line">            s_prime, reward, done, info = env.step(action)</span><br><span class="line">            <span class="comment"># s_prime, reward, done, info = env.step(action)</span></span><br><span class="line">            <span class="keyword">if</span> n_epi &gt; <span class="number">1000</span>:</span><br><span class="line">                env.render()</span><br><span class="line">            <span class="comment"># 记录动作a和动作产生的奖励r</span></span><br><span class="line">            <span class="comment"># prob shape:[1,2]</span></span><br><span class="line">            policyGradient.put_data((reward, log_prob))</span><br><span class="line">            state = s_prime  <span class="comment"># 刷新状态</span></span><br><span class="line">            ep_reward += reward</span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># 当前episode终止</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># episode终止后，训练一次网络</span></span><br><span class="line">        running_reward = <span class="number">0.05</span> * ep_reward + (<span class="number">1</span> - <span class="number">0.05</span>) * running_reward</span><br><span class="line">        <span class="comment">#交互完成后 进行学习</span></span><br><span class="line">        policyGradient.train_net()</span><br><span class="line">        <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&#x27;Episode &#123;&#125;\tLast reward: &#123;:.2f&#125;\tAverage reward: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                n_epi, ep_reward, running_reward))</span><br><span class="line">        <span class="keyword">if</span> running_reward &gt; env.spec.reward_threshold:  <span class="comment"># 大于游戏的最大阈值475时，退出游戏</span></span><br><span class="line">            print(<span class="string">&quot;Solved! Running reward is now &#123;&#125; and &quot;</span></span><br><span class="line">                  <span class="string">&quot;the last episode runs to &#123;&#125; time steps!&quot;</span>.<span class="built_in">format</span>(running_reward, t))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    policyGradient.plot_cost()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="dqn算法实现"><a class="markdownIt-Anchor" href="#dqn算法实现"></a> DQN算法实现</h1>
<p>DQN算法的思想在<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">另一篇博客</a>中有介绍了，下面是算法的具体实现。</p>
<h2 id="经验回放池"><a class="markdownIt-Anchor" href="#经验回放池"></a> 经验回放池</h2>
<p>这里使用python的双向队列实现了经验回放池，实现了状态的存储以及随机采样。</p>
<ul>
<li>经验回放 Experience replay：由于在强化学习中，我们得到 的观测数据是有序的，用这样的数据去更新神经网络的参数会有问题（对比监督学习，数据之间都是独立的）。因此 DQN 中使 用经验回放，即用一个 Memory 来存储经历过的数据，每次更新参数的时候从 Memory 中抽取一部分的数据来用于更新，以此来打破数据间的关联。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></table></figure>
<h2 id="q网络"><a class="markdownIt-Anchor" href="#q网络"></a> Q网络</h2>
<p>DQN使用神经网络取代了Q Table去预测Q（s，a）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,output_size,hidden_size</span>):</span></span><br><span class="line">        <span class="comment"># 创建Q网络，输入为状态向量，输出为动作的Q值</span></span><br><span class="line">        <span class="built_in">super</span>(Qnet, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size),</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a class="markdownIt-Anchor" href="#模型训练"></a> 模型训练</h2>
<p>模型使用的两个Q网络，  在原来的 Q 网络的基础上又引入了一个 target Q 网络，即用来计算 target 的网络。它和 Q 网络结构一样， 初始的权重也一样，只是 Q 网络每次迭代都会更新，而 target Q 网络是每隔一段时间才会更新。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.q_target_net.load_state_dict(self.q_net.state_dict())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过Q网络和影子网络来构造贝尔曼方程的误差，</span></span><br><span class="line">        <span class="comment"># 并只更新Q网络，影子网络的更新会滞后Q网络</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>): <span class="comment"># 训练10次</span></span><br><span class="line">            s, a, r, s_prime = self.memory.sample(self.batch_size)</span><br><span class="line">            <span class="comment"># q_prime  用旧网络、动作后的环境预测，q_a 用新网络、动作前的环境；同时预测记忆中的情形</span></span><br><span class="line">            q_next, q_eval = self.q_target_net(s),self.q_net(s_prime)</span><br><span class="line">            <span class="comment"># 每次学习都用下一个状态的动作结合反馈作为当前动作值（这样，将未来状态的动作作为目标，有一定前瞻性）</span></span><br><span class="line">            q_target = q_eval</span><br><span class="line">            <span class="comment">#action的index值 它所在的位置</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#实际Q网络的值</span></span><br><span class="line">            act_index = np.array(a.tolist()).astype(np.int64)</span><br><span class="line">            act_index = torch.from_numpy(act_index)</span><br><span class="line">            q_a = q_eval.gather(<span class="number">1</span>, act_index)  <span class="comment"># 动作的概率值, [b,1]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#target Q网络的值</span></span><br><span class="line">            max_q_prime, _ = torch.<span class="built_in">max</span>(q_next, dim=<span class="number">1</span>)</span><br><span class="line">            max_q_prime = max_q_prime.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            q_target = r + self.gamma * max_q_prime</span><br><span class="line">            <span class="comment">#q_out[batch_index, eval_act_index] = reward + self.gamma * np.max(q_prime, axis=1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            loss = self.loss(q_a,q_target)</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line">            cost = loss.item()</span><br><span class="line"></span><br><span class="line">            self.cost_his.append(cost)</span><br><span class="line"></span><br><span class="line">            self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">            self.learn_step_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="完整代码-2"><a class="markdownIt-Anchor" href="#完整代码-2"></a> 完整代码</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">torch.manual_seed(<span class="number">2333</span>)    <span class="comment"># 策略梯度算法方差很大，设置seed以保证复现性</span></span><br><span class="line">print(<span class="string">&#x27;observation space:&#x27;</span>,env.observation_space)</span><br><span class="line">print(<span class="string">&#x27;action space:&#x27;</span>,env.action_space)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma = <span class="number">0.99</span></span><br><span class="line">buffer_limit = <span class="number">50000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,output_size,hidden_size</span>):</span></span><br><span class="line">        <span class="comment"># 创建Q网络，输入为状态向量，输出为动作的Q值</span></span><br><span class="line">        <span class="built_in">super</span>(Qnet, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,output_size,hidden_size,learning_rate,reward_decay,epsilon,e_greedy_increment,e_greedy</span>):</span></span><br><span class="line">        self.n_actions = output_size</span><br><span class="line">        self.n_features = input_size</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        self.epsilon = epsilon <span class="comment">#e - 贪心方式 参数</span></span><br><span class="line">        self.q_net = Qnet(input_size,output_size,hidden_size)</span><br><span class="line">        self.q_target_net = Qnet(input_size,output_size,hidden_size)  <span class="comment"># 创建影子网络</span></span><br><span class="line">        self.q_target_net.load_state_dict(self.q_net.state_dict()) <span class="comment"># 影子网络权值来自Q</span></span><br><span class="line">        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)<span class="comment">#优化器</span></span><br><span class="line">        self.buffer_limit = <span class="number">50000</span></span><br><span class="line">        self.batch_size = <span class="number">32</span></span><br><span class="line">        self.memory = ReplayBuffer()  <span class="comment">#创建回放池</span></span><br><span class="line">        <span class="comment"># Huber Loss常用于回归问题，其最大的特点是对离群点（outliers）、噪声不敏感，具有较强的鲁棒性</span></span><br><span class="line">        self.loss  = torch.nn.SmoothL1Loss()<span class="comment">#损失函数</span></span><br><span class="line">        self.cost_his = []</span><br><span class="line">        self.epsilon_increment =e_greedy_increment</span><br><span class="line">        self.learn_step_counter = <span class="number">0</span></span><br><span class="line">        self.epsilon_max = e_greedy</span><br><span class="line"></span><br><span class="line">        self.replace_target_iter = <span class="number">300</span></span><br><span class="line">    <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        <span class="comment"># 将state转化成tensor 并且维度转化为[4]-&gt;[1,4]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">        state = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 策略改进：e-贪心方式</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            actions_value = self.q_net(state)</span><br><span class="line">            action = np.argmax(actions_value.detach().numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    <span class="comment">#训练模型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.q_target_net.load_state_dict(self.q_net.state_dict())</span><br><span class="line">        <span class="comment"># 通过Q网络和影子网络来构造贝尔曼方程的误差，</span></span><br><span class="line">        <span class="comment"># 并只更新Q网络，影子网络的更新会滞后Q网络</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>): <span class="comment"># 训练10次</span></span><br><span class="line">            s, a, r, s_prime = self.memory.sample(self.batch_size)</span><br><span class="line">            <span class="comment"># q_prime  用旧网络、动作后的环境预测，q_a 用新网络、动作前的环境；同时预测记忆中的情形</span></span><br><span class="line">            q_next, q_eval = self.q_target_net(s),self.q_net(s_prime)</span><br><span class="line">            <span class="comment"># 每次学习都用下一个状态的动作结合反馈作为当前动作值（这样，将未来状态的动作作为目标，有一定前瞻性）</span></span><br><span class="line">            q_target = q_eval</span><br><span class="line">            <span class="comment">#action的index值 它所在的位置</span></span><br><span class="line">            <span class="comment">#实际Q网络的值</span></span><br><span class="line">            act_index = np.array(a.tolist()).astype(np.int64)</span><br><span class="line">            act_index = torch.from_numpy(act_index)</span><br><span class="line">            q_a = q_eval.gather(<span class="number">1</span>, act_index)  <span class="comment"># 动作的概率值, [b,1]</span></span><br><span class="line">            <span class="comment">#target Q网络的值</span></span><br><span class="line">            max_q_prime, _ = torch.<span class="built_in">max</span>(q_next, dim=<span class="number">1</span>)</span><br><span class="line">            max_q_prime = max_q_prime.unsqueeze(<span class="number">1</span>)</span><br><span class="line">            q_target = r + self.gamma * max_q_prime</span><br><span class="line">            loss = self.loss(q_a,q_target)</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line">            cost = loss.item()</span><br><span class="line"></span><br><span class="line">            self.cost_his.append(cost)</span><br><span class="line"></span><br><span class="line">            self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">            self.learn_step_counter += <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his)), self.cost_his)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># 创建环境</span></span><br><span class="line">    dqn = DQN(input_size = <span class="number">4</span>,</span><br><span class="line">              output_size = <span class="number">2</span>,</span><br><span class="line">              hidden_size = <span class="number">10</span>,</span><br><span class="line">              learning_rate = <span class="number">0.01</span>,</span><br><span class="line">              reward_decay=<span class="number">0.9</span>,epsilon=<span class="number">0.9</span>,</span><br><span class="line">              e_greedy_increment=<span class="number">0.001</span>,</span><br><span class="line">              e_greedy=<span class="number">0.9</span>,</span><br><span class="line">              )</span><br><span class="line"></span><br><span class="line">    print_interval = <span class="number">20</span></span><br><span class="line">    reword = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2000</span>):  <span class="comment"># 训练次数</span></span><br><span class="line">        <span class="comment"># epsilon概率也会8%到1%衰减，越到后面越使用Q值最大的动作</span></span><br><span class="line">        dqn.epsilon = <span class="built_in">max</span>(<span class="number">0.01</span>, <span class="number">0.08</span> - <span class="number">0.01</span> * (n_epi / <span class="number">200</span>))</span><br><span class="line">        state = env.reset()  <span class="comment"># 复位环境</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>: <span class="comment"># 一个回合最大时间戳</span></span><br><span class="line">            <span class="comment"># 根据当前Q网络提取策略，并改进策略</span></span><br><span class="line">            action = dqn.choose_action(state)</span><br><span class="line">            <span class="comment"># 使用改进的策略与环境交互</span></span><br><span class="line">            s_prime, r, done, info = env.step(action)</span><br><span class="line">            <span class="comment">#https://blog.csdn.net/u012465304/article/details/81172759</span></span><br><span class="line">            <span class="comment">#由于CartPole这个游戏的reward是只要杆子是立起来的，他reward就是1，失败就是0，</span></span><br><span class="line">            <span class="comment"># 显然这个reward对于连续性变量是不可以接受的，所以我们通过observation修改这个值。</span></span><br><span class="line">            <span class="comment"># 点击pycharm右上角的搜索符号搜索CartPole进入他环境的源代码中，再进入step函数，</span></span><br><span class="line">            <span class="comment"># 看到里面返回值state的定义</span></span><br><span class="line">            <span class="comment">#通过这四个值定义新的reward是</span></span><br><span class="line">            x, x_dot, theta, theta_dot = s_prime</span><br><span class="line">            r1 = (env.x_threshold - <span class="built_in">abs</span>(x)) / env.x_threshold - <span class="number">0.8</span></span><br><span class="line">            r2 = (env.theta_threshold_radians - <span class="built_in">abs</span>(theta)) / env.theta_threshold_radians - <span class="number">0.5</span></span><br><span class="line">            reward = r1 + r2</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 保存四元组</span></span><br><span class="line">            dqn.memory.put((state, action,reward,s_prime))</span><br><span class="line">            state = s_prime</span><br><span class="line">            reword +=reward</span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># 回合结束</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> dqn.memory.size() &gt; <span class="number">1000</span>:  <span class="comment"># 缓冲池只有大于2000就可以训练</span></span><br><span class="line">                dqn.train()</span><br><span class="line">        <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span> <span class="keyword">and</span> n_epi != <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&quot;# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, &quot;</span> \</span><br><span class="line">                  <span class="string">&quot;epsilon : &#123;:.1f&#125;%&quot;</span> \</span><br><span class="line">                  .<span class="built_in">format</span>(n_epi, reword / print_interval, dqn.memory.size(), dqn.epsilon * <span class="number">100</span>))</span><br><span class="line">            reword = <span class="number">0.0</span></span><br><span class="line">    env.close()</span><br><span class="line">    dqn.plot_cost()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="ddpg算法实现"><a class="markdownIt-Anchor" href="#ddpg算法实现"></a> DDPG算法实现</h1>
<p>DDPG算法的思想在<a target="_blank" rel="noopener" href="https://ccclll777.github.io/2020/12/07/Reinforcement-Learning-Basic-Theory/#more">另一篇博客</a>中有介绍了，下面是算法的具体实现。</p>
<p>DDPG 可以解决连续动作空间问题，并且是actor-critic方法，即既有值函数网络(critic)，又有策略网络(actor)。</p>
<h2 id="经验回放池-2"><a class="markdownIt-Anchor" href="#经验回放池-2"></a> 经验回放池</h2>
<p>与DQN中的经验回放池的实现相同</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        buffer_limit = <span class="number">50000</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></table></figure>
<h2 id="策略网络actor网络"><a class="markdownIt-Anchor" href="#策略网络actor网络"></a> 策略网络（Actor网络）</h2>
<p>输入为state  输出为概率分布pi(a|s)（每个动作出现的概率）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 策略网络，也叫Actor网络，输入为state  输出为概率分布pi(a|s)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,hidden_size,output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># self.linear  = nn.Linear(hidden_size, output_size)</span></span><br><span class="line">        self.actor_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=output_size)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        x = self.actor_net(state)</span><br><span class="line">        x = torch.tanh(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="值函数网络-输入是stateaction输出是qsa"><a class="markdownIt-Anchor" href="#值函数网络-输入是stateaction输出是qsa"></a> 值函数网络  输入是state，action输出是Q(s,a)</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#值函数网络  输入是state，action输出是Q(s,a)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        self.critic_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state,action</span>):</span></span><br><span class="line">        inputs = torch.cat([state,action],<span class="number">1</span>)</span><br><span class="line">        x = self.critic_net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="整体实现"><a class="markdownIt-Anchor" href="#整体实现"></a> 整体实现</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span>  collections</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;Pendulum-v0&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">torch.manual_seed(<span class="number">2333</span>)    <span class="comment"># 策略梯度算法方差很大，设置seed以保证复现性</span></span><br><span class="line">env.reset()</span><br><span class="line">env.render()</span><br><span class="line">print(<span class="string">&#x27;observation space:&#x27;</span>,env.observation_space)</span><br><span class="line">print(<span class="string">&#x27;action space:&#x27;</span>,env.action_space)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        buffer_limit = <span class="number">50000</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> torch.Tensor(s_lst), \</span><br><span class="line">               torch.Tensor(a_lst), \</span><br><span class="line">                      torch.Tensor(r_lst), \</span><br><span class="line">                      torch.Tensor(s_prime_lst)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 策略网络，也叫Actor网络，输入为state  输出为概率分布pi(a|s)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,hidden_size,output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># self.linear  = nn.Linear(hidden_size, output_size)</span></span><br><span class="line">        self.actor_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size,out_features=output_size)</span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        x = self.actor_net(state)</span><br><span class="line">        x = torch.tanh(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#值函数网络  输入是state，action输出是Q(s,a)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        self.critic_net = nn.Sequential(</span><br><span class="line">            nn.Linear(in_features=input_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=hidden_size),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(in_features=hidden_size, out_features=output_size)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, state,action</span>):</span></span><br><span class="line">        inputs = torch.cat([state,action],<span class="number">1</span>)</span><br><span class="line">        x = self.critic_net(inputs)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPG</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,state_size,action_size,hidden_size = <span class="number">256</span>,actor_lr = <span class="number">0.001</span>,ctitic_lr = <span class="number">0.001</span>,batch_size = <span class="number">32</span></span>):</span></span><br><span class="line"></span><br><span class="line">        self.state_size = state_size</span><br><span class="line">        self.action_size = action_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.actor_lr = actor_lr <span class="comment">#actor网络学习率</span></span><br><span class="line">        self.critic_lr = ctitic_lr<span class="comment">#critic网络学习率</span></span><br><span class="line">        <span class="comment"># 策略网络，也叫Actor网络，输入为state  输出为概率分布pi(a|s)</span></span><br><span class="line">        self.actor = Actor(self.state_size, self.hidden_size, self.action_size)</span><br><span class="line">        <span class="comment">#target actor网络 延迟更新</span></span><br><span class="line">        self.actor_target = Actor(self.state_size, self.hidden_size, self.action_size)</span><br><span class="line">        <span class="comment"># 值函数网络  输入是state，action输出是Q(s,a)</span></span><br><span class="line">        self.critic = Critic(self.state_size + self.action_size, self.hidden_size, self.action_size)</span><br><span class="line">        self.critic_target = Critic(self.state_size + self.action_size, self.hidden_size, self.action_size)</span><br><span class="line"></span><br><span class="line">        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.actor_lr)</span><br><span class="line">        self.critic_optim = optim.Adam(self.critic.parameters(), lr=self.critic_lr)</span><br><span class="line">        self.buffer = []</span><br><span class="line">        <span class="comment"># 影子网络权值来自原网络，只不过延迟更新</span></span><br><span class="line">        self.actor_target.load_state_dict(self.actor.state_dict())</span><br><span class="line">        self.critic_target.load_state_dict(self.critic.state_dict())</span><br><span class="line">        self.gamma = <span class="number">0.99</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.memory = ReplayBuffer()  <span class="comment"># 创建回放池</span></span><br><span class="line"></span><br><span class="line">        self.memory2 = []</span><br><span class="line">        self.learn_step_counter = <span class="number">0</span> <span class="comment">#学习轮数 与影子网络的更新有关</span></span><br><span class="line">        self.replace_target_iter = <span class="number">200</span> <span class="comment">#影子网络迭代多少轮更新一次</span></span><br><span class="line">        self.cost_his_actor = []<span class="comment"># 存储cost 准备画图</span></span><br><span class="line">        self.cost_his_critic = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self,state</span>):</span></span><br><span class="line">        <span class="comment"># 将state转化成tensor 并且维度转化为[3]-&gt;[1,3]  unsqueeze(0)在第0个维度上田间</span></span><br><span class="line">        state = torch.Tensor(state).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        action = self.actor(state).squeeze(<span class="number">0</span>).detach().numpy()</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line">    <span class="comment">#critic网络的学习</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">critic_learn</span>(<span class="params">self,s0,a0,r1,s1</span>):</span></span><br><span class="line">        <span class="comment">#从actor_target通过状态获取对应的动作  detach()将tensor从计算图上剥离</span></span><br><span class="line">        a1 = self.actor_target(s0).detach()</span><br><span class="line">        <span class="comment">#删减一个维度  [b,1,1]变成[b,1]</span></span><br><span class="line">        a0 = a0.squeeze(<span class="number">2</span>)</span><br><span class="line">        y_pred = self.critic(s0,a0)</span><br><span class="line">        y_target = r1 +self.gamma *self.critic_target(s1,a1).detach()</span><br><span class="line">        loss_fn = nn.MSELoss()</span><br><span class="line">        loss = loss_fn(y_pred, y_target)</span><br><span class="line">        self.critic_optim.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.critic_optim.step()</span><br><span class="line">        self.cost_his_critic.append(loss.item())</span><br><span class="line">    <span class="comment">#actor网络的学习</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">actor_learn</span>(<span class="params">self,s0,a0,r1,s1</span>):</span></span><br><span class="line">        loss = -torch.mean(self.critic(s0, self.actor(s0)))</span><br><span class="line">        self.actor_optim.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.actor_optim.step()</span><br><span class="line">        self.cost_his_actor.append(loss.item())</span><br><span class="line">    <span class="comment">#模型的训练</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.actor_target.load_state_dict(self.actor.state_dict())</span><br><span class="line">            self.critic_target.load_state_dict(self.critic.state_dict())</span><br><span class="line">        <span class="comment">#随机采样出 batch_size 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">        s0, a0, r, s_prime = self.memory.sample(self.batch_size)</span><br><span class="line">        self.critic_learn(s0, a0, r, s_prime)</span><br><span class="line">        self.actor_learn(s0, a0, r, s_prime)</span><br><span class="line"></span><br><span class="line">        self.soft_update(self.critic_target, self.critic, <span class="number">0.02</span>)</span><br><span class="line">        self.soft_update(self.actor_target, self.actor, <span class="number">0.02</span>)</span><br><span class="line">    <span class="comment">#target网络的更新</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">soft_update</span>(<span class="params">self,net_target, net, tau</span>):</span></span><br><span class="line">        <span class="keyword">for</span> target_param, param <span class="keyword">in</span> <span class="built_in">zip</span>(net_target.parameters(), net.parameters()):</span><br><span class="line">            target_param.data.copy_(target_param.data * (<span class="number">1.0</span> - tau) + param.data * tau)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his_critic)), self.cost_his_critic)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    print(env.observation_space.shape[<span class="number">0</span>])</span><br><span class="line">    print(env.action_space.shape[<span class="number">0</span>])</span><br><span class="line">    ddgp = DDPG(state_size=env.observation_space.shape[<span class="number">0</span>],</span><br><span class="line">                action_size=env.action_space.shape[<span class="number">0</span>],</span><br><span class="line">                hidden_size=<span class="number">256</span>,</span><br><span class="line">                actor_lr=<span class="number">0.001</span>,</span><br><span class="line">                ctitic_lr=  <span class="number">0.001</span>,</span><br><span class="line">                batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">    print_interval = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        state = env.reset()</span><br><span class="line">        episode_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">            env.render()</span><br><span class="line">            action0 = ddgp.choose_action(state)</span><br><span class="line">            s_prime, r, done, info = env.step(action0)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 保存四元组</span></span><br><span class="line">            ddgp.memory.put((state, action0, r, s_prime))</span><br><span class="line">            episode_reward += r</span><br><span class="line">            state = s_prime</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># 回合结束</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ddgp.memory.size() &gt; <span class="number">32</span>:  <span class="comment"># 缓冲池只有大于500就可以训练</span></span><br><span class="line">                ddgp.train()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> episode % print_interval == <span class="number">0</span> <span class="keyword">and</span> episode != <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">&quot;# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, &quot;</span></span><br><span class="line">                  .<span class="built_in">format</span>(episode, episode_reward / print_interval, ddgp.memory.size()))</span><br><span class="line">    env.close()</span><br><span class="line">    ddgp.plot_cost()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/10/16/Arithmetic-LeetCode/282/"><img class="prev-cover" src="/2021/10/16/Arithmetic-LeetCode/282/show.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Leetcode 282. 给表达式添加运算符</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">PyTorch常用工具模块</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">PyTorch常用工具模块</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorch中神经网络工具箱nn模块</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorch中的Autograd</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-Tensor/" title="Pytorch中的Tensor"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorch中的Tensor</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-Introductory-knowledge/" title="PyTorch入门知识"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">PyTorch入门知识</div></div></a></div><div><a href="/2020/12/08/Tensorflow/Tensorflow-and-Reinforcement-learning/" title="Tensorflow与强化学习"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-08</div><div class="title">Tensorflow与强化学习</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ccclll777</div><div class="author-info__description">胸怀猛虎 细嗅蔷薇</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ccclll777"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccclll777" target="_blank" title="fab fa-github"><i class="GitHub"></i></a><a class="social-icon" href="mailto:sdu945860882@gmail.com" target="_blank" title="fa fa-envelope"><i class="E-Mail"></i></a><a class="social-icon" href="https://www.weibo.com/6732062654" target="_blank" title="fab fa-weibo"><i class="Weibo"></i></a><a class="social-icon" href="https://blog.csdn.net/baidu_41871794" target="_blank" title="gratipay"><i class="CSDN"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#policy-gradient%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.</span> <span class="toc-text"> Policy Gradient算法实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#policy%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.</span> <span class="toc-text"> Policy网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E7%8A%B6%E6%80%81%E8%BE%93%E5%85%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%80%89%E6%8B%A9%E5%8A%A8%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text"> 将状态输入神经网络，选择动作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.3.</span> <span class="toc-text"> 模型的训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-number">1.4.</span> <span class="toc-text"> 完整代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#dqn%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text"> DQN算法实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%E6%B1%A0"><span class="toc-number">2.1.</span> <span class="toc-text"> 经验回放池</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#q%E7%BD%91%E7%BB%9C"><span class="toc-number">2.2.</span> <span class="toc-text"> Q网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">2.3.</span> <span class="toc-text"> 模型训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81-2"><span class="toc-number">2.4.</span> <span class="toc-text"> 完整代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ddpg%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text"> DDPG算法实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%E6%B1%A0-2"><span class="toc-number">3.1.</span> <span class="toc-text"> 经验回放池</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9Cactor%E7%BD%91%E7%BB%9C"><span class="toc-number">3.2.</span> <span class="toc-text"> 策略网络（Actor网络）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0%E7%BD%91%E7%BB%9C-%E8%BE%93%E5%85%A5%E6%98%AFstateaction%E8%BE%93%E5%87%BA%E6%98%AFqsa"><span class="toc-number">3.3.</span> <span class="toc-text"> 值函数网络  输入是state，action输出是Q(s,a)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.4.</span> <span class="toc-text"> 整体实现</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符"><img src="/2021/10/16/Arithmetic-LeetCode/282/show.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Leetcode 282. 给表达式添加运算符"/></a><div class="content"><a class="title" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符">Leetcode 282. 给表达式添加运算符</a><time datetime="2021-10-16T15:35:16.000Z" title="发表于 2021-10-16 23:35:16">2021-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch强化学习算法实现"/></a><div class="content"><a class="title" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现">Pytorch强化学习算法实现</a><time datetime="2020-12-12T02:54:37.000Z" title="发表于 2020-12-12 10:54:37">2020-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyTorch常用工具模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块">PyTorch常用工具模块</a><time datetime="2020-12-09T13:32:23.000Z" title="发表于 2020-12-09 21:32:23">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块"><img src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中神经网络工具箱nn模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块">Pytorch中神经网络工具箱nn模块</a><time datetime="2020-12-09T13:25:38.000Z" title="发表于 2020-12-09 21:25:38">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中的Autograd"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd">Pytorch中的Autograd</a><time datetime="2020-12-09T13:25:19.000Z" title="发表于 2020-12-09 21:25:19">2020-12-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By ccclll777</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>