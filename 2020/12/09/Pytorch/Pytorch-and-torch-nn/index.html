<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Pytorch中神经网络工具箱nn模块 | ccclll777's blogs</title><meta name="keywords" content="深度学习框架,python,pytorch"><meta name="author" content="ccclll777"><meta name="copyright" content="ccclll777"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="torch.nn专门为深度学习而设计的模块。torch.nn的核心数据结构是Module，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络层.">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch中神经网络工具箱nn模块">
<meta property="og:url" content="http://yoursite.com/2020/12/09/Pytorch/Pytorch-and-torch-nn/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="torch.nn专门为深度学习而设计的模块。torch.nn的核心数据结构是Module，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络层.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png">
<meta property="article:published_time" content="2020-12-09T13:25:38.000Z">
<meta property="article:modified_time" content="2021-10-17T01:36:25.740Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="python">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png"><link rel="shortcut icon" href="/images/avatar.png"><link rel="canonical" href="http://yoursite.com/2020/12/09/Pytorch/Pytorch-and-torch-nn/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch中神经网络工具箱nn模块',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-10-17 09:36:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ccclll777's blogs</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch中神经网络工具箱nn模块</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-12-09T13:25:38.000Z" title="发表于 2020-12-09 21:25:38">2020-12-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-17T01:36:25.740Z" title="更新于 2021-10-17 09:36:25">2021-10-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>29分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pytorch中神经网络工具箱nn模块"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>torch.nn专门为深度学习而设计的模块。torch.nn的核心数据结构是<code>Module</code>，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承<code>nn.Module</code>，撰写自己的网络层.</p>
<span id="more"></span>
<h1 id="使用torchnn实现全连接层"><a class="markdownIt-Anchor" href="#使用torchnn实现全连接层"></a> 使用torch.nn实现全连接层</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable <br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Linear</span>(<span class="hljs-params">nn.Module</span>):</span> <span class="hljs-comment"># 继承nn.Module</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_features, out_features</span>):</span><br>        <span class="hljs-built_in">super</span>(Linear, self).__init__() <span class="hljs-comment"># 等价于nn.Module.__init__(self)</span><br>        self.w = nn.Parameter(torch.randn(in_features, out_features))<br>        self.b = nn.Parameter(torch.randn(out_features))<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        x = x.mm(self.w) <span class="hljs-comment"># x.@(self.w) 矩阵乘法</span><br>        <span class="hljs-keyword">return</span> x + self.b.expand_as(x)<br><br>layer = Linear(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>)<br><span class="hljs-built_in">input</span> = Variable(torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">4</span>))<br>output = layer(<span class="hljs-built_in">input</span>)<br>output<br><br><br></code></pre></td></tr></table></figure>
<p>可见，全连接层的实现非常简单，其代码量不超过10行，但需注意以下几点：</p>
<ul>
<li>自定义层<code>Linear</code>必须继承<code>nn.Module</code>，并且在其构造函数中需调用<code>nn.Module</code>的构造函数，即<code>super(Linear, self).__init__()</code> 或<code>nn.Module.__init__(self)</code>，推荐使用第一种用法，尽管第二种写法更直观。</li>
<li>在构造函数<code>__init__</code>中必须自己定义可学习的参数，并封装成<code>Parameter</code>，如在本例中我们把<code>w</code>和<code>b</code>封装成<code>parameter</code>。<code>parameter</code>是一种特殊的<code>Variable</code>，但其默认需要求导（requires_grad = True），感兴趣的读者可以通过<code>nn.Parameter??</code>，查看<code>Parameter</code>类的源代码。</li>
<li><code>forward</code>函数实现前向传播过程，其输入可以是一个或多个variable，对x的任何操作也必须是variable支持的操作。</li>
<li>无需写反向传播函数，因其前向传播都是对variable进行操作，nn.Module能够利用autograd自动实现反向传播，这点比Function简单许多。</li>
<li>使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于<code>layers.__call__(input)</code>，在<code>__call__</code>函数中，主要调用的是 <code>layer.forward(x)</code>，另外还对钩子做了一些处理。所以在实际使用中应尽量使用<code>layer(x)</code>而不是使用<code>layer.forward(x)</code>，关于钩子技术将在下文讲解。</li>
<li><code>Module</code>中的可学习参数可以通过<code>named_parameters()</code>或者<code>parameters()</code>返回迭代器，前者会给每个parameter都附上名字，使其更具有辨识度。</li>
</ul>
<p>可见利用Module实现的全连接层，比利用<code>Function</code>实现的更为简单，因其不再需要写反向传播函数。</p>
<p>module中parameter的命名规范：</p>
<ul>
<li>对于类似<code>self.param_name = nn.Parameter(t.randn(3, 4))</code>，命名为<code>param_name</code></li>
<li>对于子Module中的parameter，会其名字之前加上当前Module的名字。如对于<code>self.sub_module = SubModel()</code>，SubModel中有个parameter的名字叫做param_name，那么二者拼接而成的parameter name 就是<code>sub_module.param_name</code>。</li>
</ul>
<p>为方便用户使用，PyTorch实现了神经网络中绝大多数的layer，这些layer都继承于nn.Module，封装了可学习参数<code>parameter</code>，并实现了forward函数,阅读文档时应主要关注以下几点：</p>
<ul>
<li>构造函数的参数，如nn.Linear(in_features, out_features, bias)，需关注这三个参数的作用。</li>
<li>属性，可学习参数，子module。如nn.Linear中有<code>weight</code>和<code>bias</code>两个可学习参数，不包含子module。</li>
<li>输入输出的形状，如nn.linear的输入形状是(N, input_features)，输出为(N，output_features)，N是batch_size。</li>
</ul>
<p>这些自定义layer对输入形状都有假设：输入的不是单个数据，而是一个batch。若想输入一个数据，则必须调用<code>unsqueeze(0)</code>函数将数据伪装成batch_size=1的batch</p>
<h1 id="常用神经网络层"><a class="markdownIt-Anchor" href="#常用神经网络层"></a> 常用神经网络层</h1>
<h2 id="图像相关层"><a class="markdownIt-Anchor" href="#图像相关层"></a> 图像相关层</h2>
<p>图像相关层主要包括卷积层（Conv）、池化层（Pool）等，这些层在实际使用中可分为一维(1D)、二维(2D)、三维（3D），池化方式又分为平均池化（AvgPool）、最大值池化（MaxPool）、自适应池化（AdaptiveAvgPool）等。而卷积层除了常用的前向卷积之外，还有逆卷积（TransposeConv）。下面举例说明一些基础的使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor, ToPILImage<br>to_tensor = ToTensor() <span class="hljs-comment"># img -&gt; tensor</span><br>to_pil = ToPILImage()<br>lena = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;imgs/lena.png&#x27;</span>)<br><br><span class="hljs-comment"># 输入是一个batch，batch_size＝1</span><br><span class="hljs-built_in">input</span> = to_tensor(lena).unsqueeze(<span class="hljs-number">0</span>) <br><br><span class="hljs-comment"># 锐化卷积核</span><br>kernel = torch.ones(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)/-<span class="hljs-number">9.</span><br>kernel[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] = <span class="hljs-number">1</span><br>conv = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>conv.weight.data = kernel.view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br><br>out = conv(Variable(<span class="hljs-built_in">input</span>))<br>to_pil(out.data.squeeze(<span class="hljs-number">0</span>))<br></code></pre></td></tr></table></figure>
<ul>
<li>池化层可以看作是一种特殊的卷积层，用来下采样。但池化层没有可学习参数，其weight是固定的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">pool = nn.AvgPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br><span class="hljs-built_in">list</span>(pool.parameters())<br><br>out = pool(Variable(<span class="hljs-built_in">input</span>))<br>to_pil(out.data.squeeze(<span class="hljs-number">0</span>))<br><br></code></pre></td></tr></table></figure>
<p>除了卷积层和池化层，深度学习中还将常用到以下几个层：</p>
<ul>
<li>Linear：全连接层。</li>
<li>BatchNorm：批规范化层，分为1D、2D和3D。除了标准的BatchNorm之外，还有在风格迁移中常用到的InstanceNorm层。</li>
<li>Dropout：dropout层，用来防止过拟合，同样分为1D、2D和3D。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输入 batch_size=2，维度3</span><br><span class="hljs-built_in">input</span> = Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br>linear = nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>h = linear(<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 4 channel，初始化标准差为4，均值为0</span><br>bn = nn.BatchNorm1d(<span class="hljs-number">4</span>)<br>bn.weight.data = torch.ones(<span class="hljs-number">4</span>) * <span class="hljs-number">4</span><br>bn.bias.data = torch.zeros(<span class="hljs-number">4</span>)<br>bn_out = bn(h)<br><span class="hljs-comment"># 注意输出的均值和方差</span><br><span class="hljs-comment"># 方差是标准差的平方，计算无偏方差分母会减1</span><br><span class="hljs-comment"># 使用unbiased=False 分母不减1</span><br>bn_out.mean(<span class="hljs-number">0</span>), bn_out.var(<span class="hljs-number">0</span>, unbiased=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 每个元素以0.5的概率舍弃</span><br>dropout = nn.Dropout(<span class="hljs-number">0.5</span>)<br>o = dropout(bn_out)<br><span class="hljs-comment"># 有一半左右的数变为0</span><br></code></pre></td></tr></table></figure>
<p>以上很多例子中都对module的属性直接操作，其大多数是可学习参数，一般会随着学习的进行而不断改变。实际使用中除非需要使用特殊的初始化，应尽量不要直接修改这些参数。</p>
<h2 id="激活函数"><a class="markdownIt-Anchor" href="#激活函数"></a> 激活函数</h2>
<p>PyTorch实现了常见的激活函数，其具体的接口信息可参见官方文档，这些激活函数可作为独立的layer使用。这里将介绍最常用的激活函数ReLU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">input</span> = Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br>print(<span class="hljs-built_in">input</span>)<br>output = relu(<span class="hljs-built_in">input</span>)<br>print(output) <span class="hljs-comment"># 小于0的都被截断为0</span><br><span class="hljs-comment"># 等价于input.clamp(min=0)</span><br></code></pre></td></tr></table></figure>
<p>ReLU函数有个inplace参数，如果设为True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算ReLU的反向传播时，只需根据输出就能够推算出反向传播的梯度。但是只有少数的autograd操作支持inplace操作（如variable.sigmoid_()），除非你明确地知道自己在做什么，否则一般不要使用inplace操作。</p>
<ul>
<li>在以上的例子中，基本上都是将每一层的输出直接作为下一层的输入，这种网络称为前馈传播网络（feedforward neural network）。对于此类网络如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的module，它包含几个子Module，前向传播时会将输入一层接一层的传递下去。ModuleList也是一个特殊的module，可以包含几个子module，可以像用list一样使用它，但不能直接把输入传给ModuleList。下面举例说明。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment"># Sequential的三种写法</span><br>net1 = nn.Sequential()<br>net1.add_module(<span class="hljs-string">&#x27;conv&#x27;</span>, nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>net1.add_module(<span class="hljs-string">&#x27;batchnorm&#x27;</span>, nn.BatchNorm2d(<span class="hljs-number">3</span>))<br>net1.add_module(<span class="hljs-string">&#x27;activation_layer&#x27;</span>, nn.ReLU())<br>net2 = nn.Sequential(<br>        nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>),<br>        nn.BatchNorm2d(<span class="hljs-number">3</span>),<br>        nn.ReLU()<br>        )<br><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br>net3= nn.Sequential(OrderedDict([<br>          (<span class="hljs-string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>)),<br>          (<span class="hljs-string">&#x27;bn1&#x27;</span>, nn.BatchNorm2d(<span class="hljs-number">3</span>)),<br>          (<span class="hljs-string">&#x27;relu1&#x27;</span>, nn.ReLU())<br>        ]))<br>print(<span class="hljs-string">&#x27;net1:&#x27;</span>, net1)<br>print(<span class="hljs-string">&#x27;net2:&#x27;</span>, net2)<br>print(<span class="hljs-string">&#x27;net3:&#x27;</span>, net3)<br>output:<br><br>net1: Sequential(<br>  (conv): Conv2d (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (batchnorm): BatchNorm2d(<span class="hljs-number">3</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>)<br>  (activation_layer): ReLU()<br>)<br>net2: Sequential(<br>  (<span class="hljs-number">0</span>): Conv2d (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (<span class="hljs-number">1</span>): BatchNorm2d(<span class="hljs-number">3</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>)<br>  (<span class="hljs-number">2</span>): ReLU()<br>)<br>net3: Sequential(<br>  (conv1): Conv2d (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>  (bn1): BatchNorm2d(<span class="hljs-number">3</span>, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>)<br>  (relu1): ReLU()<br>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 可根据名字或序号取出子module</span><br>net1.conv, net2[<span class="hljs-number">0</span>], net3.conv1<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span> = Variable(torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>))<br>output = net1(<span class="hljs-built_in">input</span>)<br>output = net2(<span class="hljs-built_in">input</span>)<br>output = net3(<span class="hljs-built_in">input</span>)<br>output = net3.relu1(net1.batchnorm(net1.conv(<span class="hljs-built_in">input</span>)))<br></code></pre></td></tr></table></figure>
<ul>
<li><code>ModuleList</code>是<code>Module</code>的子类，当在<code>Module</code>中使用它的时候，就能自动识别为子module。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyModule</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(MyModule, self).__init__()<br>        self.<span class="hljs-built_in">list</span> = [nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>), nn.ReLU()]<br>        self.module_list = nn.ModuleList([nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>), nn.ReLU()])<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">pass</span><br>model = MyModule()<br>model<br>output:<br>MyModule(<br>  (module_list): ModuleList(<br>    (<span class="hljs-number">0</span>): Conv2d (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), stride=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    (<span class="hljs-number">1</span>): ReLU()<br>  )<br>)<br></code></pre></td></tr></table></figure>
<ul>
<li>list中的子module并不能被主module所识别，而ModuleList中的子module能够被主module所识别。这意味着如果用list保存子module，将无法调整其参数，因其未加入到主module的参数中。</li>
<li>除ModuleList之外还有ParameterList，其是一个可以包含多个parameter的类list对象。在实际应用中，使用方式与ModuleList类似。如果在构造函数<code>__init__</code>中用到list、tuple、dict等对象时，一定要思考是否应该用ModuleList或ParameterList代替。</li>
</ul>
<h2 id="循环神经网络层rnn"><a class="markdownIt-Anchor" href="#循环神经网络层rnn"></a> 循环神经网络层(RNN)</h2>
<p>近些年随着深度学习和自然语言处理的结合加深，RNN的使用也越来越多，关于RNN的基础知识，推荐阅读colah的文章[^4]入门。PyTorch中实现了如今最常用的三种RNN：RNN（vanilla RNN）、LSTM和GRU。此外还有对应的三种RNNCell。</p>
<p>RNN和RNNCell层的区别在于前者一次能够处理整个序列，而后者一次只处理序列中一个时间点的数据，前者封装更完备更易于使用，后者更具灵活性。实际上RNN层的一种后端实现方式就是调用RNNCell来实现的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">1000</span>)<br><span class="hljs-comment"># 输入：batch_size=3，序列长度都为2，序列中每个元素占4维</span><br><span class="hljs-built_in">input</span> = Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br><span class="hljs-comment"># lstm输入向量4维，隐藏元3，1层</span><br>lstm = nn.LSTM(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br><span class="hljs-comment"># 初始状态：1层，batch_size=3，3个隐藏元</span><br>h0 = Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>c0 = Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>out, hn = lstm(<span class="hljs-built_in">input</span>, (h0, c0))<br>out<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">1000</span>)<br><span class="hljs-built_in">input</span> = Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br><span class="hljs-comment"># 一个LSTMCell对应的层数只能是一层</span><br>lstm = nn.LSTMCell(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>)<br>hx = Variable(torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>cx = Variable(torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>out = []<br><span class="hljs-keyword">for</span> i_ <span class="hljs-keyword">in</span> <span class="hljs-built_in">input</span>:<br>    hx, cx=lstm(i_, (hx, cx))<br>    out.append(hx)<br>torch.stack(out)<br></code></pre></td></tr></table></figure>
<ul>
<li>词向量在自然语言中应用十分普及，PyTorch同样提供了Embedding层。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 有4个词，每个词用5维的向量表示</span><br>embedding = nn.Embedding(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br><span class="hljs-comment"># 可以用预训练好的词向量初始化embedding</span><br>embedding.weight.data = torch.arange(<span class="hljs-number">0</span>,<span class="hljs-number">20</span>).view(<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span> =Variable(torch.arange(<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>)).long()<br>output = embedding(<span class="hljs-built_in">input</span>)<br>output<br></code></pre></td></tr></table></figure>
<h2 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h2>
<p>在深度学习中要用到各种各样的损失函数（loss function），这些损失函数可看作是一种特殊的layer，PyTorch也将这些损失函数实现为<code>nn.Module</code>的子类。然而在实际使用中通常将这些loss function专门提取出来，和主模型互相独立。详细的loss使用请参照文档，这里以分类中最常用的交叉熵损失CrossEntropyloss为例说明。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># batch_size=3，计算对应每个类别的分数（只有两个类别）</span><br>score = Variable(torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>))<br><span class="hljs-comment"># 三个样本分别属于1，0，1类，label必须是LongTensor</span><br>label = Variable(torch.Tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])).long()<br><span class="hljs-comment"># loss与普通的layer无差异</span><br>criterion = nn.CrossEntropyLoss()<br>loss = criterion(score, label)<br>loss<br></code></pre></td></tr></table></figure>
<h1 id="优化器"><a class="markdownIt-Anchor" href="#优化器"></a> 优化器</h1>
<p>PyTorch将深度学习中常用的优化方法全部封装在<code>torch.optim</code>中，其设计十分灵活，能够很方便的扩展成自定义的优化方法。</p>
<p>所有的优化方法都是继承基类<code>optim.Optimizer</code>，并实现了自己的优化步骤。下面就以最基本的优化方法——随机梯度下降法（SGD）举例说明。这里需重点掌握：</p>
<ul>
<li>优化方法的基本使用方法</li>
<li>如何对模型的不同部分设置不同的学习率</li>
<li>如何调整学习率</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment"># 首先定义一个LeNet网络</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        self.features = nn.Sequential(<br>                    nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>),<br>                    nn.ReLU(),<br>                    nn.MaxPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),<br>                    nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>),<br>                    nn.ReLU(),<br>                    nn.MaxPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>        )<br>        self.classifier = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        x = self.features(x)<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>)<br>        x = self.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br><br>net = Net()<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span>  optim<br>optimizer = optim.SGD(params=net.parameters(), lr=<span class="hljs-number">1</span>)<br>optimizer.zero_grad() <span class="hljs-comment"># 梯度清零，等价于net.zero_grad()</span><br><br><span class="hljs-built_in">input</span> = Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))<br>output = net(<span class="hljs-built_in">input</span>)<br>output.backward(output) <span class="hljs-comment"># fake backward</span><br>optimizer.step() <span class="hljs-comment"># 执行优化</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 为不同子网络设置不同的学习率，在finetune中经常用到</span><br><span class="hljs-comment"># 如果对某个参数不指定学习率，就使用最外层的默认学习率</span><br>optimizer =optim.SGD([<br>                &#123;<span class="hljs-string">&#x27;params&#x27;</span>: net.features.parameters()&#125;, <span class="hljs-comment"># 学习率为1e-5</span><br>                &#123;<span class="hljs-string">&#x27;params&#x27;</span>: net.classifier.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">1e-2</span>&#125;<br>            ], lr=<span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 只为两个全连接层设置较大的学习率，其余层的学习率较小</span><br>special_layers = nn.ModuleList([net.classifier[<span class="hljs-number">0</span>], net.classifier[<span class="hljs-number">3</span>]])<br>special_layers_params = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">id</span>, special_layers.parameters()))<br>base_params = <span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: <span class="hljs-built_in">id</span>(p) <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> special_layers_params,<br>                     net.parameters())<br><br>optimizer = torch.optim.SGD([<br>            &#123;<span class="hljs-string">&#x27;params&#x27;</span>: base_params&#125;,<br>            &#123;<span class="hljs-string">&#x27;params&#x27;</span>: special_layers.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0.01</span>&#125;<br>        ], lr=<span class="hljs-number">0.001</span> )<br><br></code></pre></td></tr></table></figure>
<ul>
<li>对于如何调整学习率，主要有两种做法。一种是修改optimizer.param_groups中对应的学习率，另一种是更简单也是较为推荐的做法——新建优化器，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 调整学习率，新建一个optimizer</span><br>old_lr = <span class="hljs-number">0.1</span><br>optimizer =optim.SGD([<br>                &#123;<span class="hljs-string">&#x27;params&#x27;</span>: net.features.parameters()&#125;,<br>                &#123;<span class="hljs-string">&#x27;params&#x27;</span>: net.classifier.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: old_lr*<span class="hljs-number">0.1</span>&#125;<br>            ], lr=<span class="hljs-number">1e-5</span>)<br><br></code></pre></td></tr></table></figure>
<h2 id="nnfunctional"><a class="markdownIt-Anchor" href="#nnfunctional"></a> nn.functional</h2>
<p>nn中还有一个很常用的模块：<code>nn.functional</code>，nn中的大多数layer，在<code>functional</code>中都有一个与之相对应的函数。<code>nn.functional</code>中的函数和<code>nn.Module</code>的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由<code>class layer(nn.Module)</code>定义，会自动提取可学习的参数。而<code>nn.functional</code>中的函数更像是纯函数，由<code>def function(input)</code>定义。下面举例说明functional的使用，并指出二者的不同之处。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span> = Variable(torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>))<br>model = nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>output1 = model(<span class="hljs-built_in">input</span>)<br>output2 = nn.functional.linear(<span class="hljs-built_in">input</span>, model.weight, model.bias)<br>output1 == output2<br>output:<br>tensor([[<span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>],<br>        [<span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>]])<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">b = nn.functional.relu(<span class="hljs-built_in">input</span>)<br>b2 = nn.ReLU()(<span class="hljs-built_in">input</span>)<br>b == b2<br>output：<br>tensor([[<span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>],<br>        [<span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>]])<br></code></pre></td></tr></table></figure>
<ul>
<li>应该什么时候使用nn.Module，什么时候使用nn.functional呢？答案很简单，如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用取决于个人的喜好。如激活函数（ReLU、sigmoid、tanh），池化（MaxPool）等层由于没有可学习参数，则可以使用对应的functional函数代替，而对于卷积、全连接等具有可学习参数的网络建议使用nn.Module。下面举例说明，如何在模型中搭配使用nn.Module和nn.functional。另外虽然dropout操作也没有可学习操作，但建议还是使用<code>nn.Dropout</code>而不是<code>nn.functional.dropout</code>，因为dropout在训练和测试两个阶段的行为有所差别，使用<code>nn.Module</code>对象能够通过<code>model.eval</code>操作加以区分。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)<br>        self.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        self.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment">#激活函数使用了functional中的函数</span><br>        x = F.pool(F.relu(self.conv1(x)), <span class="hljs-number">2</span>)<br>        x = F.pool(F.relu(self.conv2(x)), <span class="hljs-number">2</span>)<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>)<br>        x = F.relu(self.fc1(x))<br>        x = F.relu(self.fc2(x))<br>        x = self.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<ul>
<li>对于不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样则可以不用放置在构造函数<code>__init__</code>中。对于有可学习参数的模块，也可以用functional来代替，只不过实现起来较为繁琐，需要手动定义参数parameter，如前面实现自定义的全连接层，就可将weight和bias两个参数单独拿出来，在构造函数中初始化为parameter。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLinear</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(MyLinear, self).__init__()<br>        self.weight = nn.Parameter(t.randn(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br>        self.bias = nn.Parameter(t.zeros(<span class="hljs-number">3</span>))<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> F.linear(<span class="hljs-built_in">input</span>, weight, bias)<br></code></pre></td></tr></table></figure>
<h1 id="初始化策略"><a class="markdownIt-Anchor" href="#初始化策略"></a> 初始化策略</h1>
<p>在深度学习中参数的初始化十分重要，良好的初始化能让模型更快收敛，并达到更高水平，而糟糕的初始化则可能使得模型迅速瘫痪。PyTorch中nn.Module的模块参数都采取了较为合理的初始化策略，因此一般不用我们考虑，当然我们也可以用自定义初始化去代替系统的默认初始化。而当我们在使用Parameter时，自定义初始化则尤为重要，因t.Tensor()返回的是内存中的随机数，很可能会有极大值，这在实际训练网络中会造成溢出或者梯度消失。<strong>PyTorch中<code>nn.init</code>模块就是专门为初始化而设计</strong>，如果某种初始化策略<code>nn.init</code>不提供，用户也可以自己直接初始化。</p>
<ul>
<li>利用nn.init初始化</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 利用nn.init初始化</span><br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> init<br>linear = nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>torch.manual_seed(<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 等价于 linear.weight.data.normal_(0, std)</span><br>init.xavier_normal(linear.weight)<br></code></pre></td></tr></table></figure>
<ul>
<li>直接初始化</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 直接初始化</span><br><span class="hljs-keyword">import</span> math<br>torch.manual_seed(<span class="hljs-number">1</span>)<br><span class="hljs-comment"># xavier初始化的计算公式</span><br>std = math.sqrt(<span class="hljs-number">2</span>)/math.sqrt(<span class="hljs-number">7.</span>)<br>linear.weight.data.normal_(<span class="hljs-number">0</span>,std)<br></code></pre></td></tr></table></figure>
<ul>
<li>对模型的所有参数进行初始化</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 对模型的所有参数进行初始化</span><br><span class="hljs-keyword">for</span> name, params <span class="hljs-keyword">in</span> net.named_parameters():<br>    <span class="hljs-keyword">if</span> name.find(<span class="hljs-string">&#x27;linear&#x27;</span>) != -<span class="hljs-number">1</span>:<br>        <span class="hljs-comment"># init linear</span><br>        params[<span class="hljs-number">0</span>] <span class="hljs-comment"># weight</span><br>        params[<span class="hljs-number">1</span>] <span class="hljs-comment"># bias</span><br>    <span class="hljs-keyword">elif</span> name.find(<span class="hljs-string">&#x27;conv&#x27;</span>) != -<span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">pass</span><br>    <span class="hljs-keyword">elif</span> name.find(<span class="hljs-string">&#x27;norm&#x27;</span>) != -<span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">pass</span><br><br></code></pre></td></tr></table></figure>
<h1 id="nnmodule深入分析"><a class="markdownIt-Anchor" href="#nnmodule深入分析"></a> nn.Module深入分析</h1>
<p>如果想要更深入地理解nn.Module，究其原理是很有必要的。首先来看看nn.Module基类的构造函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>    self._parameters = OrderedDict()<br>    self._modules = OrderedDict()<br>    self._buffers = OrderedDict()<br>    self._backward_hooks = OrderedDict()<br>    self._forward_hooks = OrderedDict()<br>    self.training = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure>
<p>其中每个属性的解释如下：</p>
<ul>
<li><code>_parameters</code>：字典，保存用户直接设置的parameter，<code>self.param1 = nn.Parameter(t.randn(3, 3))</code>会被检测到，在字典中加入一个key为’param’，value为对应parameter的item。而self.submodule = nn.Linear(3, 4)中的parameter则不会存于此。</li>
<li><code>_modules</code>：子module，通过<code>self.submodel = nn.Linear(3, 4)</code>指定的子module会保存于此。</li>
<li><code>_buffers</code>：缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。</li>
<li><code>_backward_hooks</code>与<code>_forward_hooks</code>：钩子技术，用来提取中间变量，类似variable的hook。</li>
<li><code>training</code>：BatchNorm与Dropout层在训练阶段和测试阶段中采取的策略不同，通过判断training值来决定前向传播策略。</li>
</ul>
<p>上述几个属性中，<code>_parameters</code>、<code>_modules</code>和<code>_buffers</code>这三个字典中的键值，都可以通过<code>self.key</code>方式获得，效果等价于<code>self._parameters['key']</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        <span class="hljs-comment"># 等价与self.register_parameter(&#x27;param1&#x27; ,nn.Parameter(t.randn(3, 3)))</span><br>        self.param1 = nn.Parameter(torch.rand(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>        self.submodel1 = nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>) <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):</span><br>        x = self.param1.mm(<span class="hljs-built_in">input</span>)<br>        x = self.submodel1(x)<br>        <span class="hljs-keyword">return</span> x<br>net = Net()<br>net<br>output:<br>Net(<br>  (submodel1): Linear(in_features=<span class="hljs-number">3</span>, out_features=<span class="hljs-number">4</span>, bias=<span class="hljs-literal">True</span>)<br>)<br><br>net._modules<br>output:<br>OrderedDict([(<span class="hljs-string">&#x27;submodel1&#x27;</span>, Linear(in_features=<span class="hljs-number">3</span>, out_features=<span class="hljs-number">4</span>, bias=<span class="hljs-literal">True</span>))])<br><br>net._parameters<br>output:<br>OrderedDict([(<span class="hljs-string">&#x27;param1&#x27;</span>, Parameter containing:<br>               <span class="hljs-number">0.3398</span>  <span class="hljs-number">0.5239</span>  <span class="hljs-number">0.7981</span><br>               <span class="hljs-number">0.7718</span>  <span class="hljs-number">0.0112</span>  <span class="hljs-number">0.8100</span><br>               <span class="hljs-number">0.6397</span>  <span class="hljs-number">0.9743</span>  <span class="hljs-number">0.8300</span><br>              [torch.FloatTensor of size 3x3])])<br>              <br>net.param1 <span class="hljs-comment"># 等价于net._parameters[&#x27;param1&#x27;]</span><br>output:<br>Parameter containing:<br> <span class="hljs-number">0.3398</span>  <span class="hljs-number">0.5239</span>  <span class="hljs-number">0.7981</span><br> <span class="hljs-number">0.7718</span>  <span class="hljs-number">0.0112</span>  <span class="hljs-number">0.8100</span><br> <span class="hljs-number">0.6397</span>  <span class="hljs-number">0.9743</span>  <span class="hljs-number">0.8300</span><br>[torch.FloatTensor of size 3x3]<br></code></pre></td></tr></table></figure>
<ul>
<li>nn.Module在实际使用中可能层层嵌套，一个module包含若干个子module，每一个子module又包含了更多的子module。为方便用户访问各个子module，nn.Module实现了很多方法，如函数<code>children</code>可以查看直接子module，函数<code>module</code>可以查看所有的子module（包括当前module）。与之相对应的还有函数<code>named_childen</code>和<code>named_modules</code>，其能够在返回module列表的同时返回它们的名字。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span> = Variable(torch.arange(<span class="hljs-number">0</span>, <span class="hljs-number">12</span>).view(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>).<span class="hljs-built_in">float</span>())<br>model = nn.Dropout()<br><span class="hljs-comment"># 在训练阶段，会有一半左右的数被随机置为0</span><br>model(<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model.training  = <span class="hljs-literal">False</span><br><span class="hljs-comment"># 在测试阶段，dropout什么都不做</span><br>model(<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure>
<ul>
<li>对于batchnorm、dropout、instancenorm等在训练和测试阶段行为差距巨大的层，如果在测试时不将其training值设为True，则可能会有很大影响，这在实际使用中要千万注意。虽然可通过直接设置<code>training</code>属性，来将子module设为train和eval模式，但这种方式较为繁琐，因如果一个模型具有多个dropout层，就需要为每个dropout层指定training属性。更为推荐的做法是调用<code>model.train()</code>函数，它会将当前module及其子module中的所有training属性都设为True，相应的，<code>model.eval()</code>函数会把training属性都设为False。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">print(net.training, net.submodel1.training)<br>net.<span class="hljs-built_in">eval</span>()<br>net.training, net.submodel1.training<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">list</span>(net.named_modules())<br></code></pre></td></tr></table></figure>
<p><code>register_forward_hook</code>与<code>register_backward_hook</code>，这两个函数的功能类似于variable函数的<code>register_hook</code>，可在module前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：<code>hook(module, input, output) -&gt; None</code>，而反向传播则具有如下形式：<code>hook(module, grad_input, grad_output) -&gt; Tensor or None</code>。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在forward函数中，但如果在forward函数中专门加上这些处理，可能会使处理逻辑比较复杂，这时候使用钩子技术就更合适一些。下面考虑一种场景，有一个预训练好的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，但又不希望修改其原有的模型定义文件，这时就可以利用钩子函数。下面给出实现的伪代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">model = VGG()<br>features = t.Tensor()<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hook</span>(<span class="hljs-params">module, <span class="hljs-built_in">input</span>, output</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;把这层的输出拷贝到features中&#x27;&#x27;&#x27;</span><br>    features.copy_(output.data)<br>    <br>handle = model.layer8.register_forward_hook(hook)<br>_ = model(<span class="hljs-built_in">input</span>)<br><span class="hljs-comment"># 用完hook后删除</span><br>handle.remove()<br></code></pre></td></tr></table></figure>
<p><code>nn.Module</code>对象在构造函数中的行为看起来有些怪异，如果想要真正掌握其原理，就需要看两个魔法方法<code>__getattr__</code>和<code>__setattr__</code>。在Python中有两个常用的buildin方法<code>getattr</code>和<code>setattr</code>，<code>getattr(obj, 'attr1')</code>等价于<code>obj.attr</code>，如果<code>getattr</code>函数无法找到所需属性，Python会转而调用<code>obj.__getattr__('attr1')</code>方法，即<code>getattr</code>函数无法找到的交给<code>__getattr__</code>函数处理，没有实现<code>__getattr__</code>或者<code>__getattr__</code>也无法处理的就会raise AttributeError。<code>setattr(obj, 'name', value)</code>等价于<code>obj.name=value</code>，如果obj对象实现了<code>__setattr__</code>方法，setattr会直接调用<code>obj.__setattr__('name', value)</code>，否则调用buildin方法。总结一下：</p>
<ul>
<li>result  = obj.name会调用buildin函数<code>getattr(obj, 'name')</code>，如果该属性找不到，会调用<code>obj.__getattr__('name')</code></li>
<li><a target="_blank" rel="noopener" href="http://obj.name">obj.name</a> = value会调用buildin函数<code>setattr(obj, 'name', value)</code>，如果obj对象实现了<code>__setattr__</code>方法，<code>setattr</code>会直接调用<code>obj.__setattr__('name', value')</code></li>
</ul>
<p>nn.Module实现了自定义的<code>__setattr__</code>函数，当执行<code>module.name=value</code>时，会在<code>__setattr__</code>中判断value是否为<code>Parameter</code>或<code>nn.Module</code>对象，如果是则将这些对象加到<code>_parameters</code>和<code>_modules</code>两个字典中，而如果是其它类型的对象，如<code>Variable</code>、<code>list</code>、<code>dict</code>等，则调用默认的操作，将这个值保存在<code>__dict__</code>中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">module = nn.Module()<br>module.param = nn.Parameter(torch.ones(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>module._parameters<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">submodule1 = nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>submodule2 = nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>module_list =  [submodule1, submodule2]<br><span class="hljs-comment"># 对于list对象，调用buildin函数，保存在__dict__中</span><br>module.submodules = module_list<br>print(<span class="hljs-string">&#x27;_modules: &#x27;</span>, module._modules)<br>print(<span class="hljs-string">&quot;__dict__[&#x27;submodules&#x27;]:&quot;</span>,module.__dict__.get(<span class="hljs-string">&#x27;submodules&#x27;</span>))<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">module_list = nn.ModuleList(module_list)<br>module.submodules = module_list<br>print(<span class="hljs-string">&#x27;ModuleList is instance of nn.Module: &#x27;</span>, <span class="hljs-built_in">isinstance</span>(module_list, nn.Module))<br>print(<span class="hljs-string">&#x27;_modules: &#x27;</span>, module._modules)<br>print(<span class="hljs-string">&quot;__dict__[&#x27;submodules&#x27;]:&quot;</span>, module.__dict__.get(<span class="hljs-string">&#x27;submodules&#x27;</span>))<br></code></pre></td></tr></table></figure>
<ul>
<li>因<code>_modules</code>和<code>_parameters</code>中的item未保存在<code>__dict__</code>中，所以默认的getattr方法无法获取它，因而<code>nn.Module</code>实现了自定义的<code>__getattr__</code>方法，如果默认的<code>getattr</code>无法处理，就调用自定义的<code>__getattr__</code>方法，尝试从<code>_modules</code>、<code>_parameters</code>和<code>_buffers</code>这三个字典中获取。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">getattr</span>(module, <span class="hljs-string">&#x27;training&#x27;</span>) <span class="hljs-comment"># 等价于module.training</span><br><span class="hljs-comment"># error</span><br><span class="hljs-comment"># module.__getattr__(&#x27;training&#x27;)</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">module.attr1 = <span class="hljs-number">2</span><br><span class="hljs-built_in">getattr</span>(module, <span class="hljs-string">&#x27;attr1&#x27;</span>)<br><span class="hljs-comment"># 报错</span><br><span class="hljs-comment"># module.__getattr__(&#x27;attr1&#x27;)</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 即module.param, 会调用module.__getattr__(&#x27;param&#x27;)</span><br><span class="hljs-built_in">getattr</span>(module, <span class="hljs-string">&#x27;param&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h1 id="模型的保存"><a class="markdownIt-Anchor" href="#模型的保存"></a> 模型的保存</h1>
<ul>
<li>在PyTorch中保存模型十分简单，所有的Module对象都具有state_dict()函数，返回当前Module所有的状态数据。将这些状态数据保存后，下次使用模型时即可利用<code>model.load_state_dict()</code>函数将状态加载进来。优化器（optimizer）也有类似的机制，不过一般并不需要保存优化器的运行状态。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 保存模型</span><br>torch.save(net.state_dict(), <span class="hljs-string">&#x27;net.pth&#x27;</span>)<br><span class="hljs-comment"># 加载已保存的模型</span><br>net2 = Net()<br>net2.load_state_dict(torch.load(<span class="hljs-string">&#x27;net.pth&#x27;</span>))<br></code></pre></td></tr></table></figure>
<ul>
<li>实际上还有另外一种保存方法，但因其严重依赖模型定义方式及文件路径结构等，很容易出问题，因而不建议使用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(net, <span class="hljs-string">&#x27;net_all.pth&#x27;</span>)<br>net2 = torch.load(<span class="hljs-string">&#x27;net_all.pth&#x27;</span>)<br>net2<br></code></pre></td></tr></table></figure>
<h1 id="使用gpu运算"><a class="markdownIt-Anchor" href="#使用gpu运算"></a> 使用GPU运算</h1>
<p>将Module放在GPU上运行也十分简单，只需两步：</p>
<ul>
<li>model = model.cuda()：将模型的所有参数转存到GPU</li>
<li>input.cuda()：将输入数据也放置到GPU上</li>
</ul>
<p>至于如何在多个GPU上并行计算，PyTorch也提供了两个函数，可实现简单高效的并行GPU计算</p>
<ul>
<li>nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)</li>
<li>class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)</li>
</ul>
<p>可见二者的参数十分相似，通过<code>device_ids</code>参数可以指定在哪些GPU上进行优化，output_device指定输出到哪个GPU上。唯一的不同就在于前者直接利用多GPU并行计算得出结果，而后者则返回一个新的module，能够自动在多GPU上进行并行加速。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plain"># method 1<br>new_net &#x3D; nn.DataParallel(net, device_ids&#x3D;[0, 1])<br>output &#x3D; new_net(input)<br># method 2<br>output &#x3D; nn.parallel.data_parallel(new_net, input, device_ids&#x3D;[0, 1])<br></code></pre></td></tr></table></figure>
<p>DataParallel并行的方式，是将输入一个batch的数据均分成多份，分别送到对应的GPU进行计算，各个GPU得到的梯度累加。与Module相关的所有数据也都会以浅复制的方式复制多份，在此需要注意，在module中属性应该是只读的。</p>
<h1 id="nn和autograd的关系"><a class="markdownIt-Anchor" href="#nn和autograd的关系"></a> nn和autograd的关系</h1>
<p>nn.Module利用的也是autograd技术，其主要工作是实现前向传播。在forward函数中，nn.Module对输入的Variable进行的各种操作，本质上都是用到了autograd技术。这里需要对比autograd.Function和nn.Module之间的区别：</p>
<ul>
<li>autograd.Function利用了Tensor对autograd技术的扩展，为autograd实现了新的运算op，不仅要实现前向传播还要手动实现反向传播</li>
<li>nn.Module利用了autograd技术，对nn的功能进行扩展，实现了深度学习中更多的层。只需实现前向传播功能，autograd即会自动实现反向传播</li>
<li>nn.functional是一些autograd操作的集合，是经过封装的函数</li>
</ul>
<p>作为两大类扩充PyTorch接口的方法，我们在实际使用中应该如何选择呢？如果某一个操作，在autograd中尚未支持，那么只能实现Function接口对应的前向传播和反向传播。如果某些时候利用autograd接口比较复杂，则可以利用Function将多个操作聚合，实现优化，正如<code>Sigmoid</code>一样，比直接利用autograd低级别的操作要快。而如果只是想在深度学习中增加某一层，使用nn.Module进行封装则更为简单高效。</p>
<h1 id="搭建resnet"><a class="markdownIt-Anchor" href="#搭建resnet"></a> 搭建ResNet</h1>
<p>Kaiming He的深度残差网络（ResNet）[^7]在深度学习的发展中起到了很重要的作用，ResNet不仅一举拿下了当年CV下多个比赛项目的冠军，更重要的是这一结构解决了训练极深网络时的梯度消失问题。</p>
<p>首先来看看ResNet的网络结构，这里选取的是ResNet的一个变种：ResNet34。ResNet的网络结构如图所示，可见除了最开始的卷积池化和最后的池化全连接之外，网络中有很多结构相似的单元，这些重复单元的共同点就是有个跨层直连的shortcut。ResNet中将一个跨层直连的单元称为Residual block，其结构如图所示，左边部分是普通的卷积网络结构，右边是直连，但如果输入和输出的通道数不一致，或其步长不为1，那么就需要有一个专门的单元将二者转成一致，使其可以相加。</p>
<p>另外我们可以发现Residual block的大小也是有规律的，在最开始的pool之后有连续的几个一模一样的Residual block单元，这些单元的通道数一样，在这里我们将这几个拥有多个Residual block单元的结构称之为layer，注意和之前讲的layer区分开来，这里的layer是几个层的集合。</p>
<p>考虑到Residual block和layer出现了多次，我们可以把它们实现为一个子Module或函数。这里我们将Residual block实现为一个子moduke，而将layer实现为一个函数。下面是实现代码，规律总结如下：</p>
<ul>
<li>对于模型中的重复部分，实现为子module或用函数生成相应的module<code>make_layer</code></li>
<li>nn.Module和nn.Functional结合使用</li>
<li>尽量使用<code>nn.Seqential</code><br />
<img src="https://img-blog.csdnimg.cn/20201209211947838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span>  nn<br><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span>  functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ResidualBlock</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    实现子module: Residual Block</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, inchannel, outchannel, stride=<span class="hljs-number">1</span>, shortcut=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-built_in">super</span>(ResidualBlock, self).__init__()<br>        self.left = nn.Sequential(<br>                nn.Conv2d(inchannel,outchannel,<span class="hljs-number">3</span>,stride, <span class="hljs-number">1</span>,bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(outchannel),<br>                nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>                nn.Conv2d(outchannel,outchannel,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(outchannel) )<br>        self.right = shortcut<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        out = self.left(x)<br>        residual = x <span class="hljs-keyword">if</span> self.right <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.right(x)<br>        out += residual<br>        <span class="hljs-keyword">return</span> F.relu(out)<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ResNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    实现主module：ResNet34</span><br><span class="hljs-string">    ResNet34 包含多个layer，每个layer又包含多个residual block</span><br><span class="hljs-string">    用子module来实现residual block，用_make_layer函数来实现layer</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">1000</span></span>):</span><br>        <span class="hljs-built_in">super</span>(ResNet, self).__init__()<br>        <span class="hljs-comment"># 前几层图像转换</span><br>        self.pre = nn.Sequential(<br>                nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, <span class="hljs-number">7</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>                nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>                nn.MaxPool2d(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>))<br>        <br>        <span class="hljs-comment"># 重复的layer，分别有3，4，6，3个residual block</span><br>        self.layer1 = self._make_layer( <span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>)<br>        self.layer2 = self._make_layer( <span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>)<br>        self.layer3 = self._make_layer( <span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">6</span>, stride=<span class="hljs-number">2</span>)<br>        self.layer4 = self._make_layer( <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)<br><br>        <span class="hljs-comment">#分类用的全连接</span><br>        self.fc = nn.Linear(<span class="hljs-number">512</span>, num_classes)<br>    <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_make_layer</span>(<span class="hljs-params">self,  inchannel, outchannel, block_num, stride=<span class="hljs-number">1</span></span>):</span><br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        构建layer,包含多个residual block</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        shortcut = nn.Sequential(<br>                nn.Conv2d(inchannel,outchannel,<span class="hljs-number">1</span>,stride, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(outchannel))<br>        layers = []<br>        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))<br>        <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, block_num):<br>            layers.append(ResidualBlock(outchannel, outchannel))<br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br>        <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        x = self.pre(x)<br>        <br>        x = self.layer1(x)<br>        x = self.layer2(x)<br>        x = self.layer3(x)<br>        x = self.layer4(x)<br><br>        x = F.avg_pool2d(x, <span class="hljs-number">7</span>)<br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> self.fc(x)<br><br>model = ResNet()<br><span class="hljs-built_in">input</span>  = torch.autograd.Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br>o = model(<span class="hljs-built_in">input</span>)<br><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<br>model = models.resnet34()<br></code></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">PyTorch常用工具模块</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/09/Pytorch/Pytorch-and-Autograd/"><img class="next-cover" src="/2020/12/09/Pytorch/Pytorch-and-Autograd/torch.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Pytorch中的Autograd</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-12</div><div class="title">Pytorch强化学习算法实现</div></div></a></div><div><a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">PyTorch常用工具模块</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd"><img class="cover" src="/2020/12/09/Pytorch/Pytorch-and-Autograd/torch.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorch中的Autograd</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-and-Tensor/" title="Pytorch中的Tensor"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">Pytorch中的Tensor</div></div></a></div><div><a href="/2020/12/09/Pytorch/Pytorch-Introductory-knowledge/" title="PyTorch入门知识"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-09</div><div class="title">PyTorch入门知识</div></div></a></div><div><a href="/2020/12/08/Tensorflow/Tensorflow-and-Reinforcement-learning/" title="Tensorflow与强化学习"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-08</div><div class="title">Tensorflow与强化学习</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ccclll777</div><div class="author-info__description">胸怀猛虎 细嗅蔷薇</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ccclll777"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccclll777" target="_blank" title="fab fa-github"><i class="GitHub"></i></a><a class="social-icon" href="mailto:sdu945860882@gmail.com" target="_blank" title="fa fa-envelope"><i class="E-Mail"></i></a><a class="social-icon" href="https://www.weibo.com/6732062654" target="_blank" title="fab fa-weibo"><i class="Weibo"></i></a><a class="social-icon" href="https://blog.csdn.net/baidu_41871794" target="_blank" title="gratipay"><i class="CSDN"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8torchnn%E5%AE%9E%E7%8E%B0%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="toc-number">1.</span> <span class="toc-text"> 使用torch.nn实现全连接层</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="toc-number">2.</span> <span class="toc-text"> 常用神经网络层</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E7%9B%B8%E5%85%B3%E5%B1%82"><span class="toc-number">2.1.</span> <span class="toc-text"> 图像相关层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text"> 激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82rnn"><span class="toc-number">2.3.</span> <span class="toc-text"> 循环神经网络层(RNN)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.4.</span> <span class="toc-text"> 损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text"> 优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#nnfunctional"><span class="toc-number">3.1.</span> <span class="toc-text"> nn.functional</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E7%AD%96%E7%95%A5"><span class="toc-number">4.</span> <span class="toc-text"> 初始化策略</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#nnmodule%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90"><span class="toc-number">5.</span> <span class="toc-text"> nn.Module深入分析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98"><span class="toc-number">6.</span> <span class="toc-text"> 模型的保存</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8gpu%E8%BF%90%E7%AE%97"><span class="toc-number">7.</span> <span class="toc-text"> 使用GPU运算</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#nn%E5%92%8Cautograd%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">8.</span> <span class="toc-text"> nn和autograd的关系</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%90%AD%E5%BB%BAresnet"><span class="toc-number">9.</span> <span class="toc-text"> 搭建ResNet</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符"><img src="/2021/10/16/Arithmetic-LeetCode/282/show.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Leetcode 282. 给表达式添加运算符"/></a><div class="content"><a class="title" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符">Leetcode 282. 给表达式添加运算符</a><time datetime="2021-10-16T15:35:16.000Z" title="发表于 2021-10-16 23:35:16">2021-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch强化学习算法实现"/></a><div class="content"><a class="title" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现">Pytorch强化学习算法实现</a><time datetime="2020-12-12T02:54:37.000Z" title="发表于 2020-12-12 10:54:37">2020-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyTorch常用工具模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块">PyTorch常用工具模块</a><time datetime="2020-12-09T13:32:23.000Z" title="发表于 2020-12-09 21:32:23">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中神经网络工具箱nn模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块">Pytorch中神经网络工具箱nn模块</a><time datetime="2020-12-09T13:25:38.000Z" title="发表于 2020-12-09 21:25:38">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd"><img src="/2020/12/09/Pytorch/Pytorch-and-Autograd/torch.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中的Autograd"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd">Pytorch中的Autograd</a><time datetime="2020-12-09T13:25:19.000Z" title="发表于 2020-12-09 21:25:19">2020-12-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By ccclll777</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>