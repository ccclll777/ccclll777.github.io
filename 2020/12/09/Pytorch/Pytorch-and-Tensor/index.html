<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Pytorch中的Tensor | ccclll777&#39;s blogs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="介绍Pytorch中的张量系统（Tensor）">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch中的Tensor">
<meta property="og:url" content="http://yoursite.com/2020/12/09/Pytorch/Pytorch-and-Tensor/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="介绍Pytorch中的张量系统（Tensor）">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201209150955171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2020-12-09T13:25:08.000Z">
<meta property="article:modified_time" content="2021-10-16T15:32:53.621Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="python">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20201209150955171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="ccclll777&#39;s blogs" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">ccclll777&#39;s blogs</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Pytorch/Pytorch-and-Tensor" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/12/09/Pytorch/Pytorch-and-Tensor/" class="article-date">
  <time datetime="2020-12-09T13:25:08.000Z" itemprop="datePublished">2020-12-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Pytorch中的Tensor
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>介绍Pytorch中的张量系统（Tensor）<br><span id="more"></span></p>
<h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch  <span class="keyword">as</span> t</span><br><span class="line">t.__version__</span><br></pre></td></tr></table></figure>
<h2 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h2><p>从接口的角度来讲，对tensor的操作可分为两类：</p>
<ol>
<li><code>torch.function</code>，如<code>torch.save</code>等。</li>
<li>另一类是<code>tensor.function</code>，如<code>tensor.view</code>等。</li>
</ol>
<p>为方便使用，对tensor的大部分操作同时支持这两类接口，在本书中不做具体区分，如<code>torch.sum (torch.sum(a, b))</code>与<code>tensor.sum (a.sum(b))</code>功能等价。</p>
<p>而从存储的角度来讲，对tensor的操作又可分为两类：</p>
<ol>
<li>不会修改自身的数据，如 <code>a.add(b)</code>， 加法的结果会返回一个新的tensor。</li>
<li>会修改自身的数据，如 <code>a.add_(b)</code>， 加法的结果仍存储在a中，a被修改了。</li>
</ol>
<p>函数名以<code>_</code>结尾的都是inplace方式, 即会修改调用者自己的数据，在实际应用中需加以区分。</p>
<ul>
<li>创建Tensor</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Tensor(*sizes)</td>
<td style="text-align:center">基础构造函数</td>
</tr>
<tr>
<td style="text-align:center">ones(*sizes)</td>
<td style="text-align:center">全1Tensor</td>
</tr>
<tr>
<td style="text-align:center">zeros(*sizes)</td>
<td style="text-align:center">全0Tensor</td>
</tr>
<tr>
<td style="text-align:center">eye(*sizes)</td>
<td style="text-align:center">对角线为1，其他为0</td>
</tr>
<tr>
<td style="text-align:center">arange(s,e,step</td>
<td style="text-align:center">从s到e，步长为step</td>
</tr>
<tr>
<td style="text-align:center">linspace(s,e,steps)</td>
<td style="text-align:center">从s到e，均匀切分成steps份</td>
</tr>
<tr>
<td style="text-align:center">rand/randn(*sizes)</td>
<td style="text-align:center">均匀/标准分布</td>
</tr>
<tr>
<td style="text-align:center">normal(mean,std)/uniform(from,to)</td>
<td style="text-align:center">正态分布/均匀分布</td>
</tr>
<tr>
<td style="text-align:center">randperm(m)</td>
<td style="text-align:center">随机排列</td>
</tr>
</tbody>
</table>
</div>
<p>其中使用<code>Tensor</code>函数新建tensor是最复杂多变的方式，它既可以接收一个list，并根据list的数据新建tensor，也能根据指定的形状新建tensor，还能传入其他的tensor，下面举几个例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定tensor的形状</span></span><br><span class="line">a = t.Tensor(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用list的数据创建tensor</span></span><br><span class="line">b = t.Tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把tensor转为list</span></span><br><span class="line">b.tolist() </span><br></pre></td></tr></table></figure>
<ul>
<li>tensor.size()返回torch.Size对象，它是tuple的子类，但其使用方式与tuple略有区别</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看张量的size</span></span><br><span class="line">b_size = b.size()</span><br><span class="line"></span><br><span class="line"><span class="comment"># b中元素总个数，2*3，等价于b.nelement()</span></span><br><span class="line">b.numel() </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个和b形状一样的tensor</span></span><br><span class="line">c = t.Tensor(b_size)</span><br><span class="line"><span class="comment"># 创建一个元素为2和3的tensor</span></span><br><span class="line">d = t.Tensor((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>除了<code>tensor.size()</code>，还可以利用<code>tensor.shape</code>直接查看tensor的形状，<code>tensor.shape</code>等价于<code>tensor.size()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看张量形状</span></span><br><span class="line">c.shape</span><br></pre></td></tr></table></figure>
<ul>
<li>需要注意的是，<code>t.Tensor(*sizes)</code>创建tensor时，系统不会马上分配空间，只是会计算剩余的内存是否足够使用，使用到tensor时才会分配，而其它操作都是在创建完tensor之后马上进行空间分配。其它常用的创建tensor的方法举例如下。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t.ones(<span class="number">2</span>, <span class="number">3</span>)<span class="comment">#生成全1张量</span></span><br><span class="line">t.zeros(<span class="number">2</span>, <span class="number">3</span>)<span class="comment">#生成全0张量</span></span><br><span class="line">t.arange(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>)<span class="comment"># 1-&gt;6 步长为2 tensor([1, 3, 5])</span></span><br><span class="line">t.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">3</span>) <span class="comment"># 1-&gt;10 均匀切分成steps份   tensor([ 1.0000,  5.5000, 10.0000])</span></span><br><span class="line">t.randn(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment">#  均匀/标准分布  tensor([[-0.2624, -0.9963, -0.4028],[ 1.4468,  0.0915, -0.4754]])</span></span><br><span class="line">t.randperm(<span class="number">5</span>) <span class="comment"># 长度为5的随机排列  tensor([1, 3, 4, 2, 0])</span></span><br><span class="line">t.eye(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># 对角线为1, 不要求行列数一致   tensor([[1., 0., 0.], [0., 1., 0.]])</span></span><br></pre></td></tr></table></figure>
<h2 id="常用Tensor操作"><a href="#常用Tensor操作" class="headerlink" title="常用Tensor操作"></a>常用Tensor操作</h2><ul>
<li>通过<code>tensor.view</code>方法可以调整tensor的形状，但必须保证调整前后元素总数一致。<code>view</code>不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。在实际应用中可能经常需要添加或减少某一维度，这时候<code>squeeze</code>和<code>unsqueeze</code>两个函数就派上用场了。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">a.view(<span class="number">2</span>, <span class="number">3</span>) <span class="comment"># tensor([[0, 1, 2],[3, 4, 5]])</span></span><br><span class="line"></span><br><span class="line">b = a.view(-<span class="number">1</span>, <span class="number">3</span>) <span class="comment"># 当某一维为-1的时候，会自动计算它的大小   tensor([[0, 1, 2],[3, 4, 5]])</span></span><br><span class="line"></span><br><span class="line">b.unsqueeze(<span class="number">1</span>) <span class="comment"># 注意形状，在第1维（下标从0开始）上增加“１”</span></span><br><span class="line">b.unsqueeze(-<span class="number">2</span>)<span class="comment"># -2表示倒数第二个维度</span></span><br><span class="line"></span><br><span class="line">c = b.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">c.squeeze(<span class="number">0</span>) <span class="comment"># 压缩第0维的“１”</span></span><br><span class="line"></span><br><span class="line">c.squeeze() <span class="comment"># 把所有维度为“1”的压缩</span></span><br><span class="line"></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">b <span class="comment"># a修改，b作为view之后的，也会跟着修改</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>resize</code>是另一种可用来调整<code>size</code>的方法，但与<code>view</code>不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，看一个例子。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b.resize_(<span class="number">1</span>, <span class="number">3</span>) <span class="comment">#tensor([[0, 1, 2]])</span></span><br><span class="line">b.resize_(<span class="number">3</span>, <span class="number">3</span>) <span class="comment"># 旧的数据依旧保存着，多出的大小会分配新空间</span></span><br></pre></td></tr></table></figure>
<h2 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h2></li>
<li>Tensor支持与numpy.ndarray类似的索引操作，语法上也类似，下面通过一些例子，讲解常用的索引操作。如无特殊说明，索引出来的结果与原tensor共享内存，也即修改一个，另一个会跟着修改。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>) <span class="comment"># </span></span><br><span class="line">a[<span class="number">0</span>] <span class="comment"># 第0行(下标从0开始)</span></span><br><span class="line">a[:, <span class="number">0</span>] <span class="comment"># 第0列</span></span><br><span class="line">a[<span class="number">0</span>][<span class="number">2</span>] <span class="comment"># 第0行第2个元素，等价于a[0, 2]</span></span><br><span class="line">a[<span class="number">0</span>, -<span class="number">1</span>] <span class="comment"># 第0行最后一个元素</span></span><br><span class="line">a[:<span class="number">2</span>] <span class="comment"># 前两行</span></span><br><span class="line">print(a[<span class="number">0</span>:<span class="number">1</span>, :<span class="number">2</span>]) <span class="comment"># 第0行，前两列 </span></span><br><span class="line">print(a[<span class="number">0</span>, :<span class="number">2</span>]) <span class="comment"># 注意两者的区别：形状不同</span></span><br><span class="line"></span><br><span class="line">a &gt; <span class="number">1</span> <span class="comment"># 返回一个ByteTensor</span></span><br><span class="line">a[a&gt;<span class="number">1</span>] <span class="comment"># 等价于a.masked_select(a&gt;1)  # 选择结果与原tensor不共享内存空间</span></span><br><span class="line"></span><br><span class="line">a[t.LongTensor([<span class="number">0</span>,<span class="number">1</span>])] </span><br></pre></td></tr></table></figure>
<ul>
<li>其它常用的选择函数</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">index_select(input, dim, index)</td>
<td style="text-align:center">在指定维度dim上选取，比如选取某些行、某些列</td>
</tr>
<tr>
<td style="text-align:center">masked_select(input, mask)</td>
<td style="text-align:center">例子如上，a[a&gt;0]，使用ByteTensor进行选取</td>
</tr>
<tr>
<td style="text-align:center">non_zero(input)</td>
<td style="text-align:center">非0元素的下标</td>
</tr>
<tr>
<td style="text-align:center">gather(input, dim, index)</td>
<td style="text-align:center">根据index，在dim维度上选取数据，输出的size与index一样</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><code>gather</code>是一个比较复杂的操作，对一个2维tensor，输出的每个元素如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out[i][j] = <span class="built_in">input</span>[index[i][j]][j]  <span class="comment"># dim=0</span></span><br><span class="line">out[i][j] = <span class="built_in">input</span>[i][index[i][j]]  <span class="comment"># dim=1</span></span><br></pre></td></tr></table></figure></li>
<li>三维tensor的<code>gather</code>操作同理</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">16</span>).view(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取对角线的元素</span></span><br><span class="line">index = t.LongTensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line"><span class="comment">#在第0个维度上取 第0行第0个元素   第1行第1个元素  第2行第2个元素   第3行第3个元素</span></span><br><span class="line">a.gather(<span class="number">0</span>, index)</span><br><span class="line">output：</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">5</span>, <span class="number">10</span>, <span class="number">15</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取反对角线上的元素   第0列第3个元素   第1列第2个元素  第2列第1个元素   第3列第0个元素</span></span><br><span class="line">index = t.LongTensor([[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]]).t()</span><br><span class="line"><span class="comment">#在第一个维度上取 </span></span><br><span class="line">a.gather(<span class="number">1</span>, index)</span><br><span class="line">output：</span><br><span class="line">tensor([[ <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">6</span>],</span><br><span class="line">        [ <span class="number">9</span>],</span><br><span class="line">        [<span class="number">12</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取反对角线上的元素，注意与上面的不同  </span></span><br><span class="line">index = t.LongTensor([[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line">a.gather(<span class="number">0</span>, index)  <span class="comment">#</span></span><br><span class="line">output：</span><br><span class="line">tensor([[<span class="number">12</span>,  <span class="number">9</span>,  <span class="number">6</span>,  <span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取两个对角线上的元素</span></span><br><span class="line">index = t.LongTensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]]).t()</span><br><span class="line">b = a.gather(<span class="number">1</span>, index)</span><br><span class="line">output：</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [<span class="number">10</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">15</span>, <span class="number">12</span>]])</span><br></pre></td></tr></table></figure>
<ul>
<li>与<code>gather</code>相对应的逆操作是<code>scatter_</code>，<code>gather</code>把数据从input中按index取出，而<code>scatter_</code>是把取出的数据再放回去。注意<code>scatter_</code>函数是inplace操作。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">out = <span class="built_in">input</span>.gather(dim, index)</span><br><span class="line">--&gt;近似逆操作</span><br><span class="line">out = Tensor()</span><br><span class="line">out.scatter_(dim, index)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把两个对角线元素放回去到指定位置</span></span><br><span class="line">c = t.zeros(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">c.scatter_(<span class="number">1</span>, index, b)</span><br></pre></td></tr></table></figure>
<h2 id="高级索引"><a href="#高级索引" class="headerlink" title="高级索引"></a>高级索引</h2><p>Pytorch目前已经支持绝大多数numpy的高级索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">x = t.arange(<span class="number">0</span>,<span class="number">27</span>).view(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">output：</span><br><span class="line"></span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">         [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>],</span><br><span class="line">         [<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>],</span><br><span class="line">         [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>]]])</span><br><span class="line">         </span><br><span class="line">x[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">0</span>]] <span class="comment"># x[1,1,2]和x[2,2,0]  将第一个维度拼接  第二个维度拼接 则编程1 1 2  和2 2 0 </span></span><br><span class="line">output：</span><br><span class="line">tensor([<span class="number">14</span>, <span class="number">24</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x[[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]] <span class="comment"># x[2,0,1],x[1,0,1],x[0,0,1]</span></span><br><span class="line">output:</span><br><span class="line">tensor([<span class="number">19</span>, <span class="number">10</span>,  <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">x[[<span class="number">0</span>, <span class="number">2</span>], ...] <span class="comment"># x[0] 和 x[2]</span></span><br><span class="line">output:</span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>],</span><br><span class="line">         [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>]]])</span><br><span class="line"></span><br><span class="line">rows = np.array([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">3</span>,<span class="number">3</span>]]) </span><br><span class="line">cols = np.array([[<span class="number">0</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">2</span>]])</span><br><span class="line">y = x[rows,cols]  <span class="comment">#取出 [[(0,0),(0,2)]，[3,0, 3,2]]</span></span><br><span class="line"><span class="built_in">print</span> (y)</span><br></pre></td></tr></table></figure>
<h2 id="Tensor类型"><a href="#Tensor类型" class="headerlink" title="Tensor类型"></a>Tensor类型</h2><p>Tensor有不同的数据类型，如表示，每种类型分别对应有CPU和GPU版本(HalfTensor除外)。默认的tensor是FloatTensor，可通过<code>t.set_default_tensor_type</code> 来修改默认tensor类型(如果默认类型为GPU tensor，则所有操作都将在GPU上进行)。Tensor的类型对分析内存占用很有帮助。例如对于一个size为(1000, 1000, 1000)的FloatTensor，它有<code>1000*1000*1000=10^9</code>个元素，每个元素占32bit/8 = 4Byte内存，所以共占大约4GB内存/显存。HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大缓解GPU显存不足的问题，但由于HalfTensor所能表示的数值大小和精度有限<sup><a href="#fn_2" id="reffn_2">2</a></sup>，所以可能出现溢出等问题。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">数据类型</th>
<th style="text-align:center">CPU tensor</th>
<th style="text-align:center">GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">32-bit 浮点</td>
<td style="text-align:center">torch.FloatTensor</td>
<td style="text-align:center">torch.cuda.FloatTensor</td>
</tr>
<tr>
<td style="text-align:center">64-bit 浮点</td>
<td style="text-align:center">torch.DoubleTensor</td>
<td style="text-align:center">torch.cuda.DoubleTensor</td>
</tr>
<tr>
<td style="text-align:center">16-bit 半精度浮点</td>
<td style="text-align:center">N/A</td>
<td style="text-align:center">torch.cuda.HalfTensor</td>
</tr>
<tr>
<td style="text-align:center">8-bit 无符号整形(0~255)</td>
<td style="text-align:center">torch.ByteTensor</td>
<td style="text-align:center">torch.cuda.ByteTensor</td>
</tr>
<tr>
<td style="text-align:center">8-bit 有符号整形(-128~127)</td>
<td style="text-align:center">torch.CharTensor</td>
<td style="text-align:center">torch.cuda.CharTensor</td>
</tr>
<tr>
<td style="text-align:center">16-bit 有符号整形</td>
<td style="text-align:center">torch.ShortTensor</td>
<td style="text-align:center">torch.cuda.ShortTensor</td>
</tr>
<tr>
<td style="text-align:center">32-bit 有符号整形</td>
<td style="text-align:center">torch.IntTensor</td>
<td style="text-align:center">torch.cuda.IntTensor</td>
</tr>
<tr>
<td style="text-align:center">64-bit 有符号整形</td>
<td style="text-align:center">torch.LongTensor</td>
<td style="text-align:center">torch.cuda.LongTensor</td>
</tr>
</tbody>
</table>
</div>
<p>各数据类型之间可以互相转换，<code>type(new_type)</code>是通用的做法，同时还有<code>float</code>、<code>long</code>、<code>half</code>等快捷方法。CPU tensor与GPU tensor之间的互相转换通过<code>tensor.cuda</code>和<code>tensor.cpu</code>方法实现。Tensor还有一个<code>new</code>方法，用法与<code>t.Tensor</code>一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置默认tensor，注意参数是字符串</span></span><br><span class="line">t.set_default_tensor_type(<span class="string">&#x27;torch.IntTensor&#x27;</span>)</span><br><span class="line">a = t.Tensor(<span class="number">2</span>,<span class="number">3</span>) a <span class="comment"># 现在a是IntTensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把a转成FloatTensor，等价于b=a.type(t.FloatTensor)</span></span><br><span class="line">b = a.<span class="built_in">float</span>() </span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复之前的默认设置</span></span><br><span class="line">t.set_default_tensor_type(<span class="string">&#x27;torch.FloatTensor&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="逐元素操作"><a href="#逐元素操作" class="headerlink" title="逐元素操作"></a>逐元素操作</h2><p>这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">abs/sqrt/div/exp/fmod/log/pow..</td>
<td style="text-align:center">绝对值/平方根/除法/指数/求余/求幂..</td>
</tr>
<tr>
<td style="text-align:center">cos/sin/asin/atan2/cosh..</td>
<td style="text-align:center">相关三角函数</td>
</tr>
<tr>
<td style="text-align:center">ceil/round/floor/trunc</td>
<td style="text-align:center">上取整/四舍五入/下取整/只保留整数部分</td>
</tr>
<tr>
<td style="text-align:center">clamp(input, min, max)</td>
<td style="text-align:center">超过min和max部分截断</td>
</tr>
<tr>
<td style="text-align:center">sigmod/tanh..</td>
<td style="text-align:center">激活函数</td>
</tr>
</tbody>
</table>
</div>
<p>对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如<code>a ** 2</code> 等价于<code>torch.pow(a,2)</code>, <code>a * 2</code>等价于<code>torch.mul(a,2)</code>。</p>
<p>其中<code>clamp(x, min, max)</code>的输出满足以下公式：</p>
<script type="math/tex; mode=display">
y_i =
\begin{cases}
min,  & \text{if  } x_i \lt min \\
x_i,  & \text{if  } min \le x_i \le max  \\
max,  & \text{if  } x_i \gt max\\
\end{cases}</script><p><code>clamp</code>常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">t.cos(a)</span><br><span class="line">a % <span class="number">3</span> <span class="comment"># 等价于t.fmod(a, 3)</span></span><br><span class="line">a ** <span class="number">2</span> <span class="comment"># 等价于t.pow(a, 2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取a中的每一个元素与3相比较大的一个 (小于3的截断成3)</span></span><br><span class="line">print(a)</span><br><span class="line">t.clamp(a, <span class="built_in">min</span>=<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="归并操作"><a href="#归并操作" class="headerlink" title="归并操作"></a>归并操作</h2><p>此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法<code>sum</code>，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">mean/sum/median/mode</td>
<td style="text-align:center">均值/和/中位数/众数</td>
</tr>
<tr>
<td style="text-align:center">norm/dist</td>
<td style="text-align:center">范数/距离</td>
</tr>
<tr>
<td style="text-align:center">std/var</td>
<td style="text-align:center">标准差/方差</td>
</tr>
<tr>
<td style="text-align:center">cumsum/cumprod</td>
<td style="text-align:center">累加/累乘</td>
<td>以上大多数函数都有一个参数<strong><code>dim</code></strong>，用来指定这些操作是在哪个维度上执行的。关于dim(对应于Numpy中的axis)的解释众说纷纭，这里提供一个简单的记忆方式：</td>
</tr>
</tbody>
</table>
</div>
<p>假设输入的形状是(m, n, k)</p>
<ul>
<li>如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)</li>
<li>如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)</li>
<li>如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)</li>
</ul>
<p>size中是否有”1”，取决于参数<code>keepdim</code>，<code>keepdim=True</code>会保留维度<code>1</code>。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如<code>cumsum</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">b = t.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b.<span class="built_in">sum</span>(dim = <span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># keepdim=False，不保留维度&quot;1&quot;，注意形状</span></span><br><span class="line">b.<span class="built_in">sum</span>(dim=<span class="number">0</span>, keepdim=<span class="literal">False</span>)</span><br><span class="line">output:</span><br><span class="line">tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>])</span><br><span class="line"></span><br><span class="line">b.<span class="built_in">sum</span>(dim=<span class="number">1</span>)</span><br><span class="line">output:</span><br><span class="line">tensor([<span class="number">3.</span>, <span class="number">3.</span>])</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">a.cumsum(dim=<span class="number">1</span>) <span class="comment"># 沿着行累加</span></span><br><span class="line">output：</span><br><span class="line">tensor([<span class="number">3.</span>, <span class="number">3.</span>])</span><br></pre></td></tr></table></figure>
<h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><p>比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">gt/lt/ge/le/eq/ne</td>
<td style="text-align:center">大于/小于/大于等于/小于等于/等于/不等</td>
</tr>
<tr>
<td style="text-align:center">topk</td>
<td style="text-align:center">最大的k个数</td>
</tr>
<tr>
<td style="text-align:center">sort</td>
<td style="text-align:center">排序</td>
</tr>
<tr>
<td style="text-align:center">max/min</td>
<td style="text-align:center">比较两个tensor最大最小值</td>
</tr>
</tbody>
</table>
</div>
<p>表中第一行的比较操作已经实现了运算符重载，因此可以使用<code>a&gt;=b</code>、<code>a&gt;b</code>、<code>a!=b</code>、<code>a==b</code>，其返回结果是一个<code>ByteTensor</code>，可用来选取元素。max/min这两个操作比较特殊，以max来说，它有以下三种使用情况：</p>
<ul>
<li>t.max(tensor)：返回tensor中最大的一个数</li>
<li>t.max(tensor,dim)：指定维上最大的数，返回tensor和下标</li>
<li>t.max(tensor1, tensor2): 比较两个tensor相比较大的元素</li>
</ul>
<p>至于比较一个tensor和一个数，可以使用clamp函数。下面举例说明。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = t.linspace(<span class="number">0</span>, <span class="number">15</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">output：</span><br><span class="line">  <span class="number">0</span>   <span class="number">3</span>   <span class="number">6</span></span><br><span class="line">  <span class="number">9</span>  <span class="number">12</span>  <span class="number">15</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br><span class="line"></span><br><span class="line">b = t.linspace(<span class="number">15</span>, <span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">output：</span><br><span class="line">tensor([[<span class="number">15.</span>, <span class="number">12.</span>,  <span class="number">9.</span>],</span><br><span class="line">        [ <span class="number">6.</span>,  <span class="number">3.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a&gt;b</span><br><span class="line">output：</span><br><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a[a&gt;b] <span class="comment"># a中大于b的元素</span></span><br><span class="line">output：</span><br><span class="line">tensor([ <span class="number">9.</span>, <span class="number">12.</span>, <span class="number">15.</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.<span class="built_in">max</span>(a)</span><br><span class="line">output：</span><br><span class="line">tensor(<span class="number">15.</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t.<span class="built_in">max</span>(b, dim=<span class="number">1</span>) </span><br><span class="line"><span class="comment"># 第一个返回值的15和6分别表示第0行和第1行最大的元素</span></span><br><span class="line"><span class="comment"># 第二个返回值的0和0表示上述最大的数是该行第0个元素</span></span><br><span class="line">output：</span><br><span class="line">torch.return_types.<span class="built_in">max</span>(</span><br><span class="line">values=tensor([<span class="number">15.</span>,  <span class="number">6.</span>]),</span><br><span class="line">indices=tensor([<span class="number">0</span>, <span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 比较a和10较大的元素</span></span><br><span class="line">t.clamp(a, <span class="built_in">min</span>=<span class="number">10</span>)</span><br><span class="line">output：</span><br><span class="line">tensor([[<span class="number">10.</span>, <span class="number">10.</span>, <span class="number">10.</span>],</span><br><span class="line">        [<span class="number">10.</span>, <span class="number">12.</span>, <span class="number">15.</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><p>PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如表3-7所示。</p>
<p>表3-7: 常用的线性代数函数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">trace</td>
<td style="text-align:center">对角线元素之和(矩阵的迹)</td>
</tr>
<tr>
<td style="text-align:center">diag</td>
<td style="text-align:center">对角线元素</td>
</tr>
<tr>
<td style="text-align:center">triu/tril</td>
<td style="text-align:center">矩阵的上三角/下三角，可指定偏移量</td>
</tr>
<tr>
<td style="text-align:center">mm/bmm</td>
<td style="text-align:center">矩阵乘法，batch的矩阵乘法</td>
</tr>
<tr>
<td style="text-align:center">addmm/addbmm/addmv/addr/badbmm..</td>
<td style="text-align:center">矩阵运算</td>
</tr>
<tr>
<td style="text-align:center">t</td>
<td style="text-align:center">转置</td>
</tr>
<tr>
<td style="text-align:center">dot/cross</td>
<td style="text-align:center">内积/外积</td>
</tr>
<tr>
<td style="text-align:center">inverse</td>
<td style="text-align:center">求逆矩阵</td>
</tr>
<tr>
<td style="text-align:center">svd</td>
<td style="text-align:center">奇异值分解</td>
</tr>
</tbody>
</table>
</div>
<p>具体使用说明请参见官方文档<sup><a href="#fn_3" id="reffn_3">3</a></sup>，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的<code>.contiguous</code>方法将其转为连续。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">b = a.t()</span><br><span class="line">b.is_contiguous()</span><br><span class="line">output：</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"></span><br><span class="line">b.contiguous()</span><br><span class="line">output：</span><br><span class="line">  <span class="number">0</span>   <span class="number">9</span></span><br><span class="line">  <span class="number">3</span>  <span class="number">12</span></span><br><span class="line">  <span class="number">6</span>  <span class="number">15</span></span><br><span class="line">[torch.FloatTensor of size 3x2]</span><br></pre></td></tr></table></figure>
<h1 id="Tensor和Numpy"><a href="#Tensor和Numpy" class="headerlink" title="Tensor和Numpy"></a>Tensor和Numpy</h1><p>Tensor和Numpy数组之间具有很高的相似性，彼此之间的互操作也非常简单高效。需要注意的是，Numpy和Tensor共享内存。由于Numpy历史悠久，支持丰富的操作，所以当遇到Tensor不支持的操作时，可先转成Numpy数组，处理后再转回tensor，其转换开销很小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones([<span class="number">2</span>, <span class="number">3</span>],dtype=np.float32)</span><br><span class="line">b = t.from_numpy(a)</span><br><span class="line">b = t.Tensor(a)  <span class="comment"># 也可以直接将numpy对象传入Tensor</span></span><br><span class="line"><span class="comment">#两者共享内存</span></span><br><span class="line">a[<span class="number">0</span>, <span class="number">1</span>]=<span class="number">100</span></span><br><span class="line"></span><br><span class="line">c = b.numpy() </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>注意</strong>： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。</li>
</ul>
<p>广播法则(broadcast)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。<br>Numpy的广播法则定义如下：</p>
<ul>
<li>让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐</li>
<li>两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算 </li>
<li>当输入数组的某个维度的长度为1时，计算时沿此维度复制扩充成一样的形状</li>
</ul>
<p>PyTorch当前已经支持了自动广播法则，但是笔者还是建议读者通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错：</p>
<ul>
<li><code>unsqueeze</code>或者<code>view</code>：为数据某一维的形状补1，实现法则1</li>
<li><code>expand</code>或者<code>expand_as</code>，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。</li>
</ul>
<p>注意，repeat实现与expand相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">b = t.zeros(<span class="number">2</span>, <span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 自动广播法则</span></span><br><span class="line"><span class="comment"># 第一步：a是2维,b是3维，所以先在较小的a前面补1 ，</span></span><br><span class="line"><span class="comment">#               即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,</span></span><br><span class="line"><span class="comment"># 第二步:   a和b在第一维和第三维形状不一样，其中一个为1 ，</span></span><br><span class="line"><span class="comment">#               可以利用广播法则扩展，两个形状都变成了（2，3，2）</span></span><br><span class="line">a+b</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 手动广播法则</span></span><br><span class="line"><span class="comment"># 或者 a.view(1,3,2).expand(2,3,2)+b.expand(2,3,2)</span></span><br><span class="line">a.unsqueeze(<span class="number">0</span>).expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>) + b.expand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存</span></span><br><span class="line">e = a.unsqueeze(<span class="number">0</span>).expand(<span class="number">10000000000000</span>, <span class="number">3</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h1 id="内部结构"><a href="#内部结构" class="headerlink" title="内部结构"></a>内部结构</h1><p>tensor的数据结构如图所示。tensor分为头信息区(Tensor)和存储区(Storage)，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。</p>
<p>一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。下面看两个例子。<br><img src="https://img-blog.csdnimg.cn/20201209150955171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>绝大多数操作并不修改tensor的数据，而只是修改了tensor的头信息。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。<br>此外有些操作会导致tensor不连续，这时需调用<code>tensor.contiguous</code>方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享storage。</li>
</ul>
<h1 id="其它有关Tensor的话题"><a href="#其它有关Tensor的话题" class="headerlink" title="其它有关Tensor的话题"></a>其它有关Tensor的话题</h1><h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><ul>
<li>Tensor的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的<code>pickle</code>模块，在load时还可将GPU的tensor映射到CPU或其它GPU上。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> t.cuda.is_available():</span><br><span class="line">    a = a.cuda(<span class="number">1</span>) <span class="comment"># 把a转为GPU1上的tensor,</span></span><br><span class="line">    t.save(a,<span class="string">&#x27;a.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载为b, 存储于GPU1上(因为保存时tensor就在GPU1上)</span></span><br><span class="line">    b = t.load(<span class="string">&#x27;a.pth&#x27;</span>)</span><br><span class="line">    <span class="comment"># 加载为c, 存储于CPU</span></span><br><span class="line">    c = t.load(<span class="string">&#x27;a.pth&#x27;</span>, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">    <span class="comment"># 加载为d, 存储于GPU0上</span></span><br><span class="line">    d = t.load(<span class="string">&#x27;a.pth&#x27;</span>, map_location=&#123;<span class="string">&#x27;cuda:1&#x27;</span>:<span class="string">&#x27;cuda:0&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p>向量化计算是一种特殊的并行计算方式，相对于一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组/向量上。向量化可极大提高科学运算的效率，Python本身是一门高级语言，使用很方便，但这也意味着很多操作很低效，尤其是<code>for</code>循环。在科学计算程序中应当极力避免使用Python原生的<code>for循环</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">for_loop_add</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i,j <span class="keyword">in</span> <span class="built_in">zip</span>(x, y):</span><br><span class="line">        result.append(i + j)</span><br><span class="line">    <span class="keyword">return</span> t.Tensor(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">x = t.zeros(<span class="number">100</span>)</span><br><span class="line">y = t.ones(<span class="number">100</span>)</span><br><span class="line">%timeit -n <span class="number">10</span> for_loop_add(x, y)</span><br><span class="line">%timeit -n <span class="number">10</span> x + y</span><br></pre></td></tr></table></figure>
<p>可见二者有超过40倍的速度差距，因此在实际使用中应尽量调用内建函数(buildin-function)，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。因此在平时写代码时，就应养成向量化的思维习惯。</p>
<p>此外还有以下几点需要注意：</p>
<ul>
<li>大多数<code>t.function</code>都有一个参数<code>out</code>，这时候产生的结果将保存在out指定tensor之中。</li>
<li><code>t.set_num_threads</code>可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来限制PyTorch所占用的CPU数目。</li>
<li><code>t.set_printoptions</code>可以用来设置打印tensor时的数值精度和格式。<br>下面举例说明。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">20000000</span>)</span><br><span class="line">print(a[-<span class="number">1</span>], a[-<span class="number">2</span>]) <span class="comment"># 32bit的IntTensor精度有限导致溢出</span></span><br><span class="line">b = t.LongTensor()</span><br><span class="line">t.arange(<span class="number">0</span>, <span class="number">200000</span>, out=b) <span class="comment"># 64bit的LongTensor不会溢出</span></span><br><span class="line">a = t.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t.set_printoptions(precision=<span class="number">10</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。首先让我们来确认线性回归的损失函数：</p>
<script type="math/tex; mode=display">
loss = \sum_i^N \frac 1 2 ({y_i-(wx_i+b)})^2</script><p>然后利用随机梯度下降法更新参数$\textbf{w}$和$\textbf{b}$来最小化损失函数，最终学得$\textbf{w}$和$\textbf{b}$的数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子，保证在不同电脑上运行时下面的输出一致</span></span><br><span class="line">t.manual_seed(<span class="number">1000</span>) </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fake_data</span>(<span class="params">batch_size=<span class="number">8</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; 产生随机数据：y=x*2+3，加上了一些噪声&#x27;&#x27;&#x27;</span></span><br><span class="line">    x = t.rand(batch_size, <span class="number">1</span>) * <span class="number">20</span></span><br><span class="line">    y = x * <span class="number">2</span> + (<span class="number">1</span> + t.randn(batch_size, <span class="number">1</span>))*<span class="number">3</span></span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 来看看产生的x-y分布</span></span><br><span class="line">x, y = get_fake_data()</span><br><span class="line">plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机初始化参数</span></span><br><span class="line">w = t.rand(<span class="number">1</span>, <span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">b = t.zeros(<span class="number">1</span>, <span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">lr =<span class="number">0.001</span> <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">    x, y = get_fake_data()</span><br><span class="line">    x = x.<span class="built_in">float</span>()</span><br><span class="line">    y = y.<span class="built_in">float</span>()</span><br><span class="line">    <span class="comment"># forward：计算loss</span></span><br><span class="line">    y_pred = x.mm(w).<span class="built_in">float</span>() + b.expand_as(y).<span class="built_in">float</span>() <span class="comment"># x@W等价于x.mm(w);for python3 only</span></span><br><span class="line">    loss = <span class="number">0.5</span> * (y_pred - y) ** <span class="number">2</span> <span class="comment"># 均方误差</span></span><br><span class="line">    loss = loss.<span class="built_in">sum</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># backward：手动计算梯度</span></span><br><span class="line">    dloss = <span class="number">1</span></span><br><span class="line">    dy_pred = dloss * (y_pred - y)</span><br><span class="line">    </span><br><span class="line">    dw = x.t().mm(dy_pred)</span><br><span class="line">    db = dy_pred.<span class="built_in">sum</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    w.sub_(lr * dw)</span><br><span class="line">    b.sub_(lr * db)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> ii%<span class="number">1000</span> ==<span class="number">0</span>:</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 画图</span></span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line">        x = t.arange(<span class="number">0</span>, <span class="number">20</span>).view(-<span class="number">1</span>, <span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">        y = x.mm(w) + b.expand_as(x)</span><br><span class="line">        plt.plot(x.numpy(), y.numpy()) <span class="comment"># predicted</span></span><br><span class="line">        </span><br><span class="line">        x2, y2 = get_fake_data(batch_size=<span class="number">20</span>) </span><br><span class="line">        plt.scatter(x2.numpy(), y2.numpy()) <span class="comment"># true data</span></span><br><span class="line">        </span><br><span class="line">        plt.xlim(<span class="number">0</span>, <span class="number">20</span>)</span><br><span class="line">        plt.ylim(<span class="number">0</span>, <span class="number">41</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(<span class="number">0.5</span>)</span><br><span class="line">        </span><br><span class="line">print(w.squeeze()[<span class="number">0</span>], b.squeeze()[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/12/09/Pytorch/Pytorch-and-Tensor/" data-id="ckutylm9d00440eru48bd0q2o" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="tag">深度学习框架</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Pytorch中的Autograd
        
      </div>
    </a>
  
  
    <a href="/2020/12/09/Pytorch/Pytorch-Introductory-knowledge/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">PyTorch入门知识</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/PTA%E7%94%B2%E7%BA%A7%E5%88%B7%E9%A2%98/">PTA甲级刷题</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode%E5%88%B7%E9%A2%98/">leetcode刷题</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/">爬虫学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/">论文复现</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/">课程设计</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/" rel="tag">c++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/django/" rel="tag">django</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyquery/" rel="tag">pyquery</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/springboot/" rel="tag">springboot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vue/" rel="tag">vue</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" rel="tag">动态规划</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE/" rel="tag">图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/" rel="tag">多文档摘要</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" rel="tag">字符串</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" rel="tag">操作系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%91/" rel="tag">树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E6%8B%9F/" rel="tag">模拟</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82/" rel="tag">模拟网络请求</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="tag">深度学习框架</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90/" rel="tag">网页解析</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/c/" style="font-size: 17.78px;">c++</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/java/" style="font-size: 14.44px;">java</a> <a href="/tags/pyquery/" style="font-size: 10px;">pyquery</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/pytorch/" style="font-size: 15.56px;">pytorch</a> <a href="/tags/springboot/" style="font-size: 10px;">springboot</a> <a href="/tags/tensorflow/" style="font-size: 16.67px;">tensorflow</a> <a href="/tags/vue/" style="font-size: 11.11px;">vue</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 10px;">动态规划</a> <a href="/tags/%E5%9B%BE/" style="font-size: 10px;">图</a> <a href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/" style="font-size: 10px;">多文档摘要</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 10px;">字符串</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">强化学习</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 13.33px;">操作系统</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 11.11px;">数据库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%A0%91/" style="font-size: 12.22px;">树</a> <a href="/tags/%E6%A8%A1%E6%8B%9F/" style="font-size: 10px;">模拟</a> <a href="/tags/%E6%A8%A1%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82/" style="font-size: 11.11px;">模拟网络请求</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 14.44px;">深度学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" style="font-size: 18.89px;">深度学习框架</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 15.56px;">爬虫</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 17.78px;">算法</a> <a href="/tags/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90/" style="font-size: 10px;">网页解析</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/">Pytorch强化学习算法实现</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/">PyTorch常用工具模块</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/">Pytorch中神经网络工具箱nn模块</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/Pytorch-and-Autograd/">Pytorch中的Autograd</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/Pytorch-and-Tensor/">Pytorch中的Tensor</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 ccclll777<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>