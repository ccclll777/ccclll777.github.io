<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Tensorflow2.0的进阶知识">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow2.0进阶知识">
<meta property="og:url" content="http://yoursite.com/2020/12/03/Tensorflow-advanced-knowledge/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="Tensorflow2.0的进阶知识">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203200253360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203213935313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203220631843.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203221154114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203221449852.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203221838187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2020-12-03T08:44:59.000Z">
<meta property="article:modified_time" content="2020-12-04T15:02:31.000Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="python">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20201203200253360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">

<link rel="canonical" href="http://yoursite.com/2020/12/03/Tensorflow-advanced-knowledge/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Tensorflow2.0进阶知识 | ccclll777's blogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
<a href="https://github.com/ccclll777" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ccclll777's blogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/03/Tensorflow-advanced-knowledge/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ccclll777">
      <meta itemprop="description" content="胸怀猛虎 细嗅蔷薇">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ccclll777's blogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tensorflow2.0进阶知识
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-12-03 16:44:59" itemprop="dateCreated datePublished" datetime="2020-12-03T16:44:59+08:00">2020-12-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-12-04 23:02:31" itemprop="dateModified" datetime="2020-12-04T23:02:31+08:00">2020-12-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>13k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Tensorflow2.0的进阶知识</p>
<a id="more"></a>

<h1 id="合并与分割"><a href="#合并与分割" class="headerlink" title="合并与分割"></a>合并与分割</h1><h2 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h2><p>张量的合并可以使用拼接(Concatenate)和堆叠(Stack)操作实现，拼接操作并不会产生新 的维度，仅在现有的维度上合并，而堆叠会创建新维度。</p>
<ul>
<li><strong>拼接</strong>，通过 tf.concat(tensors, axis)函数拼接张量，其中参数 tensors 保存了所有需要合并的张量 List，axis 参数指定需要合并的维度索引</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">4</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">4</span>]) </span><br><span class="line">tf.concat([a,b],axis=<span class="number">2</span>) <span class="comment"># 在第三个维度上拼接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: id=<span class="number">28</span>, shape=(<span class="number">10</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy= array([[[<span class="number">-5.13509691e-01</span>, <span class="number">-1.79707789e+00</span>, <span class="number">6.50747120e-01</span>, ...,<span class="number">2.58447856e-01</span>, <span class="number">8.47878829e-02</span>, <span class="number">4.13468748e-01</span>], [<span class="number">-1.17108583e+00</span>, <span class="number">1.93961406e+00</span>, <span class="number">1.27830813e-02</span>, ...,</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">6</span>,<span class="number">35</span>,<span class="number">8</span>]) tf.concat([a,b],axis=<span class="number">0</span>) <span class="comment"># 非法拼接，其他维度长度不相同</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>堆叠</strong>,拼接操作直接在现有维度上合并数据，并不会创建新的维度。如果在合并数据 时，希望创建一个新的维度，则需要使用 tf.stack 操作。</li>
<li>使用 tf.stack(tensors, axis)可以堆叠方式合并多个张量，通过 tensors 列表表示，参数 axis 指定新维度插入的位置，axis 的用法与 tf.expand_dims 的一致，当axis ≥ 0时，在 axis 之前插入;当axis &lt; 0时，在 axis 之后插入新维度。例如 shape 为[𝑏, 𝑐, h, 𝑤]的张量，在不 同位置通过 stack 操作插入新维度<br><img src="https://img-blog.csdnimg.cn/20201203200253360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">tf.stack([a,b],axis=<span class="number">0</span>) <span class="comment"># 堆叠合并为2个班级，班级维度插入在最前</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">55</span>, shape=(<span class="number">2</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">3.68728966e-01</span>, <span class="number">-8.54765773e-01</span>, <span class="number">-4.77824420e-01</span>,<span class="number">-3.83714020e-01</span>, <span class="number">-1.73216307e+00</span>, <span class="number">2.03872994e-02</span>, <span class="number">2.63810277e+00</span>, <span class="number">-1.12998331e+00</span>],...</span><br></pre></td></tr></table></figure>
<p>若选择使用 tf.concat 拼接合并，则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">tf.concat([a,b],axis=<span class="number">0</span>) <span class="comment"># 拼接方式合并，没有2个班级的概念</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">108</span>, shape=(<span class="number">70</span>, <span class="number">8</span>), dtype=float32, numpy=array([[<span class="number">-0.5516891</span> , <span class="number">-1.5031327</span> , <span class="number">-0.35369992</span>, <span class="number">0.31304857</span>, <span class="number">0.13965549</span>, <span class="number">0.6696881</span> , <span class="number">-0.50115544</span>, <span class="number">0.15550546</span>],[ <span class="number">0.8622069</span> , <span class="number">1.0188094</span> , <span class="number">0.18977325</span>, <span class="number">0.6353301</span> , <span class="number">0.05809061</span>,...</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在这里插入代码片</span><br></pre></td></tr></table></figure>

<h2 id="分割"><a href="#分割" class="headerlink" title="分割"></a>分割</h2><ul>
<li>合并操作的逆过程就是分割，将一个张量分拆为多个张量。</li>
<li>通过 tf.split(x, num_or_size_splits, axis)可以完成张量的分割操作，参数意义如下:</li>
<li>x参数:待分割张量</li>
<li>num_or_size_splits参数:切割方案。当num_or_size_splits为单个数值时，如10，表 示等长切割为 10 份;当 num_or_size_splits 为 List 时，List 的每个元素表示每份的长 度，如[2,4,2,2]表示切割为 4 份，每份的长度依次是 2、4、2、2。</li>
<li>axis参数:指定分割的维度索引号</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="comment"># 自定义长度的切割，切割为4份，返回4个张量的列表result</span></span><br><span class="line">result = tf.split(x, num_or_size_splits=[<span class="number">4</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>] ,axis=<span class="number">0</span>)</span><br><span class="line">len(result)</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure>

<ul>
<li>希望在某个维度上全部按长度为 1 的方式分割，还可以使用 tf.unstack(x, axis)函数。这种方式是 tf.split 的一种特殊情况，切割长度固定为 1，只需要指定切割维度 的索引号即可。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">result = tf.unstack(x,axis=<span class="number">0</span>) <span class="comment"># Unstack为长度为1的张量 </span></span><br><span class="line">len(result) <span class="comment"># 返回10个张量的列表</span></span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure>

<h1 id="数据统计"><a href="#数据统计" class="headerlink" title="数据统计"></a>数据统计</h1><p>在神经网络的计算过程中，经常需要统计数据的各种属性，如最值、最值位置、均值、范数等信息。由于张量通常较大，直接观察数据很难获得有用信息，通过获取这些张量的统计信息可以较轻松地推测张量数值的分布。</p>
<h2 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h2><ul>
<li>L1范数，定义为向量𝒙的所有元素绝对值之和<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.ones([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">tf.norm(x,ord=<span class="number">1</span>) <span class="comment"># 计算L1范数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">183</span>, shape=(), dtype=float32, numpy=<span class="number">4.0</span>&gt;</span><br></pre></td></tr></table></figure></li>
<li>L2范数，定义为向量𝒙的所有元素的平方和，再开根号</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.norm(x,ord=<span class="number">2</span>) <span class="comment"># 计算L2范数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">189</span>, shape=(), dtype=float32, numpy=<span class="number">2.0</span>&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>∞−范数，定义为向量𝒙的所有元素绝对值的最大值:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">tf.norm(x,ord=np.inf) <span class="comment"># 计算∞范数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">194</span>, shape=(), dtype=float32, numpy=<span class="number">1.0</span>&gt;</span><br></pre></td></tr></table></figure>


<h2 id="最值、均值、和"><a href="#最值、均值、和" class="headerlink" title="最值、均值、和"></a>最值、均值、和</h2><ul>
<li>通过 tf.reduce_max、tf.reduce_min、tf.reduce_mean、tf.reduce_sum<br>函数可以求解张量在某个维度上的最大、最小、均值、和，也可以求全局最大、最小、均值、和信息。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_min(x,axis=<span class="number">1</span>) <span class="comment"># 统计概率维度上的最小值</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">206</span>, shape=(<span class="number">4</span>,), dtype=float32, numpy=array([-</span><br><span class="line"><span class="number">0.27862206</span>, <span class="number">-2.4480672</span> , <span class="number">-1.9983795</span> , <span class="number">-1.5287997</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 统计全局的最大、最小、均值、和，返回的张量均为标量 </span></span><br><span class="line">tf.reduce_max(x),tf.reduce_min(x),tf.reduce_mean(x)</span><br><span class="line">(&lt;tf.Tensor: id=<span class="number">218</span>, shape=(), dtype=float32, numpy=<span class="number">1.8653786</span>&gt;,</span><br><span class="line"> &lt;tf.Tensor: id=<span class="number">220</span>, shape=(), dtype=float32, numpy=<span class="number">-1.9751656</span>&gt;,</span><br><span class="line"> &lt;tf.Tensor: id=<span class="number">222</span>, shape=(), dtype=float32, numpy=<span class="number">0.014772797</span>&gt;)</span><br></pre></td></tr></table></figure>

<ul>
<li>通过 tf.argmax(x, axis)和 tf.argmin(x, axis)可以求解在 axis 轴上，x 的最大值、最小值所<br>在的索引号</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">out = tf.random.normal([<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line">out = tf.nn.softmax(out, axis=<span class="number">1</span>) <span class="comment"># 通过softmax函数转换为概率值</span></span><br><span class="line">pred = tf.argmax(out, axis=<span class="number">1</span>) <span class="comment"># 选取概率最大的位置 pred</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">262</span>, shape=(<span class="number">2</span>,), dtype=int64, numpy=array([<span class="number">0</span>, <span class="number">0</span>],dtype=int64)&gt;</span><br></pre></td></tr></table></figure>

<h1 id="张量比较"><a href="#张量比较" class="headerlink" title="张量比较"></a>张量比较</h1><p>为了计算分类任务的准确率等指标，一般需要将预测结果和真实标签比较，统计比较 结果中正确的数量来计算准确率。</p>
<ul>
<li>通过 tf.argmax 获取预测类别</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">out = tf.random.normal([<span class="number">100</span>,<span class="number">10</span>])</span><br><span class="line">out = tf.nn.softmax(out, axis=<span class="number">1</span>) <span class="comment"># 输出转换为概率 </span></span><br><span class="line">pred = tf.argmax(out, axis=<span class="number">1</span>) <span class="comment"># 计算预测值</span></span><br><span class="line"><span class="comment"># 模型生成真实标签</span></span><br><span class="line">y = tf.random.uniform([<span class="number">100</span>],dtype=tf.int64,maxval=<span class="number">10</span>)</span><br><span class="line">out = tf.equal(pred,y) <span class="comment"># 预测值与真实值比较，返回布尔类型的张量</span></span><br><span class="line">out = tf.cast(out, dtype=tf.float32) <span class="comment"># 布尔型转int型 </span></span><br><span class="line">correct = tf.reduce_sum(out) <span class="comment"># 统计True的个数</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">293</span>, shape=(), dtype=float32, numpy=<span class="number">12.0</span>&gt;</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20201203213935313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="填充与复制"><a href="#填充与复制" class="headerlink" title="填充与复制"></a>填充与复制</h1><h2 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h2><ul>
<li>对于图片数据的高和宽、序列信号的长度，维度长度可能各不相同。为了方便网络的 并行计算，需要将不同长度的数据扩张为相同长度。</li>
<li>填充操作可以通过 tf.pad(x, paddings)函数实现，参数 paddings 是包含了多个 [Left Padding,Right Padding]的嵌套方案 List，如[[0,0], [2,1], [1,2]]表示第一个维度不填<br>充，第二个维度左边(起始处)填充两个单元，右边(结束处)填充一个单元，第三个维度左边 填充一个单元，右边填充两个单元。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]) <span class="comment"># 第一个句子 b = tf.constant([7,8,1,6]) # 第二个句子</span></span><br><span class="line">b = tf.pad(b, [[<span class="number">0</span>,<span class="number">2</span>]]) <span class="comment"># 句子末尾填充 2 个 0</span></span><br><span class="line">b <span class="comment"># 填充后的结果</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">3</span>, shape=(<span class="number">6</span>,), dtype=int32, numpy=array([<span class="number">7</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">6</span>,<span class="number">0</span>, <span class="number">0</span>])&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>对于小于 80 个单词的句子，在末尾填充 相应数量的 0;对大于 80 个单词的句子，截断超过规定长度的部分单词</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">total_words = <span class="number">10000</span> <span class="comment"># 设定词汇量大小 max_review_len = 80 # 最大句子长度 embedding_len = 100 # 词向量长度</span></span><br><span class="line"><span class="comment"># 加载IMDB数据集</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) =</span><br><span class="line">keras.datasets.imdb.load_data(num_words=total_words)</span><br><span class="line"><span class="comment"># 将句子填充或截断到相同长度，设置为末尾填充和末尾截断方式</span></span><br><span class="line">x_train = keras.preprocessing.sequence.pad_sequences(x_train,</span><br><span class="line">maxlen=max_review_len,truncating=<span class="string">'post'</span>,padding=<span class="string">'post'</span>)</span><br><span class="line">x_test = keras.preprocessing.sequence.pad_sequences(x_test,</span><br><span class="line">maxlen=max_review_len,truncating=<span class="string">'post'</span>,padding=<span class="string">'post'</span>) print(x_train.shape, x_test.shape) <span class="comment"># 打印等长的句子张量形状</span></span><br></pre></td></tr></table></figure>

<ul>
<li>以 28 × 28大小的图片数据为例，如果网络层所接受的数据高宽为32 × 32，则必须将28 × 28 大小填充到32 × 32，可以选择在图片矩阵的上、下、左、右方向各填充 2 个单元</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 图片上下、左右各填充 2 个单元 tf.pad(x,[[0,0],[2,2],[2,2],[0,0]])</span></span><br></pre></td></tr></table></figure>

<h2 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h2><ul>
<li>通过 tf.tile 函数可以在任意维度将数据重复复制多份，如 shape 为[4,32,32,3]的数据， 复制方案为<br>multiples=[2,3,3,1]，即通道数据不复制，高和宽方向分别复制 2 份，图片数再 复制 1 份</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>]) </span><br><span class="line">tf.tile(x,[<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>]) <span class="comment"># 数据复制</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">25</span>, shape=(<span class="number">8</span>, <span class="number">96</span>, <span class="number">96</span>, <span class="number">3</span>), dtype=float32, numpy= array([[[[ <span class="number">1.20957184e+00</span>, <span class="number">2.82766962e+00</span>, <span class="number">1.65782201e+00</span>],[ <span class="number">3.85402292e-01</span>, <span class="number">2.00732923e+00</span>, <span class="number">-2.79068202e-01</span>],</span><br></pre></td></tr></table></figure>
<h1 id="数据限幅"><a href="#数据限幅" class="headerlink" title="数据限幅"></a>数据限幅</h1><ul>
<li>通过 tf.maximum(x, a)实现数据的下限幅，即𝑥 ∈ [𝑎, +∞);</li>
<li>通过 tf.minimum(x, a)实现数据的上限幅，即𝑥 ∈ (−∞, 𝑎]</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.range(<span class="number">9</span>) </span><br><span class="line">tf.maximum(x,<span class="number">2</span>) <span class="comment"># 下限幅到 2</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">48</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>,<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])&gt;</span><br><span class="line">tf.minimum(x,<span class="number">7</span>) <span class="comment"># 上限幅到 7</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">41</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>,<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>])&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>使用 tf.clip_by_value 函数实现上下限幅</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.range(<span class="number">9</span>) </span><br><span class="line">tf.clip_by_value(x,<span class="number">2</span>,<span class="number">7</span>) <span class="comment"># 限幅为 2~7</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">66</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>,<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>])&gt;</span><br></pre></td></tr></table></figure>

<h1 id="高级操作"><a href="#高级操作" class="headerlink" title="高级操作"></a>高级操作</h1><h2 id="tf-gather"><a href="#tf-gather" class="headerlink" title="tf.gather"></a>tf.gather</h2><ul>
<li>tf.gather 可以实现根据索引号收集数据的目的<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以收集索引号[0,1]，并指定维度axis = 0 </span></span><br><span class="line">tf.gather(x,[<span class="number">0</span>,<span class="number">1</span>],axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 可以收集索引号[0,3,8,11,12,26]，并指定维度axis = 1</span></span><br><span class="line">tf.gather(x,[<span class="number">0</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">26</span>],axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#索引号可以乱序排放</span></span><br><span class="line">tf.gather(a,[<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>],axis=<span class="number">0</span>) <span class="comment"># 收集第 4,2,1,3 号元素</span></span><br></pre></td></tr></table></figure>
<h2 id="tf-gather-nd"><a href="#tf-gather-nd" class="headerlink" title="tf.gather_nd"></a>tf.gather_nd</h2></li>
<li>通过 tf.gather_nd 函数，可以通过指定每次采样点的多维坐标来实现采样多个点的目的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据多维坐标收集数据 </span></span><br><span class="line">tf.gather_nd(x,[[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">256</span>, shape=(<span class="number">3</span>, <span class="number">8</span>), dtype=int32, numpy= array([[<span class="number">45</span>, <span class="number">34</span>, <span class="number">99</span>, <span class="number">17</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">43</span>, <span class="number">86</span>],[<span class="number">11</span>, <span class="number">25</span>, <span class="number">84</span>, <span class="number">95</span>, <span class="number">97</span>, <span class="number">95</span>, <span class="number">69</span>, <span class="number">69</span>],[ <span class="number">0</span>, <span class="number">89</span>, <span class="number">52</span>, <span class="number">29</span>, <span class="number">76</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">98</span>]])&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据多维度坐标收集数据 </span></span><br><span class="line">tf.gather_nd(x,[[<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">259</span>, shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">99</span>, <span class="number">95</span>,<span class="number">76</span>])&gt;</span><br></pre></td></tr></table></figure>

<h2 id="tf-boolean-mask"><a href="#tf-boolean-mask" class="headerlink" title="tf.boolean_mask"></a>tf.boolean_mask</h2><ul>
<li>除了可以通过给定索引号的方式采样，还可以通过给定掩码(Mask)的方式进行采样。<strong>掩码的长度必须与对应维度的长度一致</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据掩码方式采样，给出掩码和维度索引 </span></span><br><span class="line">tf.boolean_mask(x,mask=[<span class="literal">True</span>, <span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>],axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>tf.boolean_mask 的用法其实与 tf.gather 非常类似，只不过一个通过掩码 方式采样，一个直接给出索引号采样</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.uniform([<span class="number">2</span>,<span class="number">3</span>,<span class="number">8</span>],maxval=<span class="number">100</span>,dtype=tf.int32)</span><br><span class="line">tf.gather_nd(x,[[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>]]) <span class="comment"># 多维坐标采集</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">325</span>, shape=(<span class="number">4</span>, <span class="number">8</span>), dtype=int32, numpy= array([[<span class="number">52</span>, <span class="number">81</span>, <span class="number">78</span>, <span class="number">21</span>, <span class="number">50</span>, <span class="number">6</span>, <span class="number">68</span>, <span class="number">19</span>],</span><br><span class="line">[<span class="number">53</span>, <span class="number">70</span>, <span class="number">62</span>, <span class="number">12</span>, <span class="number">7</span>, <span class="number">68</span>, <span class="number">36</span>, <span class="number">84</span>], [<span class="number">62</span>, <span class="number">30</span>, <span class="number">52</span>, <span class="number">60</span>, <span class="number">10</span>, <span class="number">93</span>, <span class="number">33</span>, <span class="number">6</span>], [<span class="number">97</span>, <span class="number">92</span>, <span class="number">59</span>, <span class="number">87</span>, <span class="number">86</span>, <span class="number">49</span>, <span class="number">47</span>, <span class="number">11</span>]])&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多维掩码采样 </span></span><br><span class="line">tf.boolean_mask(x,[[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">True</span>]])</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">354</span>, shape=(<span class="number">4</span>, <span class="number">8</span>), dtype=int32, numpy= array([[<span class="number">52</span>, <span class="number">81</span>, <span class="number">78</span>, <span class="number">21</span>, <span class="number">50</span>, <span class="number">6</span>, <span class="number">68</span>, <span class="number">19</span>],</span><br><span class="line">[<span class="number">53</span>, <span class="number">70</span>, <span class="number">62</span>, <span class="number">12</span>, <span class="number">7</span>, <span class="number">68</span>, <span class="number">36</span>, <span class="number">84</span>],</span><br><span class="line">[<span class="number">62</span>, <span class="number">30</span>, <span class="number">52</span>, <span class="number">60</span>, <span class="number">10</span>, <span class="number">93</span>, <span class="number">33</span>, <span class="number">6</span>], [<span class="number">97</span>, <span class="number">92</span>, <span class="number">59</span>, <span class="number">87</span>, <span class="number">86</span>, <span class="number">49</span>, <span class="number">47</span>, <span class="number">11</span>]])&gt;</span><br></pre></td></tr></table></figure>

<h2 id="tf-where"><a href="#tf-where" class="headerlink" title="tf.where"></a>tf.where</h2><ul>
<li>通过 tf.where(cond, a, b)操作可以根据 cond 条件的真假从参数𝑨或𝑩中读取数据<br><img src="https://img-blog.csdnimg.cn/20201203220631843.png" alt="在这里插入图片描述"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 a 为全 1 矩阵</span></span><br><span class="line">b = tf.zeros([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 b 为全 0 矩阵</span></span><br><span class="line"><span class="comment"># 构造采样条件</span></span><br><span class="line">cond =</span><br><span class="line">tf.constant([[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>],[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>]])</span><br><span class="line">tf.where(cond,a,b) <span class="comment"># 根据条件从 a,b 中采样</span></span><br><span class="line">&lt;tf.Tensor: id=<span class="number">384</span>, shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">      [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>]], dtype=float32)&gt;</span><br><span class="line"><span class="comment">#可以看到，返回的张量中为 1 的位置全部来自张量 a，返回的张量中为 0 的位置来自张量 b。</span></span><br></pre></td></tr></table></figure>

<ul>
<li>我们需要提取张量中所有正数的数据和索引。 首先构造张量 a，并通过比较运算得到所有正数的位置掩码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 a</span></span><br><span class="line">mask=x&gt;<span class="number">0</span> <span class="comment"># 比较操作，等同于 tf.math.greater()  通过比较运算，得到所有正数的掩码</span></span><br><span class="line">indices=tf.where(mask) <span class="comment"># 提取所有大于 0 的元素索引</span></span><br><span class="line">tf.gather_nd(x,indices) <span class="comment"># 提取正数的元素值</span></span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.boolean_mask(x,mask) <span class="comment"># 通过掩码提取正数的元素值</span></span><br></pre></td></tr></table></figure>

<h2 id="scatter-nd"><a href="#scatter-nd" class="headerlink" title="scatter_nd"></a>scatter_nd</h2><ul>
<li>通过 tf.scatter_nd(indices, updates, shape)函数可以高效地刷新张量的部分数据，但是这 个函数只能在全 0 的白板张量上面执行刷新操作，因此可能需要结合其它操作来实现现有 张量的数据刷新功能。<br><img src="https://img-blog.csdnimg.cn/20201203221154114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造需要刷新数据的位置参数，即为 4、3、1 和 7 号位置 </span></span><br><span class="line">indices = tf.constant([[<span class="number">4</span>], [<span class="number">3</span>], [<span class="number">1</span>], [<span class="number">7</span>]])</span><br><span class="line"><span class="comment"># 构造需要写入的数据，4 号位写入 4.4,3 号位写入 3.3，以此类推</span></span><br><span class="line">updates = tf.constant([<span class="number">4.4</span>, <span class="number">3.3</span>, <span class="number">1.1</span>, <span class="number">7.7</span>])</span><br><span class="line"><span class="comment"># 在长度为 8 的全 0 向量上根据 indices 写入 updates 数据 </span></span><br><span class="line">tf.scatter_nd(indices, updates, [<span class="number">8</span>])</span><br></pre></td></tr></table></figure>

<h2 id="meshgrid"><a href="#meshgrid" class="headerlink" title="meshgrid"></a>meshgrid</h2><ul>
<li>通过 tf.meshgrid 函数可以方便地生成二维网格的采样点坐标，方便可视化等应用场<br>合。考虑2个自变量x和y的Sinc函数表达式为：<br><img src="https://img-blog.csdnimg.cn/20201203221449852.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201203221838187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<h1 id="经典数据集加载"><a href="#经典数据集加载" class="headerlink" title="经典数据集加载"></a>经典数据集加载</h1><p>在 TensorFlow 中，keras.datasets 模块提供了常用经典数据集的自动下载、管理、加载 与转换功能，并且提供了 tf.data.Dataset 数据集对象，方便实现多线程(Multi-threading)、预 处理(Preprocessing)、随机打散(Shuffle)和批训练(Training on Batch)等常用数据集的功能。</p>
<ul>
<li>Boston Housing，波士顿房价趋势数据集，用于回归模型训练与测试。</li>
<li>CIFAR10/100，真实图片数据集，用于图片分类任务。</li>
<li>MNIST/Fashion_MNIST，手写数字图片数据集，用于图片分类任务。</li>
<li>IMDB，情感分类任务数据集，用于文本分类任务。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#自动加载 MNIST 数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets <span class="comment"># 导入经典数据集加载模块</span></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">(x, y), (x_test, y_test) = datasets.mnist.load_data()</span><br><span class="line">print(<span class="string">'x:'</span>, x.shape, <span class="string">'y:'</span>, y.shape, <span class="string">'x test:'</span>, x_test.shape, <span class="string">'y test:'</span>,</span><br><span class="line">y_test)</span><br><span class="line"><span class="comment"># 返回数组的形状</span></span><br><span class="line">x: (<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>) y: (<span class="number">60000</span>,) x test: (<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>) y test: [<span class="number">7</span> <span class="number">2</span> <span class="number">1</span> ... <span class="number">4</span></span><br><span class="line"><span class="number">5</span> <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据加载进入内存后，需要转换成 Dataset 对象，才能利用 TensorFlow 提供的各种便 捷功能。</span></span><br><span class="line"><span class="comment">#通过 Dataset.from_tensor_slices 可以将训练部分的数据图片 x 和标签 y 都转换成 Dataset 对象:</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x, y)) <span class="comment"># 构建 Dataset 对象</span></span><br></pre></td></tr></table></figure>

<h2 id="随机打散"><a href="#随机打散" class="headerlink" title="随机打散"></a>随机打散</h2><ul>
<li>通过 Dataset.shuffle(buffer_size)工具可以设置 Dataset 对象随机打散数据之间的顺序</li>
<li>buffer_size 参数指定缓冲池的大小，一般设置为一个较大的常数即可</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_db = train_db.shuffle(<span class="number">10000</span>) <span class="comment"># 随机打散样本，不会打乱样本与标签映射关系</span></span><br></pre></td></tr></table></figure>

<h2 id="批训练"><a href="#批训练" class="headerlink" title="批训练"></a>批训练</h2><ul>
<li>为了一次能够从 Dataset 中产生 Batch Size 数量的样本，需要设置 Dataset 为批训练方式</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_db = train_db.batch(<span class="number">128</span>) <span class="comment"># 设置批训练，batch size 为 128</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Batch Size 一般根据用户 的 GPU 显存资源来设置，当显存不足时，可以适量减少 Batch Size 来减少算法的显存使用量。</li>
</ul>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><ul>
<li>从 keras.datasets 中加载的数据集的格式大部分情况都不能直接满足模型的输入要求， 因此需要根据用户的逻辑自行实现预处理步骤。Dataset 对象通过提供 map(func)工具函 数，可以非常方便地调用用户自定义的预处理逻辑，它实现在 func 函数里。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预处理函数实现在 preprocess 函数中，传入函数名即可 </span></span><br><span class="line">train_db = train_db.map(preprocess)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#将 MNIST 图片数据映射到𝑥 ∈ [0,1]区间，视图调整为 [𝑏, 28 ∗ 28];对于标签数据，</span></span><br><span class="line"><span class="comment">#我们选择在预处理函数里面进行 One-hot 编码。preprocess 函 数实现如下:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span> <span class="comment"># 自定义的预处理函数</span></span><br><span class="line"><span class="comment"># 调用此函数时会自动传入 x,y 对象，shape 为[b, 28, 28], [b] # 标准化到 0~1</span></span><br><span class="line">	x = tf.cast(x, dtype=tf.float32) / <span class="number">255.</span></span><br><span class="line">	x = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">	y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">	y = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line">	<span class="comment"># 打平</span></span><br><span class="line">	<span class="comment"># 转成整型张量</span></span><br><span class="line">	<span class="comment"># one-hot编码</span></span><br><span class="line">	<span class="comment"># 返回的 x,y 将替换传入的 x,y 参数，从而实现数据的预处理功能 return x,y</span></span><br></pre></td></tr></table></figure>

<h2 id="循环训练"><a href="#循环训练" class="headerlink" title="循环训练"></a>循环训练</h2><ul>
<li>对于Dataset对象进行迭代</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> enumerate(train_db): <span class="comment"># 迭代数据集对象，带 step 参数</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> train_db: <span class="comment"># 迭代数据集对象</span></span><br></pre></td></tr></table></figure>

<ul>
<li>完成一个Batch的数据训练叫做一个Step，通过多个step来完成整个训练集的一次迭代，叫做一个Epoch。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">20</span>): <span class="comment"># 训练 Epoch 数</span></span><br><span class="line">	<span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> enumerate(train_db): <span class="comment"># 迭代 Step 数</span></span><br><span class="line"><span class="comment"># training...</span></span><br></pre></td></tr></table></figure>
<ul>
<li>可以通过设置 Dataset 对象，使得数据集对象内部遍历多次才会退出<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_db = train_db.repeat(<span class="number">20</span>) <span class="comment"># 数据集迭代 20 遍才终止</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h1 id="MNIST-测试实战"><a href="#MNIST-测试实战" class="headerlink" title="MNIST 测试实战"></a>MNIST 测试实战</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%%</span></span><br><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span>    matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Default parameters for plots</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'font.size'</span>] = <span class="number">20</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.titlesize'</span>] = <span class="number">20</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">'font.family'</span>] = [<span class="string">'STKaiTi'</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="literal">False</span></span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> datasets, layers, optimizers</span><br><span class="line"><span class="keyword">import</span>  os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>]=<span class="string">'2'</span></span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="comment"># 自定义的预处理函数</span></span><br><span class="line"><span class="comment"># 调用此函数时会自动传入 x,y 对象，shape 为[b, 28, 28], [b] # 标准化到 0~1</span></span><br><span class="line"><span class="comment"># 打平</span></span><br><span class="line">	<span class="comment"># 转成整型张量</span></span><br><span class="line">	<span class="comment"># one-hot编码</span></span><br><span class="line">	<span class="comment"># 返回的 x,y 将替换传入的 x,y 参数，从而实现数据的预处理功能</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="comment"># [b, 28, 28], [b]</span></span><br><span class="line">    print(x.shape,y.shape)</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / <span class="number">255.</span></span><br><span class="line">    x = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    y = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#加载MNIST数据集</span></span><br><span class="line">(x, y), (x_test, y_test) = datasets.mnist.load_data()</span><br><span class="line">print(<span class="string">'x:'</span>, x.shape, <span class="string">'y:'</span>, y.shape, <span class="string">'x test:'</span>, x_test.shape, <span class="string">'y test:'</span>, y_test)</span><br><span class="line"><span class="comment">#batch_size</span></span><br><span class="line">batchsz = <span class="number">512</span></span><br><span class="line"><span class="comment">#通过 Dataset.from_tensor_slices 可以将训练部分的数据图片 x 和标签 y 都转换成 Dataset 对象:</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line"><span class="comment">#将数据集打乱</span></span><br><span class="line">train_db = train_db.shuffle(<span class="number">1000</span>)</span><br><span class="line"><span class="comment">#规定批处理的大小</span></span><br><span class="line">train_db = train_db.batch(batchsz)</span><br><span class="line"><span class="comment">#添加数据预处理的函数</span></span><br><span class="line">train_db = train_db.map(preprocess)</span><br><span class="line"><span class="comment">#数据会迭代20个epoch</span></span><br><span class="line">train_db = train_db.repeat(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将测试集的图片x和标签y转换成Dataset对象</span></span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line"><span class="comment">#与之前相同的操作</span></span><br><span class="line">test_db = test_db.shuffle(<span class="number">1000</span>).batch(batchsz)</span><br><span class="line">x,y = next(iter(train_db))</span><br><span class="line">print(<span class="string">'train sample:'</span>, x.shape, y.shape)</span><br><span class="line"><span class="comment"># print(x[0], y[0])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#%%</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># learning rate</span></span><br><span class="line">    lr = <span class="number">1e-2</span></span><br><span class="line">    accs,losses = [], []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 784 =&gt; 512</span></span><br><span class="line">    w1, b1 = tf.Variable(tf.random.normal([<span class="number">784</span>, <span class="number">256</span>], stddev=<span class="number">0.1</span>)), tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line">    <span class="comment"># 512 =&gt; 256</span></span><br><span class="line">    w2, b2 = tf.Variable(tf.random.normal([<span class="number">256</span>, <span class="number">128</span>], stddev=<span class="number">0.1</span>)), tf.Variable(tf.zeros([<span class="number">128</span>]))</span><br><span class="line">    <span class="comment"># 256 =&gt; 10</span></span><br><span class="line">    w3, b3 = tf.Variable(tf.random.normal([<span class="number">128</span>, <span class="number">10</span>], stddev=<span class="number">0.1</span>)), tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> enumerate(train_db):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [b, 28, 28] =&gt; [b, 784]</span></span><br><span class="line">        x = tf.reshape(x, (<span class="number">-1</span>, <span class="number">784</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># layer1.</span></span><br><span class="line">            h1 = x @ w1 + b1</span><br><span class="line">            h1 = tf.nn.relu(h1)</span><br><span class="line">            <span class="comment"># layer2</span></span><br><span class="line">            h2 = h1 @ w2 + b2</span><br><span class="line">            h2 = tf.nn.relu(h2)</span><br><span class="line">            <span class="comment"># output</span></span><br><span class="line">            out = h2 @ w3 + b3</span><br><span class="line">            <span class="comment"># out = tf.nn.relu(out)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># compute loss</span></span><br><span class="line">            <span class="comment"># [b, 10] - [b, 10]</span></span><br><span class="line">            <span class="comment">#损失函数</span></span><br><span class="line">            loss = tf.square(y-out)</span><br><span class="line">            <span class="comment"># [b, 10] =&gt; scalar</span></span><br><span class="line">            <span class="comment">#每一轮的平均loss</span></span><br><span class="line">            loss = tf.reduce_mean(loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#进行梯度的更新</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])</span><br><span class="line">        <span class="keyword">for</span> p, g <span class="keyword">in</span> zip([w1, b1, w2, b2, w3, b3], grads):</span><br><span class="line">            p.assign_sub(lr * g)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print</span></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">80</span> == <span class="number">0</span>:</span><br><span class="line">            print(step, <span class="string">'loss:'</span>, float(loss))</span><br><span class="line">            losses.append(float(loss))</span><br><span class="line">        <span class="comment">#在测试集上计算准确率</span></span><br><span class="line">        <span class="keyword">if</span> step %<span class="number">80</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># evaluate/test</span></span><br><span class="line">            total, total_correct = <span class="number">0.</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> test_db:</span><br><span class="line">                <span class="comment"># layer1.</span></span><br><span class="line">                h1 = x @ w1 + b1</span><br><span class="line">                h1 = tf.nn.relu(h1)</span><br><span class="line">                <span class="comment"># layer2</span></span><br><span class="line">                h2 = h1 @ w2 + b2</span><br><span class="line">                h2 = tf.nn.relu(h2)</span><br><span class="line">                <span class="comment"># output</span></span><br><span class="line">                out = h2 @ w3 + b3</span><br><span class="line">                <span class="comment"># [b, 10] =&gt; [b]</span></span><br><span class="line">                pred = tf.argmax(out, axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># convert one_hot y to number y</span></span><br><span class="line">                y = tf.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># bool type</span></span><br><span class="line">                correct = tf.equal(pred, y)</span><br><span class="line">                <span class="comment"># bool tensor =&gt; int tensor =&gt; numpy</span></span><br><span class="line">                total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()</span><br><span class="line">                total += x.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            print(step, <span class="string">'Evaluate Acc:'</span>, total_correct/total)</span><br><span class="line"></span><br><span class="line">            accs.append(total_correct/total)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    x = [i*<span class="number">80</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(losses))]</span><br><span class="line">    plt.plot(x, losses, color=<span class="string">'C0'</span>, marker=<span class="string">'s'</span>, label=<span class="string">'训练'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'MSE'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Step'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.savefig(<span class="string">'train.svg'</span>)</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(x, accs, color=<span class="string">'C1'</span>, marker=<span class="string">'s'</span>, label=<span class="string">'测试'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'准确率'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Step'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.savefig(<span class="string">'test.svg'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i></a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="tag"><i class="fa fa-tag"></i></a>
              <a href="/tags/tensorflow/" rel="tag"><i class="fa fa-tag"></i></a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/12/02/Tensorflow-basic-knowledge/" rel="prev" title="Tensorflow2.0基础知识">
      <i class="fa fa-chevron-left"></i> Tensorflow2.0基础知识
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/12/04/Tensorflow-and-Neural-Networks/" rel="next" title="Tensorflow构建简单神经网络">
      Tensorflow构建简单神经网络 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#合并与分割"><span class="nav-number">1.</span> <span class="nav-text">合并与分割</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#合并"><span class="nav-number">1.1.</span> <span class="nav-text">合并</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分割"><span class="nav-number">1.2.</span> <span class="nav-text">分割</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据统计"><span class="nav-number">2.</span> <span class="nav-text">数据统计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#向量范数"><span class="nav-number">2.1.</span> <span class="nav-text">向量范数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最值、均值、和"><span class="nav-number">2.2.</span> <span class="nav-text">最值、均值、和</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#张量比较"><span class="nav-number">3.</span> <span class="nav-text">张量比较</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#填充与复制"><span class="nav-number">4.</span> <span class="nav-text">填充与复制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#填充"><span class="nav-number">4.1.</span> <span class="nav-text">填充</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#复制"><span class="nav-number">4.2.</span> <span class="nav-text">复制</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据限幅"><span class="nav-number">5.</span> <span class="nav-text">数据限幅</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#高级操作"><span class="nav-number">6.</span> <span class="nav-text">高级操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-gather"><span class="nav-number">6.1.</span> <span class="nav-text">tf.gather</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-gather-nd"><span class="nav-number">6.2.</span> <span class="nav-text">tf.gather_nd</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-boolean-mask"><span class="nav-number">6.3.</span> <span class="nav-text">tf.boolean_mask</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-where"><span class="nav-number">6.4.</span> <span class="nav-text">tf.where</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scatter-nd"><span class="nav-number">6.5.</span> <span class="nav-text">scatter_nd</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#meshgrid"><span class="nav-number">6.6.</span> <span class="nav-text">meshgrid</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#经典数据集加载"><span class="nav-number">7.</span> <span class="nav-text">经典数据集加载</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#随机打散"><span class="nav-number">7.1.</span> <span class="nav-text">随机打散</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#批训练"><span class="nav-number">7.2.</span> <span class="nav-text">批训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预处理"><span class="nav-number">7.3.</span> <span class="nav-text">预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环训练"><span class="nav-number">7.4.</span> <span class="nav-text">循环训练</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MNIST-测试实战"><span class="nav-number">8.</span> <span class="nav-text">MNIST 测试实战</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">ccclll777</p>
  <div class="site-description" itemprop="description">胸怀猛虎 细嗅蔷薇</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ccclll777" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ccclll777" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:sdu945860882@gmail.com" title="E-Mail → mailto:sdu945860882@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.weibo.com/6732062654" title="Weibo → https:&#x2F;&#x2F;www.weibo.com&#x2F;6732062654" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/baidu_41871794" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;baidu_41871794" rel="noopener" target="_blank"><i class="gratipay fa-fw"></i>CSDN</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ccclll777</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">391k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">5:55</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>



        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  















  

  

  

</body>
</html>
