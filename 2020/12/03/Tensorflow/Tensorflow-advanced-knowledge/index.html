<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Tensorflow2.0进阶知识 | ccclll777&#39;s blogs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Tensorflow2.0的进阶知识">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow2.0进阶知识">
<meta property="og:url" content="http://yoursite.com/2020/12/03/Tensorflow/Tensorflow-advanced-knowledge/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="Tensorflow2.0的进阶知识">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203200253360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203213935313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203220631843.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203221154114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203221449852.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201203221838187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2020-12-03T08:44:59.000Z">
<meta property="article:modified_time" content="2021-10-16T09:35:29.369Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="python">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20201203200253360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="ccclll777&#39;s blogs" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">ccclll777&#39;s blogs</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Tensorflow/Tensorflow-advanced-knowledge" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/12/03/Tensorflow/Tensorflow-advanced-knowledge/" class="article-date">
  <time datetime="2020-12-03T08:44:59.000Z" itemprop="datePublished">2020-12-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Tensorflow2.0进阶知识
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Tensorflow2.0的进阶知识<br><span id="more"></span></p>
<h1 id="合并与分割"><a href="#合并与分割" class="headerlink" title="合并与分割"></a>合并与分割</h1><h2 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h2><p>张量的合并可以使用拼接(Concatenate)和堆叠(Stack)操作实现，拼接操作并不会产生新 的维度，仅在现有的维度上合并，而堆叠会创建新维度。</p>
<ul>
<li><strong>拼接</strong>，通过 tf.concat(tensors, axis)函数拼接张量，其中参数 tensors 保存了所有需要合并的张量 List，axis 参数指定需要合并的维度索引</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">4</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">4</span>]) </span><br><span class="line">tf.concat([a,b],axis=<span class="number">2</span>) <span class="comment"># 在第三个维度上拼接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">28</span>, shape=(<span class="number">10</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy= array([[[-<span class="number">5.13509691e-01</span>, -<span class="number">1.79707789e+00</span>, <span class="number">6.50747120e-01</span>, ...,<span class="number">2.58447856e-01</span>, <span class="number">8.47878829e-02</span>, <span class="number">4.13468748e-01</span>], [-<span class="number">1.17108583e+00</span>, <span class="number">1.93961406e+00</span>, <span class="number">1.27830813e-02</span>, ...,</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">6</span>,<span class="number">35</span>,<span class="number">8</span>]) tf.concat([a,b],axis=<span class="number">0</span>) <span class="comment"># 非法拼接，其他维度长度不相同</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>堆叠</strong>,拼接操作直接在现有维度上合并数据，并不会创建新的维度。如果在合并数据 时，希望创建一个新的维度，则需要使用 tf.stack 操作。</li>
<li>使用 tf.stack(tensors, axis)可以堆叠方式合并多个张量，通过 tensors 列表表示，参数 axis 指定新维度插入的位置，axis 的用法与 tf.expand_dims 的一致，当axis ≥ 0时，在 axis 之前插入;当axis &lt; 0时，在 axis 之后插入新维度。例如 shape 为[𝑏, 𝑐, h, 𝑤]的张量，在不 同位置通过 stack 操作插入新维度<br><img src="https://img-blog.csdnimg.cn/20201203200253360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">tf.stack([a,b],axis=<span class="number">0</span>) <span class="comment"># 堆叠合并为2个班级，班级维度插入在最前</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">55</span>, shape=(<span class="number">2</span>, <span class="number">35</span>, <span class="number">8</span>), dtype=float32, numpy=</span><br><span class="line">array([[[ <span class="number">3.68728966e-01</span>, -<span class="number">8.54765773e-01</span>, -<span class="number">4.77824420e-01</span>,-<span class="number">3.83714020e-01</span>, -<span class="number">1.73216307e+00</span>, <span class="number">2.03872994e-02</span>, <span class="number">2.63810277e+00</span>, -<span class="number">1.12998331e+00</span>],...</span><br></pre></td></tr></table></figure>
<p>若选择使用 tf.concat 拼接合并，则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.random.normal([<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">tf.concat([a,b],axis=<span class="number">0</span>) <span class="comment"># 拼接方式合并，没有2个班级的概念</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">108</span>, shape=(<span class="number">70</span>, <span class="number">8</span>), dtype=float32, numpy=array([[-<span class="number">0.5516891</span> , -<span class="number">1.5031327</span> , -<span class="number">0.35369992</span>, <span class="number">0.31304857</span>, <span class="number">0.13965549</span>, <span class="number">0.6696881</span> , -<span class="number">0.50115544</span>, <span class="number">0.15550546</span>],[ <span class="number">0.8622069</span> , <span class="number">1.0188094</span> , <span class="number">0.18977325</span>, <span class="number">0.6353301</span> , <span class="number">0.05809061</span>,...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在这里插入代码片</span><br></pre></td></tr></table></figure>
<h2 id="分割"><a href="#分割" class="headerlink" title="分割"></a>分割</h2><ul>
<li>合并操作的逆过程就是分割，将一个张量分拆为多个张量。</li>
<li>通过 tf.split(x, num_or_size_splits, axis)可以完成张量的分割操作，参数意义如下:</li>
<li>x参数:待分割张量</li>
<li>num_or_size_splits参数:切割方案。当num_or_size_splits为单个数值时，如10，表 示等长切割为 10 份;当 num_or_size_splits 为 List 时，List 的每个元素表示每份的长 度，如[2,4,2,2]表示切割为 4 份，每份的长度依次是 2、4、2、2。</li>
<li>axis参数:指定分割的维度索引号</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="comment"># 自定义长度的切割，切割为4份，返回4个张量的列表result</span></span><br><span class="line">result = tf.split(x, num_or_size_splits=[<span class="number">4</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>] ,axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">len</span>(result)</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure>
<ul>
<li>希望在某个维度上全部按长度为 1 的方式分割，还可以使用 tf.unstack(x, axis)函数。这种方式是 tf.split 的一种特殊情况，切割长度固定为 1，只需要指定切割维度 的索引号即可。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">10</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">result = tf.unstack(x,axis=<span class="number">0</span>) <span class="comment"># Unstack为长度为1的张量 </span></span><br><span class="line"><span class="built_in">len</span>(result) <span class="comment"># 返回10个张量的列表</span></span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure>
<h1 id="数据统计"><a href="#数据统计" class="headerlink" title="数据统计"></a>数据统计</h1><p>在神经网络的计算过程中，经常需要统计数据的各种属性，如最值、最值位置、均值、范数等信息。由于张量通常较大，直接观察数据很难获得有用信息，通过获取这些张量的统计信息可以较轻松地推测张量数值的分布。</p>
<h2 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h2><ul>
<li>L1范数，定义为向量𝒙的所有元素绝对值之和<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.ones([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">tf.norm(x,<span class="built_in">ord</span>=<span class="number">1</span>) <span class="comment"># 计算L1范数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">183</span>, shape=(), dtype=float32, numpy=<span class="number">4.0</span>&gt;</span><br></pre></td></tr></table></figure></li>
<li>L2范数，定义为向量𝒙的所有元素的平方和，再开根号</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.norm(x,<span class="built_in">ord</span>=<span class="number">2</span>) <span class="comment"># 计算L2范数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">189</span>, shape=(), dtype=float32, numpy=<span class="number">2.0</span>&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>∞−范数，定义为向量𝒙的所有元素绝对值的最大值:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">tf.norm(x,<span class="built_in">ord</span>=np.inf) <span class="comment"># 计算∞范数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">194</span>, shape=(), dtype=float32, numpy=<span class="number">1.0</span>&gt;</span><br></pre></td></tr></table></figure>
<h2 id="最值、均值、和"><a href="#最值、均值、和" class="headerlink" title="最值、均值、和"></a>最值、均值、和</h2><ul>
<li>通过 tf.reduce_max、tf.reduce_min、tf.reduce_mean、tf.reduce_sum<br>函数可以求解张量在某个维度上的最大、最小、均值、和，也可以求全局最大、最小、均值、和信息。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_min(x,axis=<span class="number">1</span>) <span class="comment"># 统计概率维度上的最小值</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">206</span>, shape=(<span class="number">4</span>,), dtype=float32, numpy=array([-</span><br><span class="line"><span class="number">0.27862206</span>, -<span class="number">2.4480672</span> , -<span class="number">1.9983795</span> , -<span class="number">1.5287997</span> ], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 统计全局的最大、最小、均值、和，返回的张量均为标量 </span></span><br><span class="line">tf.reduce_max(x),tf.reduce_min(x),tf.reduce_mean(x)</span><br><span class="line">(&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">218</span>, shape=(), dtype=float32, numpy=<span class="number">1.8653786</span>&gt;,</span><br><span class="line"> &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">220</span>, shape=(), dtype=float32, numpy=-<span class="number">1.9751656</span>&gt;,</span><br><span class="line"> &lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">222</span>, shape=(), dtype=float32, numpy=<span class="number">0.014772797</span>&gt;)</span><br></pre></td></tr></table></figure>
<ul>
<li>通过 tf.argmax(x, axis)和 tf.argmin(x, axis)可以求解在 axis 轴上，x 的最大值、最小值所<br>在的索引号</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">out = tf.random.normal([<span class="number">2</span>,<span class="number">10</span>])</span><br><span class="line">out = tf.nn.softmax(out, axis=<span class="number">1</span>) <span class="comment"># 通过softmax函数转换为概率值</span></span><br><span class="line">pred = tf.argmax(out, axis=<span class="number">1</span>) <span class="comment"># 选取概率最大的位置 pred</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">262</span>, shape=(<span class="number">2</span>,), dtype=int64, numpy=array([<span class="number">0</span>, <span class="number">0</span>],dtype=int64)&gt;</span><br></pre></td></tr></table></figure>
<h1 id="张量比较"><a href="#张量比较" class="headerlink" title="张量比较"></a>张量比较</h1><p>为了计算分类任务的准确率等指标，一般需要将预测结果和真实标签比较，统计比较 结果中正确的数量来计算准确率。</p>
<ul>
<li>通过 tf.argmax 获取预测类别</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">out = tf.random.normal([<span class="number">100</span>,<span class="number">10</span>])</span><br><span class="line">out = tf.nn.softmax(out, axis=<span class="number">1</span>) <span class="comment"># 输出转换为概率 </span></span><br><span class="line">pred = tf.argmax(out, axis=<span class="number">1</span>) <span class="comment"># 计算预测值</span></span><br><span class="line"><span class="comment"># 模型生成真实标签</span></span><br><span class="line">y = tf.random.uniform([<span class="number">100</span>],dtype=tf.int64,maxval=<span class="number">10</span>)</span><br><span class="line">out = tf.equal(pred,y) <span class="comment"># 预测值与真实值比较，返回布尔类型的张量</span></span><br><span class="line">out = tf.cast(out, dtype=tf.float32) <span class="comment"># 布尔型转int型 </span></span><br><span class="line">correct = tf.reduce_sum(out) <span class="comment"># 统计True的个数</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">293</span>, shape=(), dtype=float32, numpy=<span class="number">12.0</span>&gt;</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20201203213935313.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="填充与复制"><a href="#填充与复制" class="headerlink" title="填充与复制"></a>填充与复制</h1><h2 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h2><ul>
<li>对于图片数据的高和宽、序列信号的长度，维度长度可能各不相同。为了方便网络的 并行计算，需要将不同长度的数据扩张为相同长度。</li>
<li>填充操作可以通过 tf.pad(x, paddings)函数实现，参数 paddings 是包含了多个 [Left Padding,Right Padding]的嵌套方案 List，如[[0,0], [2,1], [1,2]]表示第一个维度不填<br>充，第二个维度左边(起始处)填充两个单元，右边(结束处)填充一个单元，第三个维度左边 填充一个单元，右边填充两个单元。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]) <span class="comment"># 第一个句子 b = tf.constant([7,8,1,6]) # 第二个句子</span></span><br><span class="line">b = tf.pad(b, [[<span class="number">0</span>,<span class="number">2</span>]]) <span class="comment"># 句子末尾填充 2 个 0</span></span><br><span class="line">b <span class="comment"># 填充后的结果</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">3</span>, shape=(<span class="number">6</span>,), dtype=int32, numpy=array([<span class="number">7</span>, <span class="number">8</span>, <span class="number">1</span>, <span class="number">6</span>,<span class="number">0</span>, <span class="number">0</span>])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>对于小于 80 个单词的句子，在末尾填充 相应数量的 0;对大于 80 个单词的句子，截断超过规定长度的部分单词</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">total_words = <span class="number">10000</span> <span class="comment"># 设定词汇量大小 max_review_len = 80 # 最大句子长度 embedding_len = 100 # 词向量长度</span></span><br><span class="line"><span class="comment"># 加载IMDB数据集</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) =</span><br><span class="line">keras.datasets.imdb.load_data(num_words=total_words)</span><br><span class="line"><span class="comment"># 将句子填充或截断到相同长度，设置为末尾填充和末尾截断方式</span></span><br><span class="line">x_train = keras.preprocessing.sequence.pad_sequences(x_train,</span><br><span class="line">maxlen=max_review_len,truncating=<span class="string">&#x27;post&#x27;</span>,padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line">x_test = keras.preprocessing.sequence.pad_sequences(x_test,</span><br><span class="line">maxlen=max_review_len,truncating=<span class="string">&#x27;post&#x27;</span>,padding=<span class="string">&#x27;post&#x27;</span>) print(x_train.shape, x_test.shape) <span class="comment"># 打印等长的句子张量形状</span></span><br></pre></td></tr></table></figure>
<ul>
<li>以 28 × 28大小的图片数据为例，如果网络层所接受的数据高宽为32 × 32，则必须将28 × 28 大小填充到32 × 32，可以选择在图片矩阵的上、下、左、右方向各填充 2 个单元</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 图片上下、左右各填充 2 个单元 tf.pad(x,[[0,0],[2,2],[2,2],[0,0]])</span></span><br></pre></td></tr></table></figure>
<h2 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h2><ul>
<li>通过 tf.tile 函数可以在任意维度将数据重复复制多份，如 shape 为[4,32,32,3]的数据， 复制方案为<br>multiples=[2,3,3,1]，即通道数据不复制，高和宽方向分别复制 2 份，图片数再 复制 1 份</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>]) </span><br><span class="line">tf.tile(x,[<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>]) <span class="comment"># 数据复制</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">25</span>, shape=(<span class="number">8</span>, <span class="number">96</span>, <span class="number">96</span>, <span class="number">3</span>), dtype=float32, numpy= array([[[[ <span class="number">1.20957184e+00</span>, <span class="number">2.82766962e+00</span>, <span class="number">1.65782201e+00</span>],[ <span class="number">3.85402292e-01</span>, <span class="number">2.00732923e+00</span>, -<span class="number">2.79068202e-01</span>],</span><br></pre></td></tr></table></figure>
<h1 id="数据限幅"><a href="#数据限幅" class="headerlink" title="数据限幅"></a>数据限幅</h1><ul>
<li>通过 tf.maximum(x, a)实现数据的下限幅，即𝑥 ∈ [𝑎, +∞);</li>
<li>通过 tf.minimum(x, a)实现数据的上限幅，即𝑥 ∈ (−∞, 𝑎]</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.<span class="built_in">range</span>(<span class="number">9</span>) </span><br><span class="line">tf.maximum(x,<span class="number">2</span>) <span class="comment"># 下限幅到 2</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">48</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>,<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])&gt;</span><br><span class="line">tf.minimum(x,<span class="number">7</span>) <span class="comment"># 上限幅到 7</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">41</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>,<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>])&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>使用 tf.clip_by_value 函数实现上下限幅</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.<span class="built_in">range</span>(<span class="number">9</span>) </span><br><span class="line">tf.clip_by_value(x,<span class="number">2</span>,<span class="number">7</span>) <span class="comment"># 限幅为 2~7</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">66</span>, shape=(<span class="number">9</span>,), dtype=int32, numpy=array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>,<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>])&gt;</span><br></pre></td></tr></table></figure>
<h1 id="高级操作"><a href="#高级操作" class="headerlink" title="高级操作"></a>高级操作</h1><h2 id="tf-gather"><a href="#tf-gather" class="headerlink" title="tf.gather"></a>tf.gather</h2><ul>
<li>tf.gather 可以实现根据索引号收集数据的目的<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以收集索引号[0,1]，并指定维度axis = 0 </span></span><br><span class="line">tf.gather(x,[<span class="number">0</span>,<span class="number">1</span>],axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 可以收集索引号[0,3,8,11,12,26]，并指定维度axis = 1</span></span><br><span class="line">tf.gather(x,[<span class="number">0</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">26</span>],axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#索引号可以乱序排放</span></span><br><span class="line">tf.gather(a,[<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>],axis=<span class="number">0</span>) <span class="comment"># 收集第 4,2,1,3 号元素</span></span><br></pre></td></tr></table></figure>
<h2 id="tf-gather-nd"><a href="#tf-gather-nd" class="headerlink" title="tf.gather_nd"></a>tf.gather_nd</h2></li>
<li>通过 tf.gather_nd 函数，可以通过指定每次采样点的多维坐标来实现采样多个点的目的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据多维坐标收集数据 </span></span><br><span class="line">tf.gather_nd(x,[[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">256</span>, shape=(<span class="number">3</span>, <span class="number">8</span>), dtype=int32, numpy= array([[<span class="number">45</span>, <span class="number">34</span>, <span class="number">99</span>, <span class="number">17</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">43</span>, <span class="number">86</span>],[<span class="number">11</span>, <span class="number">25</span>, <span class="number">84</span>, <span class="number">95</span>, <span class="number">97</span>, <span class="number">95</span>, <span class="number">69</span>, <span class="number">69</span>],[ <span class="number">0</span>, <span class="number">89</span>, <span class="number">52</span>, <span class="number">29</span>, <span class="number">76</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">98</span>]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据多维度坐标收集数据 </span></span><br><span class="line">tf.gather_nd(x,[[<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">259</span>, shape=(<span class="number">3</span>,), dtype=int32, numpy=array([<span class="number">99</span>, <span class="number">95</span>,<span class="number">76</span>])&gt;</span><br></pre></td></tr></table></figure>
<h2 id="tf-boolean-mask"><a href="#tf-boolean-mask" class="headerlink" title="tf.boolean_mask"></a>tf.boolean_mask</h2><ul>
<li>除了可以通过给定索引号的方式采样，还可以通过给定掩码(Mask)的方式进行采样。<strong>掩码的长度必须与对应维度的长度一致</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据掩码方式采样，给出掩码和维度索引 </span></span><br><span class="line">tf.boolean_mask(x,mask=[<span class="literal">True</span>, <span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>],axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>tf.boolean_mask 的用法其实与 tf.gather 非常类似，只不过一个通过掩码 方式采样，一个直接给出索引号采样</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.uniform([<span class="number">2</span>,<span class="number">3</span>,<span class="number">8</span>],maxval=<span class="number">100</span>,dtype=tf.int32)</span><br><span class="line">tf.gather_nd(x,[[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>]]) <span class="comment"># 多维坐标采集</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">325</span>, shape=(<span class="number">4</span>, <span class="number">8</span>), dtype=int32, numpy= array([[<span class="number">52</span>, <span class="number">81</span>, <span class="number">78</span>, <span class="number">21</span>, <span class="number">50</span>, <span class="number">6</span>, <span class="number">68</span>, <span class="number">19</span>],</span><br><span class="line">[<span class="number">53</span>, <span class="number">70</span>, <span class="number">62</span>, <span class="number">12</span>, <span class="number">7</span>, <span class="number">68</span>, <span class="number">36</span>, <span class="number">84</span>], [<span class="number">62</span>, <span class="number">30</span>, <span class="number">52</span>, <span class="number">60</span>, <span class="number">10</span>, <span class="number">93</span>, <span class="number">33</span>, <span class="number">6</span>], [<span class="number">97</span>, <span class="number">92</span>, <span class="number">59</span>, <span class="number">87</span>, <span class="number">86</span>, <span class="number">49</span>, <span class="number">47</span>, <span class="number">11</span>]])&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多维掩码采样 </span></span><br><span class="line">tf.boolean_mask(x,[[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">True</span>]])</span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">354</span>, shape=(<span class="number">4</span>, <span class="number">8</span>), dtype=int32, numpy= array([[<span class="number">52</span>, <span class="number">81</span>, <span class="number">78</span>, <span class="number">21</span>, <span class="number">50</span>, <span class="number">6</span>, <span class="number">68</span>, <span class="number">19</span>],</span><br><span class="line">[<span class="number">53</span>, <span class="number">70</span>, <span class="number">62</span>, <span class="number">12</span>, <span class="number">7</span>, <span class="number">68</span>, <span class="number">36</span>, <span class="number">84</span>],</span><br><span class="line">[<span class="number">62</span>, <span class="number">30</span>, <span class="number">52</span>, <span class="number">60</span>, <span class="number">10</span>, <span class="number">93</span>, <span class="number">33</span>, <span class="number">6</span>], [<span class="number">97</span>, <span class="number">92</span>, <span class="number">59</span>, <span class="number">87</span>, <span class="number">86</span>, <span class="number">49</span>, <span class="number">47</span>, <span class="number">11</span>]])&gt;</span><br></pre></td></tr></table></figure>
<h2 id="tf-where"><a href="#tf-where" class="headerlink" title="tf.where"></a>tf.where</h2><ul>
<li>通过 tf.where(cond, a, b)操作可以根据 cond 条件的真假从参数𝑨或𝑩中读取数据<br><img src="https://img-blog.csdnimg.cn/20201203220631843.png" alt="在这里插入图片描述"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 a 为全 1 矩阵</span></span><br><span class="line">b = tf.zeros([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 b 为全 0 矩阵</span></span><br><span class="line"><span class="comment"># 构造采样条件</span></span><br><span class="line">cond =</span><br><span class="line">tf.constant([[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">False</span>],[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>]])</span><br><span class="line">tf.where(cond,a,b) <span class="comment"># 根据条件从 a,b 中采样</span></span><br><span class="line">&lt;tf.Tensor: <span class="built_in">id</span>=<span class="number">384</span>, shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">      [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>]], dtype=float32)&gt;</span><br><span class="line"><span class="comment">#可以看到，返回的张量中为 1 的位置全部来自张量 a，返回的张量中为 0 的位置来自张量 b。</span></span><br></pre></td></tr></table></figure>
<ul>
<li>我们需要提取张量中所有正数的数据和索引。 首先构造张量 a，并通过比较运算得到所有正数的位置掩码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>]) <span class="comment"># 构造 a</span></span><br><span class="line">mask=x&gt;<span class="number">0</span> <span class="comment"># 比较操作，等同于 tf.math.greater()  通过比较运算，得到所有正数的掩码</span></span><br><span class="line">indices=tf.where(mask) <span class="comment"># 提取所有大于 0 的元素索引</span></span><br><span class="line">tf.gather_nd(x,indices) <span class="comment"># 提取正数的元素值</span></span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.boolean_mask(x,mask) <span class="comment"># 通过掩码提取正数的元素值</span></span><br></pre></td></tr></table></figure>
<h2 id="scatter-nd"><a href="#scatter-nd" class="headerlink" title="scatter_nd"></a>scatter_nd</h2><ul>
<li>通过 tf.scatter_nd(indices, updates, shape)函数可以高效地刷新张量的部分数据，但是这 个函数只能在全 0 的白板张量上面执行刷新操作，因此可能需要结合其它操作来实现现有 张量的数据刷新功能。<br><img src="https://img-blog.csdnimg.cn/20201203221154114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造需要刷新数据的位置参数，即为 4、3、1 和 7 号位置 </span></span><br><span class="line">indices = tf.constant([[<span class="number">4</span>], [<span class="number">3</span>], [<span class="number">1</span>], [<span class="number">7</span>]])</span><br><span class="line"><span class="comment"># 构造需要写入的数据，4 号位写入 4.4,3 号位写入 3.3，以此类推</span></span><br><span class="line">updates = tf.constant([<span class="number">4.4</span>, <span class="number">3.3</span>, <span class="number">1.1</span>, <span class="number">7.7</span>])</span><br><span class="line"><span class="comment"># 在长度为 8 的全 0 向量上根据 indices 写入 updates 数据 </span></span><br><span class="line">tf.scatter_nd(indices, updates, [<span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<h2 id="meshgrid"><a href="#meshgrid" class="headerlink" title="meshgrid"></a>meshgrid</h2><ul>
<li>通过 tf.meshgrid 函数可以方便地生成二维网格的采样点坐标，方便可视化等应用场<br>合。考虑2个自变量x和y的Sinc函数表达式为：<br><img src="https://img-blog.csdnimg.cn/20201203221449852.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201203221838187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<h1 id="经典数据集加载"><a href="#经典数据集加载" class="headerlink" title="经典数据集加载"></a>经典数据集加载</h1><p>在 TensorFlow 中，keras.datasets 模块提供了常用经典数据集的自动下载、管理、加载 与转换功能，并且提供了 tf.data.Dataset 数据集对象，方便实现多线程(Multi-threading)、预 处理(Preprocessing)、随机打散(Shuffle)和批训练(Training on Batch)等常用数据集的功能。</p>
<ul>
<li>Boston Housing，波士顿房价趋势数据集，用于回归模型训练与测试。</li>
<li>CIFAR10/100，真实图片数据集，用于图片分类任务。</li>
<li>MNIST/Fashion_MNIST，手写数字图片数据集，用于图片分类任务。</li>
<li>IMDB，情感分类任务数据集，用于文本分类任务。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#自动加载 MNIST 数据集</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets <span class="comment"># 导入经典数据集加载模块</span></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">(x, y), (x_test, y_test) = datasets.mnist.load_data()</span><br><span class="line">print(<span class="string">&#x27;x:&#x27;</span>, x.shape, <span class="string">&#x27;y:&#x27;</span>, y.shape, <span class="string">&#x27;x test:&#x27;</span>, x_test.shape, <span class="string">&#x27;y test:&#x27;</span>,</span><br><span class="line">y_test)</span><br><span class="line"><span class="comment"># 返回数组的形状</span></span><br><span class="line">x: (<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>) y: (<span class="number">60000</span>,) x test: (<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>) y test: [<span class="number">7</span> <span class="number">2</span> <span class="number">1</span> ... <span class="number">4</span></span><br><span class="line"><span class="number">5</span> <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据加载进入内存后，需要转换成 Dataset 对象，才能利用 TensorFlow 提供的各种便 捷功能。</span></span><br><span class="line"><span class="comment">#通过 Dataset.from_tensor_slices 可以将训练部分的数据图片 x 和标签 y 都转换成 Dataset 对象:</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x, y)) <span class="comment"># 构建 Dataset 对象</span></span><br></pre></td></tr></table></figure>
<h2 id="随机打散"><a href="#随机打散" class="headerlink" title="随机打散"></a>随机打散</h2><ul>
<li>通过 Dataset.shuffle(buffer_size)工具可以设置 Dataset 对象随机打散数据之间的顺序</li>
<li>buffer_size 参数指定缓冲池的大小，一般设置为一个较大的常数即可</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_db = train_db.shuffle(<span class="number">10000</span>) <span class="comment"># 随机打散样本，不会打乱样本与标签映射关系</span></span><br></pre></td></tr></table></figure>
<h2 id="批训练"><a href="#批训练" class="headerlink" title="批训练"></a>批训练</h2><ul>
<li>为了一次能够从 Dataset 中产生 Batch Size 数量的样本，需要设置 Dataset 为批训练方式</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_db = train_db.batch(<span class="number">128</span>) <span class="comment"># 设置批训练，batch size 为 128</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Batch Size 一般根据用户 的 GPU 显存资源来设置，当显存不足时，可以适量减少 Batch Size 来减少算法的显存使用量。</li>
</ul>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><ul>
<li>从 keras.datasets 中加载的数据集的格式大部分情况都不能直接满足模型的输入要求， 因此需要根据用户的逻辑自行实现预处理步骤。Dataset 对象通过提供 map(func)工具函 数，可以非常方便地调用用户自定义的预处理逻辑，它实现在 func 函数里。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预处理函数实现在 preprocess 函数中，传入函数名即可 </span></span><br><span class="line">train_db = train_db.<span class="built_in">map</span>(preprocess)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#将 MNIST 图片数据映射到𝑥 ∈ [0,1]区间，视图调整为 [𝑏, 28 ∗ 28];对于标签数据，</span></span><br><span class="line"><span class="comment">#我们选择在预处理函数里面进行 One-hot 编码。preprocess 函 数实现如下:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">x, y</span>):</span> <span class="comment"># 自定义的预处理函数</span></span><br><span class="line"><span class="comment"># 调用此函数时会自动传入 x,y 对象，shape 为[b, 28, 28], [b] # 标准化到 0~1</span></span><br><span class="line">	x = tf.cast(x, dtype=tf.float32) / <span class="number">255.</span></span><br><span class="line">	x = tf.reshape(x, [-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">	y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">	y = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line">	<span class="comment"># 打平</span></span><br><span class="line">	<span class="comment"># 转成整型张量</span></span><br><span class="line">	<span class="comment"># one-hot编码</span></span><br><span class="line">	<span class="comment"># 返回的 x,y 将替换传入的 x,y 参数，从而实现数据的预处理功能 return x,y</span></span><br></pre></td></tr></table></figure>
<h2 id="循环训练"><a href="#循环训练" class="headerlink" title="循环训练"></a>循环训练</h2><ul>
<li>对于Dataset对象进行迭代</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db): <span class="comment"># 迭代数据集对象，带 step 参数</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> train_db: <span class="comment"># 迭代数据集对象</span></span><br></pre></td></tr></table></figure>
<ul>
<li>完成一个Batch的数据训练叫做一个Step，通过多个step来完成整个训练集的一次迭代，叫做一个Epoch。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>): <span class="comment"># 训练 Epoch 数</span></span><br><span class="line">	<span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db): <span class="comment"># 迭代 Step 数</span></span><br><span class="line"><span class="comment"># training...</span></span><br></pre></td></tr></table></figure>
<ul>
<li>可以通过设置 Dataset 对象，使得数据集对象内部遍历多次才会退出<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_db = train_db.repeat(<span class="number">20</span>) <span class="comment"># 数据集迭代 20 遍才终止</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="MNIST-测试实战"><a href="#MNIST-测试实战" class="headerlink" title="MNIST 测试实战"></a>MNIST 测试实战</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%%</span></span><br><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span>    matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Default parameters for plots</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">20</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.titlesize&#x27;</span>] = <span class="number">20</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;STKaiTi&#x27;</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span></span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> datasets, layers, optimizers</span><br><span class="line"><span class="keyword">import</span>  os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>]=<span class="string">&#x27;2&#x27;</span></span><br><span class="line">print(tf.__version__)</span><br><span class="line"><span class="comment"># 自定义的预处理函数</span></span><br><span class="line"><span class="comment"># 调用此函数时会自动传入 x,y 对象，shape 为[b, 28, 28], [b] # 标准化到 0~1</span></span><br><span class="line"><span class="comment"># 打平</span></span><br><span class="line">	<span class="comment"># 转成整型张量</span></span><br><span class="line">	<span class="comment"># one-hot编码</span></span><br><span class="line">	<span class="comment"># 返回的 x,y 将替换传入的 x,y 参数，从而实现数据的预处理功能</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="comment"># [b, 28, 28], [b]</span></span><br><span class="line">    print(x.shape,y.shape)</span><br><span class="line">    x = tf.cast(x, dtype=tf.float32) / <span class="number">255.</span></span><br><span class="line">    x = tf.reshape(x, [-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">    y = tf.cast(y, dtype=tf.int32)</span><br><span class="line">    y = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#加载MNIST数据集</span></span><br><span class="line">(x, y), (x_test, y_test) = datasets.mnist.load_data()</span><br><span class="line">print(<span class="string">&#x27;x:&#x27;</span>, x.shape, <span class="string">&#x27;y:&#x27;</span>, y.shape, <span class="string">&#x27;x test:&#x27;</span>, x_test.shape, <span class="string">&#x27;y test:&#x27;</span>, y_test)</span><br><span class="line"><span class="comment">#batch_size</span></span><br><span class="line">batchsz = <span class="number">512</span></span><br><span class="line"><span class="comment">#通过 Dataset.from_tensor_slices 可以将训练部分的数据图片 x 和标签 y 都转换成 Dataset 对象:</span></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x, y))</span><br><span class="line"><span class="comment">#将数据集打乱</span></span><br><span class="line">train_db = train_db.shuffle(<span class="number">1000</span>)</span><br><span class="line"><span class="comment">#规定批处理的大小</span></span><br><span class="line">train_db = train_db.batch(batchsz)</span><br><span class="line"><span class="comment">#添加数据预处理的函数</span></span><br><span class="line">train_db = train_db.<span class="built_in">map</span>(preprocess)</span><br><span class="line"><span class="comment">#数据会迭代20个epoch</span></span><br><span class="line">train_db = train_db.repeat(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将测试集的图片x和标签y转换成Dataset对象</span></span><br><span class="line">test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))</span><br><span class="line"><span class="comment">#与之前相同的操作</span></span><br><span class="line">test_db = test_db.shuffle(<span class="number">1000</span>).batch(batchsz)</span><br><span class="line">x,y = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_db))</span><br><span class="line">print(<span class="string">&#x27;train sample:&#x27;</span>, x.shape, y.shape)</span><br><span class="line"><span class="comment"># print(x[0], y[0])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#%%</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># learning rate</span></span><br><span class="line">    lr = <span class="number">1e-2</span></span><br><span class="line">    accs,losses = [], []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 784 =&gt; 512</span></span><br><span class="line">    w1, b1 = tf.Variable(tf.random.normal([<span class="number">784</span>, <span class="number">256</span>], stddev=<span class="number">0.1</span>)), tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line">    <span class="comment"># 512 =&gt; 256</span></span><br><span class="line">    w2, b2 = tf.Variable(tf.random.normal([<span class="number">256</span>, <span class="number">128</span>], stddev=<span class="number">0.1</span>)), tf.Variable(tf.zeros([<span class="number">128</span>]))</span><br><span class="line">    <span class="comment"># 256 =&gt; 10</span></span><br><span class="line">    w3, b3 = tf.Variable(tf.random.normal([<span class="number">128</span>, <span class="number">10</span>], stddev=<span class="number">0.1</span>)), tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_db):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [b, 28, 28] =&gt; [b, 784]</span></span><br><span class="line">        x = tf.reshape(x, (-<span class="number">1</span>, <span class="number">784</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># layer1.</span></span><br><span class="line">            h1 = x @ w1 + b1</span><br><span class="line">            h1 = tf.nn.relu(h1)</span><br><span class="line">            <span class="comment"># layer2</span></span><br><span class="line">            h2 = h1 @ w2 + b2</span><br><span class="line">            h2 = tf.nn.relu(h2)</span><br><span class="line">            <span class="comment"># output</span></span><br><span class="line">            out = h2 @ w3 + b3</span><br><span class="line">            <span class="comment"># out = tf.nn.relu(out)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># compute loss</span></span><br><span class="line">            <span class="comment"># [b, 10] - [b, 10]</span></span><br><span class="line">            <span class="comment">#损失函数</span></span><br><span class="line">            loss = tf.square(y-out)</span><br><span class="line">            <span class="comment"># [b, 10] =&gt; scalar</span></span><br><span class="line">            <span class="comment">#每一轮的平均loss</span></span><br><span class="line">            loss = tf.reduce_mean(loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#进行梯度的更新</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])</span><br><span class="line">        <span class="keyword">for</span> p, g <span class="keyword">in</span> <span class="built_in">zip</span>([w1, b1, w2, b2, w3, b3], grads):</span><br><span class="line">            p.assign_sub(lr * g)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print</span></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">80</span> == <span class="number">0</span>:</span><br><span class="line">            print(step, <span class="string">&#x27;loss:&#x27;</span>, <span class="built_in">float</span>(loss))</span><br><span class="line">            losses.append(<span class="built_in">float</span>(loss))</span><br><span class="line">        <span class="comment">#在测试集上计算准确率</span></span><br><span class="line">        <span class="keyword">if</span> step %<span class="number">80</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># evaluate/test</span></span><br><span class="line">            total, total_correct = <span class="number">0.</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> test_db:</span><br><span class="line">                <span class="comment"># layer1.</span></span><br><span class="line">                h1 = x @ w1 + b1</span><br><span class="line">                h1 = tf.nn.relu(h1)</span><br><span class="line">                <span class="comment"># layer2</span></span><br><span class="line">                h2 = h1 @ w2 + b2</span><br><span class="line">                h2 = tf.nn.relu(h2)</span><br><span class="line">                <span class="comment"># output</span></span><br><span class="line">                out = h2 @ w3 + b3</span><br><span class="line">                <span class="comment"># [b, 10] =&gt; [b]</span></span><br><span class="line">                pred = tf.argmax(out, axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># convert one_hot y to number y</span></span><br><span class="line">                y = tf.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># bool type</span></span><br><span class="line">                correct = tf.equal(pred, y)</span><br><span class="line">                <span class="comment"># bool tensor =&gt; int tensor =&gt; numpy</span></span><br><span class="line">                total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()</span><br><span class="line">                total += x.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            print(step, <span class="string">&#x27;Evaluate Acc:&#x27;</span>, total_correct/total)</span><br><span class="line"></span><br><span class="line">            accs.append(total_correct/total)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    x = [i*<span class="number">80</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(losses))]</span><br><span class="line">    plt.plot(x, losses, color=<span class="string">&#x27;C0&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>, label=<span class="string">&#x27;训练&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;MSE&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Step&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.savefig(<span class="string">&#x27;train.svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(x, accs, color=<span class="string">&#x27;C1&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>, label=<span class="string">&#x27;测试&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;准确率&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Step&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.savefig(<span class="string">&#x27;test.svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/12/03/Tensorflow/Tensorflow-advanced-knowledge/" data-id="ckutyt6or004cav0ah4353xuq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="tag">深度学习框架</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/12/04/Tensorflow/Tensorflow-and-Neural-Networks/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Tensorflow构建简单神经网络
        
      </div>
    </a>
  
  
    <a href="/2020/12/02/Tensorflow/Tensorflow-basic-knowledge/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Tensorflow2.0基础知识</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/PTA%E7%94%B2%E7%BA%A7%E5%88%B7%E9%A2%98/">PTA甲级刷题</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode%E5%88%B7%E9%A2%98/">leetcode刷题</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/">爬虫学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/">论文复现</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/">课程设计</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/" rel="tag">c++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/django/" rel="tag">django</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyquery/" rel="tag">pyquery</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/springboot/" rel="tag">springboot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vue/" rel="tag">vue</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" rel="tag">动态规划</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E6%BA%AF/" rel="tag">回溯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE/" rel="tag">图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/" rel="tag">多文档摘要</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" rel="tag">字符串</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" rel="tag">操作系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%91/" rel="tag">树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E6%8B%9F/" rel="tag">模拟</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82/" rel="tag">模拟网络请求</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="tag">深度学习框架</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90/" rel="tag">网页解析</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/c/" style="font-size: 17.78px;">c++</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/java/" style="font-size: 14.44px;">java</a> <a href="/tags/pyquery/" style="font-size: 10px;">pyquery</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/pytorch/" style="font-size: 15.56px;">pytorch</a> <a href="/tags/springboot/" style="font-size: 10px;">springboot</a> <a href="/tags/tensorflow/" style="font-size: 16.67px;">tensorflow</a> <a href="/tags/vue/" style="font-size: 11.11px;">vue</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 10px;">动态规划</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size: 10px;">回溯</a> <a href="/tags/%E5%9B%BE/" style="font-size: 10px;">图</a> <a href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/" style="font-size: 10px;">多文档摘要</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 10px;">字符串</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">强化学习</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 13.33px;">操作系统</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 11.11px;">数据库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%A0%91/" style="font-size: 12.22px;">树</a> <a href="/tags/%E6%A8%A1%E6%8B%9F/" style="font-size: 10px;">模拟</a> <a href="/tags/%E6%A8%A1%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82/" style="font-size: 11.11px;">模拟网络请求</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 14.44px;">深度学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" style="font-size: 18.89px;">深度学习框架</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 15.56px;">爬虫</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 17.78px;">算法</a> <a href="/tags/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90/" style="font-size: 10px;">网页解析</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/10/16/Arithmetic-LeetCode/282/">282</a>
          </li>
        
          <li>
            <a href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/">Pytorch强化学习算法实现</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/">PyTorch常用工具模块</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/">Pytorch中神经网络工具箱nn模块</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/Pytorch-and-Autograd/">Pytorch中的Autograd</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 ccclll777<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>