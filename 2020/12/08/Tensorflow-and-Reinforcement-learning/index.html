<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="使用tensorflow框架实现强化学习算法，其中包括Policy Gradient ，A3C，DQN等算法">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow_and_Reinforcement_learning">
<meta property="og:url" content="http://yoursite.com/2020/12/08/Tensorflow-and-Reinforcement-learning/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="使用tensorflow框架实现强化学习算法，其中包括Policy Gradient ，A3C，DQN等算法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020120810052028.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208111441250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208142626810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208142526539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208142537282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208145941156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208150024114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208150610547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208150629891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208153008119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208153049582.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208153104220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/202012081649069.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208164928927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208193539146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208193622351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201208193638743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020120819533547.png">
<meta property="article:published_time" content="2020-12-08T12:46:47.000Z">
<meta property="article:modified_time" content="2020-12-08T12:48:48.171Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="python">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/2020120810052028.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">

<link rel="canonical" href="http://yoursite.com/2020/12/08/Tensorflow-and-Reinforcement-learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Tensorflow_and_Reinforcement_learning | ccclll777's blogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
<a href="https://github.com/ccclll777" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ccclll777's blogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/12/08/Tensorflow-and-Reinforcement-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ccclll777">
      <meta itemprop="description" content="胸怀猛虎 细嗅蔷薇">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ccclll777's blogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tensorflow_and_Reinforcement_learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-12-08 20:46:47 / 修改时间：20:48:48" itemprop="dateCreated datePublished" datetime="2020-12-08T20:46:47+08:00">2020-12-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>37k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>33 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>使用tensorflow框架实现强化学习算法，其中包括Policy Gradient ，A3C，DQN等算法<br><a id="more"></a></p>
<h1 id="强化学习算法实例"><a href="#强化学习算法实例" class="headerlink" title="强化学习算法实例"></a>强化学习算法实例</h1><h2 id="平衡杆游戏"><a href="#平衡杆游戏" class="headerlink" title="平衡杆游戏"></a>平衡杆游戏</h2><p><img src="https://img-blog.csdnimg.cn/2020120810052028.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>衡杆游戏系统包含了三个物体:滑轨、小车和杆。如图 ，小车可以自由在<br>滑轨上移动，杆的一侧通过轴承固定在小车上。在初始状态，小车位于滑轨中央，杆竖直<br>立在小车上，智能体通过控制小车的左右移动来控制杆的平衡，当杆与竖直方向的角度大<br>于某个角度或者小车偏离滑轨中心位置一定距离后即视为游戏结束。游戏时间越长，游戏 给予的回报也就越多，智能体的操控水平也越高。</li>
<li>为了简化环境状态的表示，我们这里直接取高层的环境特征向量𝑠作为智能体的输入，它一共包含了四个高层特征，分别为:小车位置、小车速度、杆角度和杆的速度。智能体的输出动作𝑎为向左移动或者向右移动，动作施加在平衡杆系统上会产生一个新的状态， 同时系统也会返回一个奖励值，这个奖励值可以简单的记为 1，即时长加一。在每个时间 戳𝑡上面，智能体通过观察环境状态𝑠𝑡而产生动作𝑎𝑡，环境接收动作后状态改变为𝑠𝑡+1，并返回奖励𝑟 。<h2 id="Gym-平台"><a href="#Gym-平台" class="headerlink" title="Gym 平台"></a>Gym 平台</h2>一般来说，在 Gym 环境中创建游戏并进行交互主要包含了 5 个步骤:</li>
<li><p>创建游戏。通过 gym.make(name)即可创建指定名称 name 的游戏，并返回游戏对象 env。</p>
</li>
<li><p>复位游戏状态。一般游戏环境都具有初始状态，通过调用 env.reset()即可复位游戏状 态，同时返回游戏的初始状态 observation。</p>
</li>
<li>显示游戏画面。通过调用 env.render()即可显示每个时间戳的游戏画面，一般用做测 试。在训练时渲染画面会引入一定的计算代价，因此训练时可不显示画面。</li>
<li>与游戏环境交互。通过 env.step(action)即可执行 action 动作，并返回新的状态 observation、当前奖励 reward、游戏是否结束标志 done 以及额外的信息载体 info。通 过循环此步骤即可持续与环境交互，直至游戏回合结束。</li>
<li>销毁游戏。调用 env.close()即可。</li>
<li>下面演示了一段平衡杆游戏 CartPole-v1 的交互代码，每次交互时在动作空间:{向左，向右}中随机采样一个动作，与环境进行交互，直至游戏结束。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym <span class="comment"># 导入 gym 游戏平台</span></span><br><span class="line">env = gym.make(<span class="string">"CartPole-v1"</span>) <span class="comment"># 创建平衡杆游戏环境</span></span><br><span class="line">observation = env.reset() <span class="comment"># 复位游戏，回到初始状态 </span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>): <span class="comment"># 循环交互 1000 次</span></span><br><span class="line">	env.render() <span class="comment"># 显示当前时间戳的游戏画面</span></span><br><span class="line">	action = env.action_space.sample() <span class="comment"># 随机生成一个动作 </span></span><br><span class="line">	<span class="comment"># 与环境交互，返回新的状态，奖励，是否结束标志，其他信息 </span></span><br><span class="line">	observation, reward, done, info = env.step(action) </span><br><span class="line">	<span class="keyword">if</span> done:<span class="comment">#游戏回合结束，复位状态</span></span><br><span class="line">		observation = env.reset() </span><br><span class="line">env.close() <span class="comment"># 销毁游戏环境</span></span><br></pre></td></tr></table></figure>
<h2 id="策略网络"><a href="#策略网络" class="headerlink" title="策略网络"></a>策略网络</h2><p><img src="https://img-blog.csdnimg.cn/20201208111441250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>将策略网络实现为一个 2 层的全连接网络，第一层将长度为 4 的向量转换为长度 为 128 的向量，第二层将 128 的向量转换为 2 的向量，即动作的概率分布</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policy</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="comment"># 策略网络，生成动作的概率分布</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Policy, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment"># 存储轨迹</span></span><br><span class="line">        <span class="comment"># 输入为长度为4的向量，输出为左、右2个动作</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">128</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        <span class="comment"># 网络优化器</span></span><br><span class="line">        self.optimizer = optimizers.Adam(lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None)</span>:</span></span><br><span class="line">        <span class="comment"># 状态输入s的shape为向量：[4]</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = tf.nn.softmax(self.fc2(x), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>在交互时，我们将每个时间戳上的状态输入𝑠t ，动作分布输出𝑎t ，环境奖励𝑟t 和新状态 𝑠𝑡+1作为一个 4 元组 item 记录下来，用于策略网络的训练.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">put_data</span><span class="params">(self, item)</span>:</span></span><br><span class="line">    <span class="comment"># 记录r,log_P(a|s)z</span></span><br><span class="line">    self.data.append(item)</span><br></pre></td></tr></table></figure>
<ul>
<li>训练以及梯度更新</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_net</span><span class="params">(self, tape)</span>:</span></span><br><span class="line">    <span class="comment"># 计算梯度并更新策略网络参数。tape为梯度记录器</span></span><br><span class="line">    R = <span class="number">0</span> <span class="comment"># 终结状态的初始回报为0</span></span><br><span class="line">    <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::<span class="number">-1</span>]:<span class="comment">#逆序取</span></span><br><span class="line">        R = r + gamma * R <span class="comment"># 计算每个时间戳上的回报</span></span><br><span class="line">        <span class="comment"># 每个时间戳都计算一次梯度</span></span><br><span class="line">        <span class="comment"># grad_R=-log_P*R*grad_theta</span></span><br><span class="line">        loss = -log_prob * R</span><br><span class="line">        <span class="keyword">with</span> tape.stop_recording():</span><br><span class="line">            <span class="comment"># 优化策略网络</span></span><br><span class="line">            grads = tape.gradient(loss, self.trainable_variables)</span><br><span class="line">            <span class="comment"># print(grads)  compute_gradients()返回的值作为输入参数对variable进行更新  防止梯度消失或者梯度爆炸</span></span><br><span class="line">            self.optimizer.apply_gradients(zip(grads, self.trainable_variables))</span><br><span class="line">    self.data = [] <span class="comment"># 清空轨迹</span></span><br></pre></td></tr></table></figure>
<ul>
<li>训练 400 个回合，在回合的开始，复位游戏状态，通过送入输入状态来采样动作，从而与环境进行交互，并记录每一个时间戳的信息，直至游戏回合结束</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    pi = Policy() <span class="comment"># 创建策略网络</span></span><br><span class="line">    pi(tf.random.normal((<span class="number">4</span>,<span class="number">4</span>)))</span><br><span class="line">    pi.summary()</span><br><span class="line">    score = <span class="number">0.0</span> <span class="comment"># 计分</span></span><br><span class="line">    print_interval = <span class="number">20</span> <span class="comment"># 打印间隔</span></span><br><span class="line">    returns = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> range(<span class="number">400</span>):</span><br><span class="line">        s = env.reset() <span class="comment"># 回到游戏初始状态，返回s0</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">501</span>): <span class="comment"># CartPole-v1 forced to terminates at 500 step.</span></span><br><span class="line">                <span class="comment"># 送入状态向量，获取策略</span></span><br><span class="line">                s = tf.constant(s,dtype=tf.float32)</span><br><span class="line">                <span class="comment"># s: [4] =&gt; [1,4]  在第0个维度之前添加一个维度</span></span><br><span class="line">                s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">                prob = pi(s) <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">                <span class="comment"># 从类别分布中采样1个动作, shape: [1]</span></span><br><span class="line">                a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">                a = int(a) <span class="comment"># Tensor转数字</span></span><br><span class="line">                s_prime, r, done, info = env.step(a)</span><br><span class="line">                <span class="comment"># 记录动作a和动作产生的奖励r</span></span><br><span class="line">                <span class="comment"># prob shape:[1,2] </span></span><br><span class="line">                pi.put_data((r, tf.math.log(prob[<span class="number">0</span>][a])))</span><br><span class="line">                s = s_prime <span class="comment"># 刷新状态</span></span><br><span class="line">                score += r <span class="comment"># 累积奖励</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> n_epi &gt;<span class="number">1000</span>:</span><br><span class="line">                    env.render()</span><br><span class="line">                    <span class="comment"># im = Image.fromarray(s)</span></span><br><span class="line">                    <span class="comment"># im.save("res/%d.jpg" % info['frames'][0])</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> done:  <span class="comment"># 当前episode终止</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># episode终止后，训练一次网络</span></span><br><span class="line">            pi.train_net(tape)</span><br><span class="line">        <span class="keyword">del</span> tape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n_epi%print_interval==<span class="number">0</span> <span class="keyword">and</span> n_epi!=<span class="number">0</span>:</span><br><span class="line">            returns.append(score/print_interval)</span><br><span class="line">            print(<span class="string">f"# of episode :<span class="subst">&#123;n_epi&#125;</span>, avg score : <span class="subst">&#123;score/print_interval&#125;</span>"</span>)</span><br><span class="line">            score = <span class="number">0.0</span></span><br><span class="line">    env.close() <span class="comment"># 关闭环境</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.arange(len(returns))*print_interval, returns)</span><br><span class="line">    plt.plot(np.arange(len(returns))*print_interval, returns, <span class="string">'s'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'回合数'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'总回报'</span>)</span><br><span class="line">    plt.savefig(<span class="string">'reinforce-tf-cartpole.svg'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<ul>
<li>完整代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> 	gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span> 	matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Default parameters for plots</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'font.size'</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.titlesize'</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">'font.family'</span>] = [<span class="string">'KaiTi'</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="literal">False</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> 	tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"><span class="keyword">from</span>    PIL <span class="keyword">import</span> Image</span><br><span class="line">env = gym.make(<span class="string">'CartPole-v1'</span>)  <span class="comment"># 创建游戏环境</span></span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">2333</span>)</span><br><span class="line">np.random.seed(<span class="number">2333</span>)</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">'2.'</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma         = <span class="number">0.98</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policy</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="comment"># 策略网络，生成动作的概率分布</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Policy, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment"># 存储轨迹</span></span><br><span class="line">        <span class="comment"># 输入为长度为4的向量，输出为左、右2个动作</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">128</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        <span class="comment"># 网络优化器</span></span><br><span class="line">        self.optimizer = optimizers.Adam(lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None)</span>:</span></span><br><span class="line">        <span class="comment"># 状态输入s的shape为向量：[4]</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = tf.nn.softmax(self.fc2(x), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put_data</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        <span class="comment"># 记录r,log_P(a|s)z</span></span><br><span class="line">        self.data.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_net</span><span class="params">(self, tape)</span>:</span></span><br><span class="line">        <span class="comment"># 计算梯度并更新策略网络参数。tape为梯度记录器</span></span><br><span class="line">        R = <span class="number">0</span> <span class="comment"># 终结状态的初始回报为0</span></span><br><span class="line">        <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::<span class="number">-1</span>]:<span class="comment">#逆序取</span></span><br><span class="line">            R = r + gamma * R <span class="comment"># 计算每个时间戳上的回报</span></span><br><span class="line">            <span class="comment"># 每个时间戳都计算一次梯度</span></span><br><span class="line">            <span class="comment"># grad_R=-log_P*R*grad_theta</span></span><br><span class="line">            loss = -log_prob * R</span><br><span class="line">            <span class="keyword">with</span> tape.stop_recording():</span><br><span class="line">                <span class="comment"># 优化策略网络</span></span><br><span class="line">                grads = tape.gradient(loss, self.trainable_variables)</span><br><span class="line">                <span class="comment"># print(grads)  compute_gradients()返回的值作为输入参数对variable进行更新  防止梯度消失或者梯度爆炸</span></span><br><span class="line">                self.optimizer.apply_gradients(zip(grads, self.trainable_variables))</span><br><span class="line">        self.data = [] <span class="comment"># 清空轨迹</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    pi = Policy() <span class="comment"># 创建策略网络</span></span><br><span class="line">    pi(tf.random.normal((<span class="number">4</span>,<span class="number">4</span>)))</span><br><span class="line">    pi.summary()</span><br><span class="line">    score = <span class="number">0.0</span> <span class="comment"># 计分</span></span><br><span class="line">    print_interval = <span class="number">20</span> <span class="comment"># 打印间隔</span></span><br><span class="line">    returns = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> range(<span class="number">400</span>):</span><br><span class="line">        s = env.reset() <span class="comment"># 回到游戏初始状态，返回s0</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">501</span>): <span class="comment"># CartPole-v1 forced to terminates at 500 step.</span></span><br><span class="line">                <span class="comment"># 送入状态向量，获取策略</span></span><br><span class="line">                s = tf.constant(s,dtype=tf.float32)</span><br><span class="line">                <span class="comment"># s: [4] =&gt; [1,4]  在第0个维度之前添加一个维度</span></span><br><span class="line">                s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">                prob = pi(s) <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">                <span class="comment"># 从类别分布中采样1个动作, shape: [1]</span></span><br><span class="line">                a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">                a = int(a) <span class="comment"># Tensor转数字</span></span><br><span class="line">                s_prime, r, done, info = env.step(a)</span><br><span class="line">                <span class="comment"># 记录动作a和动作产生的奖励r</span></span><br><span class="line">                <span class="comment"># prob shape:[1,2]</span></span><br><span class="line">                pi.put_data((r, tf.math.log(prob[<span class="number">0</span>][a])))</span><br><span class="line">                s = s_prime <span class="comment"># 刷新状态</span></span><br><span class="line">                score += r <span class="comment"># 累积奖励</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> n_epi &gt;<span class="number">1000</span>:</span><br><span class="line">                    env.render()</span><br><span class="line">                    <span class="comment"># im = Image.fromarray(s)</span></span><br><span class="line">                    <span class="comment"># im.save("res/%d.jpg" % info['frames'][0])</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> done:  <span class="comment"># 当前episode终止</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># episode终止后，训练一次网络</span></span><br><span class="line">            pi.train_net(tape)</span><br><span class="line">        <span class="keyword">del</span> tape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n_epi%print_interval==<span class="number">0</span> <span class="keyword">and</span> n_epi!=<span class="number">0</span>:</span><br><span class="line">            returns.append(score/print_interval)</span><br><span class="line">            print(<span class="string">f"# of episode :<span class="subst">&#123;n_epi&#125;</span>, avg score : <span class="subst">&#123;score/print_interval&#125;</span>"</span>)</span><br><span class="line">            score = <span class="number">0.0</span></span><br><span class="line">    env.close() <span class="comment"># 关闭环境</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.arange(len(returns))*print_interval, returns)</span><br><span class="line">    plt.plot(np.arange(len(returns))*print_interval, returns, <span class="string">'s'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'回合数'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'总回报'</span>)</span><br><span class="line">    plt.savefig(<span class="string">'reinforce-tf-cartpole.svg'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h1 id="策略梯度方法（Policy-Gradient-）"><a href="#策略梯度方法（Policy-Gradient-）" class="headerlink" title="策略梯度方法（Policy Gradient ）"></a>策略梯度方法（Policy Gradient ）</h1><h2 id="PPO-算法"><a href="#PPO-算法" class="headerlink" title="PPO 算法"></a>PPO 算法</h2><p><img src="https://img-blog.csdnimg.cn/20201208142626810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li><strong>策略网络</strong>：Actor 网络，策略网络的输入为状态𝑠𝑡，4 个输入节点，输出为动作𝑎𝑡 的概率分布𝜋𝜃(𝑎𝑡|𝑠𝑡)，采用 2 层的全连接层网络实现<br>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># 策略网络，也叫Actor网络，输出为概率分布pi(a|s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">	    <span class="comment"># 策略网络前向传播</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># 输出2个动作的概率分布</span></span><br><span class="line">        x = tf.nn.softmax(x, axis=<span class="number">1</span>) <span class="comment"># 转换成概率</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>基准线𝑏值网络</strong>： Critic 网络，或 V 值函数网络。网络的输入为状态𝑠𝑡，4 个输入 节点，输出为标量值𝑏，采用 2 层全连接层来估计𝑏。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Critic, self).__init__()</span><br><span class="line">        <span class="comment"># 偏置b的估值网络，也叫Critic网络，输出为v(s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">1</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)<span class="comment">#输出基准线b的估计</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>策略网络、值函数网络的创建工作，同时分别创建两个优化器，用于优化 策略网络和值函数网络的参数，我们创建在 PPO 算法主体类的初始化方法中</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PPO</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># PPO算法主体</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(PPO, self).__init__()</span><br><span class="line">        self.actor = Actor() <span class="comment"># 创建Actor网络</span></span><br><span class="line">        self.critic = Critic() <span class="comment"># 创建Critic网络</span></span><br><span class="line">        self.buffer = [] <span class="comment"># 数据缓冲池</span></span><br><span class="line">        self.actor_optimizer = optimizers.Adam(<span class="number">1e-3</span>) <span class="comment"># Actor优化器</span></span><br><span class="line">        self.critic_optimizer = optimizers.Adam(<span class="number">3e-3</span>) <span class="comment"># Critic优化器</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>动作采样</strong> 通过select_action 函数可以计算出当前状态的动作分布𝜋𝜃(𝑎𝑡|𝑠𝑡)，并根据概率随机采样动作，返回动作及其概率</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">    <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">    s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># s: [4] =&gt; [1,4]   在第0个纬度之前插入一个纬度</span></span><br><span class="line">    s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 获取策略分布: [1, 2]</span></span><br><span class="line">    prob = self.actor(s)</span><br><span class="line">    <span class="comment"># 从类别分布中采样1个动作, shape: [1]   tf.random.categorical 返回的是下标的列表</span></span><br><span class="line">    a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    a = int(a)  <span class="comment"># Tensor转数字</span></span><br><span class="line">    <span class="keyword">return</span> a, float(prob[<span class="number">0</span>][a]) <span class="comment"># 返回动作及其概率</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>环境交互</strong> 在主函数 main 中，与环境交互 500 个回合，每个回合通过 select_action 函 数采样策略，并保存进缓冲池，在间隔一段时间调用 agent.optimizer()函数优化策略。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    agent = PPO()</span><br><span class="line">    returns = [] <span class="comment"># 统计总回报</span></span><br><span class="line">    total = <span class="number">0</span> <span class="comment"># 一段时间内平均回报</span></span><br><span class="line">    <span class="keyword">for</span> i_epoch <span class="keyword">in</span> range(<span class="number">500</span>): <span class="comment"># 训练回合数</span></span><br><span class="line">        state = env.reset() <span class="comment"># 复位环境</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>): <span class="comment"># 最多考虑500步</span></span><br><span class="line">            <span class="comment"># 通过最新策略与环境交互</span></span><br><span class="line">            action, action_prob = agent.select_action(state)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            <span class="comment"># 构建样本并存储  'state', 'action', 'a_log_prob 动作出现的概率', 'reward', 'next_state'</span></span><br><span class="line">            trans = Transition(state, action, action_prob, reward, next_state)</span><br><span class="line">            <span class="comment">#存储状态</span></span><br><span class="line">            agent.store_transition(trans)</span><br><span class="line">            state = next_state <span class="comment"># 刷新状态</span></span><br><span class="line">            total += reward <span class="comment"># 累积激励</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done: <span class="comment"># 合适的时间点训练网络</span></span><br><span class="line">                <span class="keyword">if</span> len(agent.buffer) &gt;= batch_size:</span><br><span class="line">                    <span class="comment"># 交互一定轮次之后进行网络的训练</span></span><br><span class="line">                    agent.optimize() <span class="comment"># 训练网络</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i_epoch % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># 每20个回合统计一次平均回报</span></span><br><span class="line">            returns.append(total/<span class="number">20</span>)</span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line">            print(i_epoch, returns[<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>网络优化</strong> 当缓冲池达到一定容量后，通过 optimizer()构建策略网络的误差和值网络的误差，优化网络的参数。首先将数据根据类别转换为 Tensor 类型，然后通过 MC 方法计算 累积回报𝑅(𝜏𝑡:𝑇 )。</li>
</ul>
<blockquote>
<p>MC：蒙特卡罗法，蒙特卡罗法是一种不基于模型的强化问题求解方法。它可以 避免动态规划求解过于复杂，同时还可以不事先知道环境转化模 型，因此可以用于海量数据和复杂模型。但是它也有自己的缺点， 这就是它每次采样都需要一个完整的状态序列</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 优化网络主函数</span></span><br><span class="line">    <span class="comment"># 从缓存中取出样本数据，转换成Tensor</span></span><br><span class="line">    <span class="comment">#状态</span></span><br><span class="line">    state = tf.constant([t.state <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    <span class="comment">#动作</span></span><br><span class="line">    action = tf.constant([t.action <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.int32)</span><br><span class="line">    <span class="comment">#转化成列向量</span></span><br><span class="line">    action = tf.reshape(action,[<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment">#奖励</span></span><br><span class="line">    reward = [t.reward <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer]</span><br><span class="line">    <span class="comment">#选择动作的概率</span></span><br><span class="line">    old_action_log_prob = tf.constant([t.a_log_prob <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    old_action_log_prob = tf.reshape(old_action_log_prob, [<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 通过MC方法循环计算R(st)</span></span><br><span class="line">    R = <span class="number">0</span></span><br><span class="line">    <span class="comment">#存放累计回报的张量</span></span><br><span class="line">    Rs = []</span><br><span class="line">    <span class="comment">#从最后一个开始循环</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> reward[::<span class="number">-1</span>]:</span><br><span class="line">        R = r + gamma * R</span><br><span class="line">        Rs.insert(<span class="number">0</span>, R)</span><br><span class="line">    <span class="comment">#构成张量</span></span><br><span class="line">    Rs = tf.constant(Rs, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<ul>
<li>对缓存池中的数据按 Batch Size 取出，迭代训练 10 遍。对于策略网络，根据 PPO2 算法的误差函数计算;对于值网络，通过均方差计算值网络的预测与𝑅(𝜏t，r )之间的距离，使得值网络的估计越来越准确。<br><img src="https://img-blog.csdnimg.cn/20201208142526539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201208142537282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 优化网络主函数</span></span><br><span class="line">    <span class="comment"># 从缓存中取出样本数据，转换成Tensor</span></span><br><span class="line">    <span class="comment">#状态</span></span><br><span class="line">    state = tf.constant([t.state <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    <span class="comment">#动作</span></span><br><span class="line">    action = tf.constant([t.action <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.int32)</span><br><span class="line">    <span class="comment">#转化成列向量</span></span><br><span class="line">    action = tf.reshape(action,[<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment">#奖励</span></span><br><span class="line">    reward = [t.reward <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer]</span><br><span class="line">    <span class="comment">#选择动作的概率</span></span><br><span class="line">    old_action_log_prob = tf.constant([t.a_log_prob <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    old_action_log_prob = tf.reshape(old_action_log_prob, [<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 通过MC方法循环计算R(st)</span></span><br><span class="line">    R = <span class="number">0</span></span><br><span class="line">    <span class="comment">#存放累计回报的张量</span></span><br><span class="line">    Rs = []</span><br><span class="line">    <span class="comment">#从最后一个开始循环</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> reward[::<span class="number">-1</span>]:</span><br><span class="line">        R = r + gamma * R</span><br><span class="line">        Rs.insert(<span class="number">0</span>, R)</span><br><span class="line">    <span class="comment">#构成张量</span></span><br><span class="line">    Rs = tf.constant(Rs, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># 对缓冲池数据大致迭代10遍</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(round(<span class="number">10</span>*len(self.buffer)/batch_size)):</span><br><span class="line">        <span class="comment"># 随机从缓冲池采样batch size大小样本</span></span><br><span class="line">        index = np.random.choice(np.arange(len(self.buffer)), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 构建梯度跟踪环境</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1, tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">            <span class="comment"># 取出R(st)，[b,1]  tf.gather 取出index对应的数据   然后扩展一个维度</span></span><br><span class="line">            v_target = tf.expand_dims(tf.gather(Rs, index, axis=<span class="number">0</span>), axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 计算v(s)预测值，也就是偏置b，我们后面会介绍为什么写成v  计算偏置b</span></span><br><span class="line">            v = self.critic(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">            delta = v_target - v <span class="comment"># 计算优势值</span></span><br><span class="line">            advantage = tf.stop_gradient(delta) <span class="comment"># 断开梯度连接 </span></span><br><span class="line">            <span class="comment"># 由于TF的gather_nd与pytorch的gather功能不一样，需要构造</span></span><br><span class="line">            <span class="comment"># gather_nd需要的坐标参数，indices:[b, 2]</span></span><br><span class="line">            <span class="comment"># pi_a = pi.gather(1, a) # pytorch只需要一行即可实现</span></span><br><span class="line">            a = tf.gather(action, index, axis=<span class="number">0</span>) <span class="comment"># 取出batch的动作at</span></span><br><span class="line">            <span class="comment"># batch的动作分布pi(a|st)  每个动作出现的概率</span></span><br><span class="line">            pi = self.actor(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">            <span class="comment">#创建序列  扩展维度</span></span><br><span class="line">            indices = tf.expand_dims(tf.range(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment">#与a进行拼接</span></span><br><span class="line">            indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">            pi_a = tf.gather_nd(pi, indices)  <span class="comment"># 动作的概率值pi(at|st), [b]  #指定每次采样点的多维坐标来实现采样多个点的目的</span></span><br><span class="line">            pi_a = tf.expand_dims(pi_a, axis=<span class="number">1</span>)  <span class="comment"># [b]=&gt; [b,1] </span></span><br><span class="line">            <span class="comment"># 重要性采样  不从原分布𝑝中进行采样，而通过另一个分布𝑞中进 行采样，只需要乘以𝑝(𝜏)/𝑞(𝜏)比率即可</span></span><br><span class="line">            ratio = (pi_a / tf.gather(old_action_log_prob, index, axis=<span class="number">0</span>))</span><br><span class="line">            surr1 = ratio * advantage</span><br><span class="line">            <span class="comment">#实现上下限幅</span></span><br><span class="line">            surr2 = tf.clip_by_value(ratio, <span class="number">1</span> - epsilon, <span class="number">1</span> + epsilon) * advantage</span><br><span class="line">            <span class="comment"># PPO误差函数</span></span><br><span class="line">            policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))</span><br><span class="line">            <span class="comment"># 对于偏置v来说，希望与MC估计的R(st)越接近越好</span></span><br><span class="line">            value_loss = losses.MSE(v_target, v)</span><br><span class="line">        <span class="comment"># 优化策略网络</span></span><br><span class="line">        grads = tape1.gradient(policy_loss, self.actor.trainable_variables)</span><br><span class="line">        self.actor_optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))</span><br><span class="line">        <span class="comment"># 优化偏置值网络</span></span><br><span class="line">        grads = tape2.gradient(value_loss, self.critic.trainable_variables)</span><br><span class="line">        self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))</span><br><span class="line"></span><br><span class="line">    self.buffer = []  <span class="comment"># 清空已训练数据</span></span><br></pre></td></tr></table></figure>
<ul>
<li>整体代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span> 	matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">matplotlib.rcParams[<span class="string">'font.size'</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.titlesize'</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">'font.family'</span>] = [<span class="string">'KaiTi'</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span>  gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"><span class="keyword">from</span>    collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span>    torch.utils.data <span class="keyword">import</span> SubsetRandomSampler,BatchSampler</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v1'</span>)  <span class="comment"># 创建游戏环境</span></span><br><span class="line">env.seed(<span class="number">2222</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">2222</span>)</span><br><span class="line">np.random.seed(<span class="number">2222</span>)</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">'2.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.98</span> <span class="comment"># 激励衰减因子</span></span><br><span class="line">epsilon = <span class="number">0.2</span> <span class="comment"># PPO误差超参数0.8~1.2</span></span><br><span class="line">batch_size = <span class="number">32</span> <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建游戏环境</span></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>).unwrapped</span><br><span class="line">Transition = namedtuple(<span class="string">'Transition'</span>, [<span class="string">'state'</span>, <span class="string">'action'</span>, <span class="string">'a_log_prob'</span>, <span class="string">'reward'</span>, <span class="string">'next_state'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># 策略网络，也叫Actor网络，输出为概率分布pi(a|s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = tf.nn.softmax(x, axis=<span class="number">1</span>) <span class="comment"># 转换成概率</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Critic, self).__init__()</span><br><span class="line">        <span class="comment"># 偏置b的估值网络，也叫Critic网络，输出为v(s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">1</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PPO</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># PPO算法主体</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(PPO, self).__init__()</span><br><span class="line">        self.actor = Actor() <span class="comment"># 创建Actor网络</span></span><br><span class="line">        self.critic = Critic() <span class="comment"># 创建Critic网络</span></span><br><span class="line">        self.buffer = [] <span class="comment"># 数据缓冲池</span></span><br><span class="line">        self.actor_optimizer = optimizers.Adam(<span class="number">1e-3</span>) <span class="comment"># Actor优化器</span></span><br><span class="line">        self.critic_optimizer = optimizers.Adam(<span class="number">3e-3</span>) <span class="comment"># Critic优化器</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">        s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># s: [4] =&gt; [1,4]   在第0个纬度之前插入一个纬度</span></span><br><span class="line">        s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 获取策略分布: [1, 2]</span></span><br><span class="line">        prob = self.actor(s)</span><br><span class="line">        <span class="comment"># 从类别分布中采样1个动作, shape: [1]   tf.random.categorical 返回的是下标的列表</span></span><br><span class="line">        a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        a = int(a)  <span class="comment"># Tensor转数字</span></span><br><span class="line">        <span class="keyword">return</span> a, float(prob[<span class="number">0</span>][a]) <span class="comment"># 返回动作及其概率</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">        s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># s: [4] =&gt; [1,4]</span></span><br><span class="line">        s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 获取策略分布: [1, 2]</span></span><br><span class="line">        v = self.critic(s)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> float(v) <span class="comment"># 返回v(s)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span><span class="params">(self, transition)</span>:</span></span><br><span class="line">        <span class="comment"># 存储采样数据</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 优化网络主函数</span></span><br><span class="line">        <span class="comment"># 从缓存中取出样本数据，转换成Tensor</span></span><br><span class="line">        <span class="comment">#状态</span></span><br><span class="line">        state = tf.constant([t.state <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">        <span class="comment">#动作</span></span><br><span class="line">        action = tf.constant([t.action <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.int32)</span><br><span class="line">        <span class="comment">#转化成列向量</span></span><br><span class="line">        action = tf.reshape(action,[<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">        <span class="comment">#奖励</span></span><br><span class="line">        reward = [t.reward <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer]</span><br><span class="line">        <span class="comment">#选择动作的概率</span></span><br><span class="line">        old_action_log_prob = tf.constant([t.a_log_prob <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">        old_action_log_prob = tf.reshape(old_action_log_prob, [<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 通过MC方法循环计算R(st)</span></span><br><span class="line">        R = <span class="number">0</span></span><br><span class="line">        <span class="comment">#存放累计回报的张量</span></span><br><span class="line">        Rs = []</span><br><span class="line">        <span class="comment">#从最后一个开始循环</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> reward[::<span class="number">-1</span>]:</span><br><span class="line">            R = r + gamma * R</span><br><span class="line">            Rs.insert(<span class="number">0</span>, R)</span><br><span class="line">        <span class="comment">#构成张量</span></span><br><span class="line">        Rs = tf.constant(Rs, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># 对缓冲池数据大致迭代10遍</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(round(<span class="number">10</span>*len(self.buffer)/batch_size)):</span><br><span class="line">            <span class="comment"># 随机从缓冲池采样batch size大小样本</span></span><br><span class="line">            index = np.random.choice(np.arange(len(self.buffer)), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line">            <span class="comment"># 构建梯度跟踪环境</span></span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1, tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">                <span class="comment"># 取出R(st)，[b,1]  tf.gather 取出index对应的数据   然后扩展一个维度</span></span><br><span class="line">                v_target = tf.expand_dims(tf.gather(Rs, index, axis=<span class="number">0</span>), axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># 计算v(s)预测值，也就是偏置b，我们后面会介绍为什么写成v  计算偏置b</span></span><br><span class="line">                v = self.critic(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">                delta = v_target - v <span class="comment"># 计算优势值</span></span><br><span class="line">                advantage = tf.stop_gradient(delta) <span class="comment"># 断开梯度连接 </span></span><br><span class="line">                <span class="comment"># 由于TF的gather_nd与pytorch的gather功能不一样，需要构造</span></span><br><span class="line">                <span class="comment"># gather_nd需要的坐标参数，indices:[b, 2]</span></span><br><span class="line">                <span class="comment"># pi_a = pi.gather(1, a) # pytorch只需要一行即可实现</span></span><br><span class="line">                a = tf.gather(action, index, axis=<span class="number">0</span>) <span class="comment"># 取出batch的动作at</span></span><br><span class="line">                <span class="comment"># batch的动作分布pi(a|st)  每个动作出现的概率</span></span><br><span class="line">                pi = self.actor(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">                <span class="comment">#创建序列  扩展维度</span></span><br><span class="line">                indices = tf.expand_dims(tf.range(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment">#与a进行拼接</span></span><br><span class="line">                indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">                pi_a = tf.gather_nd(pi, indices)  <span class="comment"># 动作的概率值pi(at|st), [b]  #指定每次采样点的多维坐标来实现采样多个点的目的</span></span><br><span class="line">                pi_a = tf.expand_dims(pi_a, axis=<span class="number">1</span>)  <span class="comment"># [b]=&gt; [b,1] </span></span><br><span class="line">                <span class="comment"># 重要性采样  不从原分布𝑝中进行采样，而通过另一个分布𝑞中进 行采样，只需要乘以𝑝(𝜏)/𝑞(𝜏)比率即可</span></span><br><span class="line">                ratio = (pi_a / tf.gather(old_action_log_prob, index, axis=<span class="number">0</span>))</span><br><span class="line">                surr1 = ratio * advantage</span><br><span class="line">                <span class="comment">#实现上下限幅</span></span><br><span class="line">                surr2 = tf.clip_by_value(ratio, <span class="number">1</span> - epsilon, <span class="number">1</span> + epsilon) * advantage</span><br><span class="line">                <span class="comment"># PPO误差函数</span></span><br><span class="line">                policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))</span><br><span class="line">                <span class="comment"># 对于偏置v来说，希望与MC估计的R(st)越接近越好</span></span><br><span class="line">                value_loss = losses.MSE(v_target, v)</span><br><span class="line">            <span class="comment"># 优化策略网络</span></span><br><span class="line">            grads = tape1.gradient(policy_loss, self.actor.trainable_variables)</span><br><span class="line">            self.actor_optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))</span><br><span class="line">            <span class="comment"># 优化偏置值网络</span></span><br><span class="line">            grads = tape2.gradient(value_loss, self.critic.trainable_variables)</span><br><span class="line">            self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))</span><br><span class="line"></span><br><span class="line">        self.buffer = []  <span class="comment"># 清空已训练数据</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    agent = PPO()</span><br><span class="line">    returns = [] <span class="comment"># 统计总回报</span></span><br><span class="line">    total = <span class="number">0</span> <span class="comment"># 一段时间内平均回报</span></span><br><span class="line">    <span class="keyword">for</span> i_epoch <span class="keyword">in</span> range(<span class="number">500</span>): <span class="comment"># 训练回合数</span></span><br><span class="line">        state = env.reset() <span class="comment"># 复位环境</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>): <span class="comment"># 最多考虑500步</span></span><br><span class="line">            <span class="comment"># 通过最新策略与环境交互</span></span><br><span class="line">            action, action_prob = agent.select_action(state)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            <span class="comment"># 构建样本并存储  'state', 'action', 'a_log_prob 动作出现的概率', 'reward', 'next_state'</span></span><br><span class="line">            trans = Transition(state, action, action_prob, reward, next_state)</span><br><span class="line">            <span class="comment">#存储状态</span></span><br><span class="line">            agent.store_transition(trans)</span><br><span class="line">            state = next_state <span class="comment"># 刷新状态</span></span><br><span class="line">            total += reward <span class="comment"># 累积激励</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done: <span class="comment"># 合适的时间点训练网络</span></span><br><span class="line">                <span class="keyword">if</span> len(agent.buffer) &gt;= batch_size:</span><br><span class="line">                    <span class="comment"># 交互一定轮次之后进行网络的训练</span></span><br><span class="line">                    agent.optimize() <span class="comment"># 训练网络</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i_epoch % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># 每20个回合统计一次平均回报</span></span><br><span class="line">            returns.append(total/<span class="number">20</span>)</span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line">            print(i_epoch, returns[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">    print(np.array(returns))</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(np.arange(len(returns))*<span class="number">20</span>, np.array(returns))</span><br><span class="line">    plt.plot(np.arange(len(returns))*<span class="number">20</span>, np.array(returns), <span class="string">'s'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'回合数'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'总回报'</span>)</span><br><span class="line">    plt.savefig(<span class="string">'ppo-tf-cartpole.svg'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br><span class="line">    print(<span class="string">"end"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="值函数方法"><a href="#值函数方法" class="headerlink" title="值函数方法"></a>值函数方法</h1><p>策略梯度方法通过直接参数化策略网络，来优化得到更好的策略模型。在强化学习领 域，除了策略方法外，还有另外一类通过建模值函数而间接获得策略的方法，我们把它统 称为值函数方法。</p>
<h2 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h2><p>状态值函数和状态-动作值函数，两者均表示在策略𝜋下的期望回报，轨迹起点定义不一样。</p>
<ul>
<li><strong>状态值函数(State Value Function，简称 V 函数)</strong>：从状态𝑠𝑡开始，在策略𝜋控 制下能获得的期望回报值:<br><img src="https://img-blog.csdnimg.cn/20201208145941156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li><strong>状态-动作值函数(State-Action Value Function，简称 Q 函数)</strong>：从状态𝑠𝑡并执行 动作𝑎𝑡的双重设定下，在策略𝜋控制下能获得的期望回报值<br><img src="https://img-blog.csdnimg.cn/20201208150024114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h2 id="值函数的估计"><a href="#值函数的估计" class="headerlink" title="值函数的估计"></a>值函数的估计</h2></li>
<li><strong>蒙特卡罗方法</strong><br><img src="https://img-blog.csdnimg.cn/20201208150610547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li><strong>时序差分方法</strong><br><img src="https://img-blog.csdnimg.cn/20201208150629891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h2 id="策略改进"><a href="#策略改进" class="headerlink" title="策略改进"></a>策略改进</h2></li>
<li><strong>ε-贪心法</strong><br><img src="https://img-blog.csdnimg.cn/20201208153008119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h2 id="DQN-算法"><a href="#DQN-算法" class="headerlink" title="DQN 算法"></a>DQN 算法</h2><img src="https://img-blog.csdnimg.cn/20201208153049582.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20201208153104220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h2 id="DQN-实战"><a href="#DQN-实战" class="headerlink" title="DQN 实战"></a>DQN 实战</h2></li>
<li><strong>Q 网络</strong>平衡杆游戏的状态是长度为 4 的向量，因此 Q 网络的输入设计为 4 个节点， 经过256 − 256 − 2的全连接层，得到输出节点数为 2 的 Q 函数估值的分布𝑄(𝑠, 𝑎)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 创建Q网络，输入为状态向量，输出为动作的Q值</span></span><br><span class="line">        super(Qnet, self).__init__()</span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        self.fc3 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, training=None)</span>:</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(x))</span><br><span class="line">        x = tf.nn.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>经验回放池</strong>在 DQN 算法中使用了经验回放池来减轻数据之间的强相关性，我们利用 ReplayBuffer 类中的 Deque 对象来实现缓存池的功能。在训练时，通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象，并通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span><span class="params">(self, transition)</span>:</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime, done_mask = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">            done_mask_lst.append([done_mask])</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(s_lst, dtype=tf.float32),\</span><br><span class="line">                      tf.constant(a_lst, dtype=tf.int32), \</span><br><span class="line">                      tf.constant(r_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(s_prime_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(done_mask_lst, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.buffer)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>策略改进</strong> 这里实现了ε-贪心法。在采样动作时，有1 − ε的概率选择arg max 𝑄𝜋 (𝑠, 𝑎)，有ε的概率随机选择一个动作。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_action</span><span class="params">(self, s, epsilon)</span>:</span></span><br><span class="line">    <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">    s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># s: [4] =&gt; [1,4]</span></span><br><span class="line">    s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">    out = self(s)[<span class="number">0</span>]</span><br><span class="line">    coin = random.random()</span><br><span class="line">    <span class="comment"># 策略改进：e-贪心方式</span></span><br><span class="line">    <span class="keyword">if</span> coin &lt; epsilon:</span><br><span class="line">        <span class="comment"># epsilon大的概率随机选取</span></span><br><span class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 选择Q值最大的动作</span></span><br><span class="line">        <span class="keyword">return</span> int(tf.argmax(out))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>网络主流程</strong>网络最多训练 10000 个回合，在回合开始时，首先复位游戏，得到初始状 态𝑠，并从当前 Q 网络中间采样一个动作，与环境进行交互，得到数据对(𝑠, 𝑎, 𝑟, 𝑠′)，并存 入经验回放池。如果当前经验回放池样本数量足够多，则采样一个 Batch 数据，根据 TD 误差优化 Q 网络的估值，直至游戏回合结束。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n_epi <span class="keyword">in</span> range(<span class="number">10000</span>):  <span class="comment"># 训练次数</span></span><br><span class="line">    <span class="comment"># epsilon概率也会8%到1%衰减，越到后面越使用Q值最大的动作</span></span><br><span class="line">    epsilon = max(<span class="number">0.01</span>, <span class="number">0.08</span> - <span class="number">0.01</span> * (n_epi / <span class="number">200</span>))</span><br><span class="line">    s = env.reset()  <span class="comment"># 复位环境</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">600</span>):  <span class="comment"># 一个回合最大时间戳</span></span><br><span class="line">        <span class="comment"># if n_epi&gt;1000:</span></span><br><span class="line">        <span class="comment">#     env.render()</span></span><br><span class="line">        <span class="comment"># 根据当前Q网络提取策略，并改进策略</span></span><br><span class="line">        a = q.sample_action(s, epsilon)</span><br><span class="line">        <span class="comment"># 使用改进的策略与环境交互</span></span><br><span class="line">        s_prime, r, done, info = env.step(a)</span><br><span class="line">        done_mask = <span class="number">0.0</span> <span class="keyword">if</span> done <span class="keyword">else</span> <span class="number">1.0</span>  <span class="comment"># 结束标志掩码</span></span><br><span class="line">        <span class="comment"># 保存5元组</span></span><br><span class="line">        memory.put((s, a, r / <span class="number">100.0</span>, s_prime, done_mask))</span><br><span class="line">        s = s_prime  <span class="comment"># 刷新状态</span></span><br><span class="line">        score += r  <span class="comment"># 记录总回报</span></span><br><span class="line">        <span class="keyword">if</span> done:  <span class="comment"># 回合结束</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> memory.size() &gt; <span class="number">2000</span>:  <span class="comment"># 缓冲池只有大于2000就可以训练</span></span><br><span class="line">        train(q, q_target, memory, optimizer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span> <span class="keyword">and</span> n_epi != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> src, dest <span class="keyword">in</span> zip(q.variables, q_target.variables):</span><br><span class="line">            dest.assign(src)  <span class="comment"># 影子网络权值来自Q</span></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/202012081649069.png" alt="在这里插入图片描述"></p>
<ul>
<li><strong>优化 Q 网络</strong><br><img src="https://img-blog.csdnimg.cn/20201208164928927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(q, q_target, memory, optimizer)</span>:</span></span><br><span class="line">    <span class="comment"># 通过Q网络和影子网络来构造贝尔曼方程的误差，</span></span><br><span class="line">    <span class="comment"># 并只更新Q网络，影子网络的更新会滞后Q网络</span></span><br><span class="line">    <span class="comment">#Smooth L1 误差可以通过 Huber 误差类实现</span></span><br><span class="line">    huber = losses.Huber()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):  <span class="comment"># 训练10次</span></span><br><span class="line">        <span class="comment"># 从缓冲池采样</span></span><br><span class="line">        s, a, r, s_prime, done_mask = memory.sample(batch_size)</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="comment"># s: [b, 4]</span></span><br><span class="line">            q_out = q(s)  <span class="comment"># 得到Q(s,a)的分布</span></span><br><span class="line">            <span class="comment"># 由于TF的gather_nd与pytorch的gather功能不一样，需要构造</span></span><br><span class="line">            <span class="comment"># gather_nd需要的坐标参数，indices:[b, 2]</span></span><br><span class="line">            <span class="comment"># pi_a = pi.gather(1, a) # pytorch只需要一行即可实现</span></span><br><span class="line">            indices = tf.expand_dims(tf.range(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">            indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">            q_a = tf.gather_nd(q_out, indices) <span class="comment"># 动作的概率值, [b]</span></span><br><span class="line">            q_a = tf.expand_dims(q_a, axis=<span class="number">1</span>) <span class="comment"># [b]=&gt; [b,1]</span></span><br><span class="line">            <span class="comment"># 得到Q(s',a)的最大值，它来自影子网络！ [b,4]=&gt;[b,2]=&gt;[b,1]</span></span><br><span class="line">            max_q_prime = tf.reduce_max(q_target(s_prime),axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># 构造Q(s,a_t)的目标值，来自贝尔曼方程</span></span><br><span class="line">            target = r + gamma * max_q_prime * done_mask</span><br><span class="line">            <span class="comment"># 计算Q(s,a_t)与目标值的误差</span></span><br><span class="line">            loss = huber(q_a, target)</span><br><span class="line">        <span class="comment"># 更新网络，使得Q(s,a_t)估计符合贝尔曼方程</span></span><br><span class="line">        grads = tape.gradient(loss, q.trainable_variables)</span><br><span class="line">        <span class="comment"># for p in grads:</span></span><br><span class="line">        <span class="comment">#     print(tf.norm(p))</span></span><br><span class="line">        <span class="comment"># print(grads)</span></span><br><span class="line">        optimizer.apply_gradients(zip(grads, q.trainable_variables))</span><br></pre></td></tr></table></figure>
<ul>
<li>完整代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v1'</span>)  <span class="comment"># 创建游戏环境</span></span><br><span class="line">env.seed(<span class="number">1234</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">1234</span>)</span><br><span class="line">np.random.seed(<span class="number">1234</span>)</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">'2.'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma = <span class="number">0.99</span></span><br><span class="line">buffer_limit = <span class="number">50000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span><span class="params">(self, transition)</span>:</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime, done_mask = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">            done_mask_lst.append([done_mask])</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(s_lst, dtype=tf.float32),\</span><br><span class="line">                      tf.constant(a_lst, dtype=tf.int32), \</span><br><span class="line">                      tf.constant(r_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(s_prime_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(done_mask_lst, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.buffer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 创建Q网络，输入为状态向量，输出为动作的Q值</span></span><br><span class="line">        super(Qnet, self).__init__()</span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line">        self.fc3 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">'he_normal'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x, training=None)</span>:</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(x))</span><br><span class="line">        x = tf.nn.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample_action</span><span class="params">(self, s, epsilon)</span>:</span></span><br><span class="line">        <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">        s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># s: [4] =&gt; [1,4]</span></span><br><span class="line">        s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">        out = self(s)[<span class="number">0</span>]</span><br><span class="line">        coin = random.random()</span><br><span class="line">        <span class="comment"># 策略改进：e-贪心方式</span></span><br><span class="line">        <span class="keyword">if</span> coin &lt; epsilon:</span><br><span class="line">            <span class="comment"># epsilon大的概率随机选取</span></span><br><span class="line">            <span class="keyword">return</span> random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 选择Q值最大的动作</span></span><br><span class="line">            <span class="keyword">return</span> int(tf.argmax(out))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(q, q_target, memory, optimizer)</span>:</span></span><br><span class="line">    <span class="comment"># 通过Q网络和影子网络来构造贝尔曼方程的误差，</span></span><br><span class="line">    <span class="comment"># 并只更新Q网络，影子网络的更新会滞后Q网络</span></span><br><span class="line">    <span class="comment">#Smooth L1 误差可以通过 Huber 误差类实现</span></span><br><span class="line">    huber = losses.Huber()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):  <span class="comment"># 训练10次</span></span><br><span class="line">        <span class="comment"># 从缓冲池采样</span></span><br><span class="line">        s, a, r, s_prime, done_mask = memory.sample(batch_size)</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="comment"># s: [b, 4]</span></span><br><span class="line">            q_out = q(s)  <span class="comment"># 得到Q(s,a)的分布</span></span><br><span class="line">            <span class="comment"># 由于TF的gather_nd与pytorch的gather功能不一样，需要构造</span></span><br><span class="line">            <span class="comment"># gather_nd需要的坐标参数，indices:[b, 2]</span></span><br><span class="line">            <span class="comment"># pi_a = pi.gather(1, a) # pytorch只需要一行即可实现</span></span><br><span class="line">            indices = tf.expand_dims(tf.range(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">            indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">            q_a = tf.gather_nd(q_out, indices) <span class="comment"># 动作的概率值, [b]</span></span><br><span class="line">            q_a = tf.expand_dims(q_a, axis=<span class="number">1</span>) <span class="comment"># [b]=&gt; [b,1]</span></span><br><span class="line">            <span class="comment"># 得到Q(s',a)的最大值，它来自影子网络！ [b,4]=&gt;[b,2]=&gt;[b,1]</span></span><br><span class="line">            max_q_prime = tf.reduce_max(q_target(s_prime),axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># 构造Q(s,a_t)的目标值，来自贝尔曼方程</span></span><br><span class="line">            target = r + gamma * max_q_prime * done_mask</span><br><span class="line">            <span class="comment"># 计算Q(s,a_t)与目标值的误差</span></span><br><span class="line">            loss = huber(q_a, target)</span><br><span class="line">        <span class="comment"># 更新网络，使得Q(s,a_t)估计符合贝尔曼方程</span></span><br><span class="line">        grads = tape.gradient(loss, q.trainable_variables)</span><br><span class="line">        <span class="comment"># for p in grads:</span></span><br><span class="line">        <span class="comment">#     print(tf.norm(p))</span></span><br><span class="line">        <span class="comment"># print(grads)</span></span><br><span class="line">        optimizer.apply_gradients(zip(grads, q.trainable_variables))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    env = gym.make(<span class="string">'CartPole-v1'</span>)  <span class="comment"># 创建环境</span></span><br><span class="line">    q = Qnet()  <span class="comment"># 创建Q网络</span></span><br><span class="line">    q_target = Qnet()  <span class="comment"># 创建影子网络</span></span><br><span class="line">    q.build(input_shape=(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">    q_target.build(input_shape=(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> src, dest <span class="keyword">in</span> zip(q.variables, q_target.variables):</span><br><span class="line">        dest.assign(src) <span class="comment"># 影子网络权值来自Q</span></span><br><span class="line">    memory = ReplayBuffer()  <span class="comment"># 创建回放池</span></span><br><span class="line"></span><br><span class="line">    print_interval = <span class="number">20</span></span><br><span class="line">    score = <span class="number">0.0</span></span><br><span class="line">    optimizer = optimizers.Adam(lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> range(<span class="number">10000</span>):  <span class="comment"># 训练次数</span></span><br><span class="line">        <span class="comment"># epsilon概率也会8%到1%衰减，越到后面越使用Q值最大的动作</span></span><br><span class="line">        epsilon = max(<span class="number">0.01</span>, <span class="number">0.08</span> - <span class="number">0.01</span> * (n_epi / <span class="number">200</span>))</span><br><span class="line">        s = env.reset()  <span class="comment"># 复位环境</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">600</span>):  <span class="comment"># 一个回合最大时间戳</span></span><br><span class="line">            <span class="comment"># if n_epi&gt;1000:</span></span><br><span class="line">            <span class="comment">#     env.render()</span></span><br><span class="line">            <span class="comment"># 根据当前Q网络提取策略，并改进策略</span></span><br><span class="line">            a = q.sample_action(s, epsilon)</span><br><span class="line">            <span class="comment"># 使用改进的策略与环境交互</span></span><br><span class="line">            s_prime, r, done, info = env.step(a)</span><br><span class="line">            done_mask = <span class="number">0.0</span> <span class="keyword">if</span> done <span class="keyword">else</span> <span class="number">1.0</span>  <span class="comment"># 结束标志掩码</span></span><br><span class="line">            <span class="comment"># 保存5元组</span></span><br><span class="line">            memory.put((s, a, r / <span class="number">100.0</span>, s_prime, done_mask))</span><br><span class="line">            s = s_prime  <span class="comment"># 刷新状态</span></span><br><span class="line">            score += r  <span class="comment"># 记录总回报</span></span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># 回合结束</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> memory.size() &gt; <span class="number">2000</span>:  <span class="comment"># 缓冲池只有大于2000就可以训练</span></span><br><span class="line">            train(q, q_target, memory, optimizer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span> <span class="keyword">and</span> n_epi != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> src, dest <span class="keyword">in</span> zip(q.variables, q_target.variables):</span><br><span class="line">                dest.assign(src)  <span class="comment"># 影子网络权值来自Q</span></span><br><span class="line">            print(<span class="string">"# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, "</span> \</span><br><span class="line">                  <span class="string">"epsilon : &#123;:.1f&#125;%"</span> \</span><br><span class="line">                  .format(n_epi, score / print_interval, memory.size(), epsilon * <span class="number">100</span>))</span><br><span class="line">            score = <span class="number">0.0</span></span><br><span class="line">    env.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h1 id="Actor-Critic-方法"><a href="#Actor-Critic-方法" class="headerlink" title="Actor-Critic 方法"></a>Actor-Critic 方法</h1><p><img src="https://img-blog.csdnimg.cn/20201208193539146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="Advantage-AC-算法"><a href="#Advantage-AC-算法" class="headerlink" title="Advantage AC 算法"></a>Advantage AC 算法</h2><p><img src="https://img-blog.csdnimg.cn/20201208193622351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="A3C-算法"><a href="#A3C-算法" class="headerlink" title="A3C 算法"></a>A3C 算法</h2><p><img src="https://img-blog.csdnimg.cn/20201208193638743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>A3C 算法全称为 Asynchronous Advantage Actor-Critic 算法，是 DeepMind 基于Advantage Actor-Critic 算法提出来的异步版本 [8]，将 Actor-Critic 网络部署在多个线程中<br>同时进行训练，并通过全局网络来同步参数。这种异步训练的模式大大提升了训练效率，训练速度更快，并且算法性能也更好。</li>
<li>如图 ，算法会新建一个全局网络 Global Network 和 M 个 Worker 线程， Global Network 包含了Actor 和 Critic 网络，每个线程均新建一个交互环境和 Actor 和 Critic 网络。初始化阶段 Global Network 随机初始化参数𝜃 和𝜙 ，Worker 中的 Actor-Critic 网络从 Global Network中同步拉取参数来初始化网络。在训练时，Worker 中的 Actor-Critic 网络首先从 Global Network拉取最新参数，然后在最新策略𝜋𝜃(𝑎𝑡|𝑠𝑡)才采样动作与私有环 境进行交互，并根据 Advantage Actor-Critic 算法方法计算参数𝜃 和𝜙的梯度信息。完成梯 度计算后，各个 Worker 将梯度信息提交到 Global Network 中，利用 Global Network 的优化 器完成 Global Network的网络参数更新。在算法测试阶段，只使用Global Network 与环境交互即可。</li>
</ul>
<h2 id="A3C-实战"><a href="#A3C-实战" class="headerlink" title="A3C 实战"></a>A3C 实战</h2><ul>
<li>异步的 A3C 算法。和普通的 Advantage AC 算法一样，需要创建 ActorCritic 网络大类，它包含了一个 Actor 子网络和一个 Critic 子网络，有时 Actor 和 Critic 会共享前面网络数层，减少网络的参数量。平衡杆游戏比较简单，我们使用一个 2 层 全连接网络来参数化 Actor 网络，使用另一个 2 层全连接网络来参数化 Critic 网络。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorCritic</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="comment"># Actor-Critic模型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, state_size, action_size)</span>:</span></span><br><span class="line">        super(ActorCritic, self).__init__()</span><br><span class="line">        self.state_size = state_size <span class="comment"># 状态向量长度</span></span><br><span class="line">        self.action_size = action_size <span class="comment"># 动作数量</span></span><br><span class="line">        <span class="comment"># 策略网络Actor</span></span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.policy_logits = layers.Dense(action_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># V网络Critic</span></span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.values = layers.Dense(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>Actor-Critic 的前向传播过程分别计算策略分布𝜋𝜃(𝑎𝑡|𝑠𝑡)和 V 函数估计𝑉𝜋(𝑠𝑡)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">    <span class="comment"># 获得策略分布Pi(a|s)</span></span><br><span class="line">    x = self.dense1(inputs)</span><br><span class="line">    logits = self.policy_logits(x)</span><br><span class="line">    <span class="comment"># 获得v(s)</span></span><br><span class="line">    v = self.dense2(inputs)</span><br><span class="line">    values = self.values(v)</span><br><span class="line">    <span class="keyword">return</span> logits, values</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Worker 线程类</strong> 在 Worker 线程中，实现和 Advantage AC 算法一样的计算流程，只是 计算产生的参数𝜃 和𝜙的梯度信息并不直接用于更新 Worker 的 Actor-Critic 网络，而是提 交到 Global Network 更新。具体地，在 Worker 类初始化阶段，获得 Global Network 传入的 server 对象和 opt 对象，分别代表了 Global Network 模型和优化器;并创建私有的 ActorCritic 网络类 client 和交互环境 env。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span><span class="params">(threading.Thread)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,  server, opt, result_queue, idx)</span>:</span></span><br><span class="line">        super(Worker, self).__init__()</span><br><span class="line">        self.result_queue = result_queue <span class="comment"># 共享队列</span></span><br><span class="line">        self.server = server <span class="comment"># 中央模型</span></span><br><span class="line">        self.opt = opt <span class="comment"># 中央优化器</span></span><br><span class="line">        self.client = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># 线程私有网络</span></span><br><span class="line">        self.worker_idx = idx <span class="comment"># 线程id</span></span><br><span class="line">        self.env = gym.make(<span class="string">'CartPole-v1'</span>).unwrapped</span><br><span class="line">        self.ep_loss = <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<ul>
<li>在线程运行阶段，每个线程最多与环境交互 400 个回合，在回合开始，利用 client 网 络采样动作与环境进行交互，并保存至 Memory 对象。在回合结束，训练 Actor 网络和 Critic 网络，得到参数𝜃 和𝜙的梯度信息，调用 Global Network 的 opt 优化器对象更新 Global Network。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span> </span><br><span class="line">    mem = Memory() <span class="comment"># 每个worker自己维护一个memory</span></span><br><span class="line">    <span class="keyword">for</span> epi_counter <span class="keyword">in</span> range(<span class="number">500</span>): <span class="comment"># 未达到最大回合数</span></span><br><span class="line">        current_state = self.env.reset() <span class="comment"># 复位client游戏状态</span></span><br><span class="line">        mem.clear()</span><br><span class="line">        ep_reward = <span class="number">0.</span></span><br><span class="line">        ep_steps = <span class="number">0</span>  </span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            <span class="comment"># 获得Pi(a|s),未经softmax</span></span><br><span class="line">            logits, _ = self.client(tf.constant(current_state[<span class="literal">None</span>, :],</span><br><span class="line">                                     dtype=tf.float32))</span><br><span class="line">            probs = tf.nn.softmax(logits)</span><br><span class="line">            <span class="comment"># 随机采样动作</span></span><br><span class="line">            action = np.random.choice(<span class="number">2</span>, p=probs.numpy()[<span class="number">0</span>])</span><br><span class="line">            new_state, reward, done, _ = self.env.step(action) <span class="comment"># 交互 </span></span><br><span class="line">            ep_reward += reward <span class="comment"># 累加奖励</span></span><br><span class="line">            mem.store(current_state, action, reward) <span class="comment"># 记录</span></span><br><span class="line">            ep_steps += <span class="number">1</span> <span class="comment"># 计算回合步数</span></span><br><span class="line">            current_state = new_state <span class="comment"># 刷新状态 </span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ep_steps &gt;= <span class="number">500</span> <span class="keyword">or</span> done: <span class="comment"># 最长步数500</span></span><br><span class="line">                <span class="comment"># 计算当前client上的误差</span></span><br><span class="line">                <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                    total_loss = self.compute_loss(done, new_state, mem) </span><br><span class="line">                <span class="comment"># 计算误差</span></span><br><span class="line">                grads = tape.gradient(total_loss, self.client.trainable_weights)</span><br><span class="line">                <span class="comment"># 梯度提交到server，在server上更新梯度</span></span><br><span class="line">                self.opt.apply_gradients(zip(grads,</span><br><span class="line">                                             self.server.trainable_weights))</span><br><span class="line">                <span class="comment"># 从server拉取最新的梯度</span></span><br><span class="line">                self.client.set_weights(self.server.get_weights())</span><br><span class="line">                mem.clear() <span class="comment"># 清空Memory </span></span><br><span class="line">                <span class="comment"># 统计此回合回报</span></span><br><span class="line">                self.result_queue.put(ep_reward)</span><br><span class="line">                print(self.worker_idx, ep_reward)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    self.result_queue.put(<span class="literal">None</span>) <span class="comment"># 结束线程</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Actor-Critic 误差计算</strong><br><img src="https://img-blog.csdnimg.cn/2020120819533547.png" alt=" "></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 done,</span></span></span><br><span class="line"><span class="function"><span class="params">                 new_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                 memory,</span></span></span><br><span class="line"><span class="function"><span class="params">                 gamma=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        reward_sum = <span class="number">0.</span> <span class="comment"># 终止状态的v(终止)=0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        reward_sum = self.client(tf.constant(new_state[<span class="literal">None</span>, :],</span><br><span class="line">                                 dtype=tf.float32))[<span class="number">-1</span>].numpy()[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 统计折扣回报</span></span><br><span class="line">    discounted_rewards = []</span><br><span class="line">    <span class="keyword">for</span> reward <span class="keyword">in</span> memory.rewards[::<span class="number">-1</span>]:  <span class="comment"># reverse buffer r</span></span><br><span class="line">        reward_sum = reward + gamma * reward_sum</span><br><span class="line">        discounted_rewards.append(reward_sum)</span><br><span class="line">    discounted_rewards.reverse()</span><br><span class="line">    <span class="comment"># 获取状态的Pi(a|s)和v(s)</span></span><br><span class="line">    logits, values = self.client(tf.constant(np.vstack(memory.states),</span><br><span class="line">                             dtype=tf.float32))</span><br><span class="line">    <span class="comment"># 计算advantage = R() - v(s)</span></span><br><span class="line">    advantage = tf.constant(np.array(discounted_rewards)[:, <span class="literal">None</span>],</span><br><span class="line">                                     dtype=tf.float32) - values</span><br><span class="line">    <span class="comment"># Critic网络损失</span></span><br><span class="line">    value_loss = advantage ** <span class="number">2</span></span><br><span class="line">    <span class="comment"># 策略损失</span></span><br><span class="line">    policy = tf.nn.softmax(logits)</span><br><span class="line">    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">                    labels=memory.actions, logits=logits)</span><br><span class="line">    <span class="comment"># 计算策略网络损失时，并不会计算V网络</span></span><br><span class="line">    policy_loss = policy_loss * tf.stop_gradient(advantage)</span><br><span class="line">    <span class="comment"># Entropy Bonus  labels标签值（真实值）logits模型的输出</span></span><br><span class="line">    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy,</span><br><span class="line">                                                      logits=logits)</span><br><span class="line">    policy_loss = policy_loss - <span class="number">0.01</span> * entropy</span><br><span class="line">    <span class="comment"># 聚合各个误差</span></span><br><span class="line">    total_loss = tf.reduce_mean((<span class="number">0.5</span> * value_loss + policy_loss))</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>智能体</strong>负责整个 A3C 算法的训练。在智能体类初始化阶段，新建 Global Network 全局网络对象 server 和它的优化器对象 opt。<br>在训练开始时，创建各个 Worker 线程对象，并启动各个线程对象与环境交互，每个 Worker 对象在交互时均会从 Global Network 中拉取最新的网络参数，并利用最新策略与环 境交互，计算各自损失函数，最后提交梯度信息给 Global Network，调用 opt 对象完成 Global Network 的优化更新。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>:</span></span><br><span class="line">    <span class="comment"># 智能体，包含了中央参数网络server</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># server优化器，client不需要，直接从server拉取参数</span></span><br><span class="line">        self.opt = optimizers.Adam(<span class="number">1e-3</span>)</span><br><span class="line">        <span class="comment"># 中央模型，类似于参数服务器</span></span><br><span class="line">        self.server = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># 状态向量，动作数量</span></span><br><span class="line">        self.server(tf.random.normal((<span class="number">2</span>, <span class="number">4</span>)))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">        res_queue = Queue() <span class="comment"># 共享队列</span></span><br><span class="line">        <span class="comment"># 创建各个交互环境</span></span><br><span class="line">        workers = [Worker(self.server, self.opt, res_queue, i)</span><br><span class="line">                   <span class="keyword">for</span> i <span class="keyword">in</span> range(multiprocessing.cpu_count())]</span><br><span class="line">        <span class="keyword">for</span> i, worker <span class="keyword">in</span> enumerate(workers):</span><br><span class="line">            print(<span class="string">"Starting worker &#123;&#125;"</span>.format(i))</span><br><span class="line">            worker.start()</span><br><span class="line">        <span class="comment"># 统计并绘制总回报曲线</span></span><br><span class="line">        returns = []</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            reward = res_queue.get()</span><br><span class="line">            <span class="keyword">if</span> reward <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                returns.append(reward)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 结束标志</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        [w.join() <span class="keyword">for</span> w <span class="keyword">in</span> workers] <span class="comment"># 等待线程退出 </span></span><br><span class="line"></span><br><span class="line">        print(returns)</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(np.arange(len(returns)), returns)</span><br><span class="line">        <span class="comment"># plt.plot(np.arange(len(moving_average_rewards)), np.array(moving_average_rewards), 's')</span></span><br><span class="line">        plt.xlabel(<span class="string">'回合数'</span>)</span><br><span class="line">        plt.ylabel(<span class="string">'总回报'</span>)</span><br><span class="line">        plt.savefig(<span class="string">'a3c-tf-cartpole.svg'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>完整代码</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span>    matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">matplotlib.rcParams[<span class="string">'font.size'</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.titlesize'</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">'font.family'</span>] = [<span class="string">'KaiTi'</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">""</span></span><br><span class="line"><span class="keyword">import</span>  threading</span><br><span class="line"><span class="keyword">import</span>  gym</span><br><span class="line"><span class="keyword">import</span>  multiprocessing</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span>    queue <span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">import</span>  matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">1231</span>)</span><br><span class="line">np.random.seed(<span class="number">1231</span>)</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">'2.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorCritic</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line">    <span class="comment"># Actor-Critic模型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, state_size, action_size)</span>:</span></span><br><span class="line">        super(ActorCritic, self).__init__()</span><br><span class="line">        self.state_size = state_size <span class="comment"># 状态向量长度</span></span><br><span class="line">        self.action_size = action_size <span class="comment"># 动作数量</span></span><br><span class="line">        <span class="comment"># 策略网络Actor</span></span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.policy_logits = layers.Dense(action_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># V网络Critic</span></span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.values = layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="comment"># 获得策略分布Pi(a|s)</span></span><br><span class="line">        x = self.dense1(inputs)</span><br><span class="line">        logits = self.policy_logits(x)</span><br><span class="line">        <span class="comment"># 获得v(s)</span></span><br><span class="line">        v = self.dense2(inputs)</span><br><span class="line">        values = self.values(v)</span><br><span class="line">        <span class="keyword">return</span> logits, values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">record</span><span class="params">(episode,</span></span></span><br><span class="line"><span class="function"><span class="params">           episode_reward,</span></span></span><br><span class="line"><span class="function"><span class="params">           worker_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">           global_ep_reward,</span></span></span><br><span class="line"><span class="function"><span class="params">           result_queue,</span></span></span><br><span class="line"><span class="function"><span class="params">           total_loss,</span></span></span><br><span class="line"><span class="function"><span class="params">           num_steps)</span>:</span></span><br><span class="line">    <span class="comment"># 统计工具函数</span></span><br><span class="line">    <span class="keyword">if</span> global_ep_reward == <span class="number">0</span>:</span><br><span class="line">        global_ep_reward = episode_reward</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        global_ep_reward = global_ep_reward * <span class="number">0.99</span> + episode_reward * <span class="number">0.01</span></span><br><span class="line">    print(</span><br><span class="line">        <span class="string">f"<span class="subst">&#123;episode&#125;</span> | "</span></span><br><span class="line">        <span class="string">f"Average Reward: <span class="subst">&#123;int(global_ep_reward)&#125;</span> | "</span></span><br><span class="line">        <span class="string">f"Episode Reward: <span class="subst">&#123;int(episode_reward)&#125;</span> | "</span></span><br><span class="line">        <span class="string">f"Loss: <span class="subst">&#123;int(total_loss / float(num_steps) * <span class="number">1000</span>) / <span class="number">1000</span>&#125;</span> | "</span></span><br><span class="line">        <span class="string">f"Steps: <span class="subst">&#123;num_steps&#125;</span> | "</span></span><br><span class="line">        <span class="string">f"Worker: <span class="subst">&#123;worker_idx&#125;</span>"</span></span><br><span class="line">    )</span><br><span class="line">    result_queue.put(global_ep_reward) <span class="comment"># 保存回报，传给主线程</span></span><br><span class="line">    <span class="keyword">return</span> global_ep_reward</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Memory</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.states = []</span><br><span class="line">        self.actions = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store</span><span class="params">(self, state, action, reward)</span>:</span></span><br><span class="line">        self.states.append(state)</span><br><span class="line">        self.actions.append(action)</span><br><span class="line">        self.rewards.append(reward)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.states = []</span><br><span class="line">        self.actions = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>:</span></span><br><span class="line">    <span class="comment"># 智能体，包含了中央参数网络server</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># server优化器，client不需要，直接从server拉取参数</span></span><br><span class="line">        self.opt = optimizers.Adam(<span class="number">1e-3</span>)</span><br><span class="line">        <span class="comment"># 中央模型，类似于参数服务器</span></span><br><span class="line">        self.server = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># 状态向量，动作数量</span></span><br><span class="line">        self.server(tf.random.normal((<span class="number">2</span>, <span class="number">4</span>)))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">        res_queue = Queue() <span class="comment"># 共享队列</span></span><br><span class="line">        <span class="comment"># 创建各个交互环境</span></span><br><span class="line">        workers = [Worker(self.server, self.opt, res_queue, i)</span><br><span class="line">                   <span class="keyword">for</span> i <span class="keyword">in</span> range(multiprocessing.cpu_count())]</span><br><span class="line">        <span class="keyword">for</span> i, worker <span class="keyword">in</span> enumerate(workers):</span><br><span class="line">            print(<span class="string">"Starting worker &#123;&#125;"</span>.format(i))</span><br><span class="line">            worker.start()</span><br><span class="line">        <span class="comment"># 统计并绘制总回报曲线</span></span><br><span class="line">        returns = []</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            reward = res_queue.get()</span><br><span class="line">            <span class="keyword">if</span> reward <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                returns.append(reward)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 结束标志</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        [w.join() <span class="keyword">for</span> w <span class="keyword">in</span> workers] <span class="comment"># 等待线程退出 </span></span><br><span class="line"></span><br><span class="line">        print(returns)</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(np.arange(len(returns)), returns)</span><br><span class="line">        <span class="comment"># plt.plot(np.arange(len(moving_average_rewards)), np.array(moving_average_rewards), 's')</span></span><br><span class="line">        plt.xlabel(<span class="string">'回合数'</span>)</span><br><span class="line">        plt.ylabel(<span class="string">'总回报'</span>)</span><br><span class="line">        plt.savefig(<span class="string">'a3c-tf-cartpole.svg'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span><span class="params">(threading.Thread)</span>:</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,  server, opt, result_queue, idx)</span>:</span></span><br><span class="line">        super(Worker, self).__init__()</span><br><span class="line">        self.result_queue = result_queue <span class="comment"># 共享队列</span></span><br><span class="line">        self.server = server <span class="comment"># 中央模型</span></span><br><span class="line">        self.opt = opt <span class="comment"># 中央优化器</span></span><br><span class="line">        self.client = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># 线程私有网络</span></span><br><span class="line">        self.worker_idx = idx <span class="comment"># 线程id</span></span><br><span class="line">        self.env = gym.make(<span class="string">'CartPole-v1'</span>).unwrapped</span><br><span class="line">        self.ep_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span> </span><br><span class="line">        mem = Memory() <span class="comment"># 每个worker自己维护一个memory</span></span><br><span class="line">        <span class="keyword">for</span> epi_counter <span class="keyword">in</span> range(<span class="number">500</span>): <span class="comment"># 未达到最大回合数</span></span><br><span class="line">            current_state = self.env.reset() <span class="comment"># 复位client游戏状态</span></span><br><span class="line">            mem.clear()</span><br><span class="line">            ep_reward = <span class="number">0.</span></span><br><span class="line">            ep_steps = <span class="number">0</span>  </span><br><span class="line">            done = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">                <span class="comment"># 获得Pi(a|s),未经softmax</span></span><br><span class="line">                logits, _ = self.client(tf.constant(current_state[<span class="literal">None</span>, :],</span><br><span class="line">                                         dtype=tf.float32))</span><br><span class="line">                probs = tf.nn.softmax(logits)</span><br><span class="line">                <span class="comment"># 随机采样动作</span></span><br><span class="line">                action = np.random.choice(<span class="number">2</span>, p=probs.numpy()[<span class="number">0</span>])</span><br><span class="line">                new_state, reward, done, _ = self.env.step(action) <span class="comment"># 交互 </span></span><br><span class="line">                ep_reward += reward <span class="comment"># 累加奖励</span></span><br><span class="line">                mem.store(current_state, action, reward) <span class="comment"># 记录</span></span><br><span class="line">                ep_steps += <span class="number">1</span> <span class="comment"># 计算回合步数</span></span><br><span class="line">                current_state = new_state <span class="comment"># 刷新状态 </span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> ep_steps &gt;= <span class="number">500</span> <span class="keyword">or</span> done: <span class="comment"># 最长步数500</span></span><br><span class="line">                    <span class="comment"># 计算当前client上的误差</span></span><br><span class="line">                    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                        total_loss = self.compute_loss(done, new_state, mem) </span><br><span class="line">                    <span class="comment"># 计算误差</span></span><br><span class="line">                    grads = tape.gradient(total_loss, self.client.trainable_weights)</span><br><span class="line">                    <span class="comment"># 梯度提交到server，在server上更新梯度</span></span><br><span class="line">                    self.opt.apply_gradients(zip(grads,</span><br><span class="line">                                                 self.server.trainable_weights))</span><br><span class="line">                    <span class="comment"># 从server拉取最新的梯度</span></span><br><span class="line">                    self.client.set_weights(self.server.get_weights())</span><br><span class="line">                    mem.clear() <span class="comment"># 清空Memory </span></span><br><span class="line">                    <span class="comment"># 统计此回合回报</span></span><br><span class="line">                    self.result_queue.put(ep_reward)</span><br><span class="line">                    print(self.worker_idx, ep_reward)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        self.result_queue.put(<span class="literal">None</span>) <span class="comment"># 结束线程</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                     done,</span></span></span><br><span class="line"><span class="function"><span class="params">                     new_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                     memory,</span></span></span><br><span class="line"><span class="function"><span class="params">                     gamma=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward_sum = <span class="number">0.</span> <span class="comment"># 终止状态的v(终止)=0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reward_sum = self.client(tf.constant(new_state[<span class="literal">None</span>, :],</span><br><span class="line">                                     dtype=tf.float32))[<span class="number">-1</span>].numpy()[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 统计折扣回报</span></span><br><span class="line">        discounted_rewards = []</span><br><span class="line">        <span class="keyword">for</span> reward <span class="keyword">in</span> memory.rewards[::<span class="number">-1</span>]:  <span class="comment"># reverse buffer r</span></span><br><span class="line">            reward_sum = reward + gamma * reward_sum</span><br><span class="line">            discounted_rewards.append(reward_sum)</span><br><span class="line">        discounted_rewards.reverse()</span><br><span class="line">        <span class="comment"># 获取状态的Pi(a|s)和v(s)</span></span><br><span class="line">        logits, values = self.client(tf.constant(np.vstack(memory.states),</span><br><span class="line">                                 dtype=tf.float32))</span><br><span class="line">        <span class="comment"># 计算advantage = R() - v(s)</span></span><br><span class="line">        advantage = tf.constant(np.array(discounted_rewards)[:, <span class="literal">None</span>],</span><br><span class="line">                                         dtype=tf.float32) - values</span><br><span class="line">        <span class="comment"># Critic网络损失</span></span><br><span class="line">        value_loss = advantage ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 策略损失</span></span><br><span class="line">        policy = tf.nn.softmax(logits)</span><br><span class="line">        policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">                        labels=memory.actions, logits=logits)</span><br><span class="line">        <span class="comment"># 计算策略网络损失时，并不会计算V网络</span></span><br><span class="line">        policy_loss = policy_loss * tf.stop_gradient(advantage)</span><br><span class="line">        <span class="comment"># Entropy Bonus  labels标签值（真实值）logits模型的输出</span></span><br><span class="line">        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy,</span><br><span class="line">                                                          logits=logits)</span><br><span class="line">        policy_loss = policy_loss - <span class="number">0.01</span> * entropy</span><br><span class="line">        <span class="comment"># 聚合各个误差</span></span><br><span class="line">        total_loss = tf.reduce_mean((<span class="number">0.5</span> * value_loss + policy_loss))</span><br><span class="line">        <span class="keyword">return</span> total_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    master = Agent()</span><br><span class="line">    master.train()</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i></a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="tag"><i class="fa fa-tag"></i></a>
              <a href="/tags/tensorflow/" rel="tag"><i class="fa fa-tag"></i></a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/12/07/Reinforcement-Learning-Basic-Theory/" rel="prev" title="强化学习基础理论">
      <i class="fa fa-chevron-left"></i> 强化学习基础理论
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/12/09/Pytorch-Introductory-knowledge/" rel="next" title="PyTorch入门知识">
      PyTorch入门知识 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#强化学习算法实例"><span class="nav-number">1.</span> <span class="nav-text">强化学习算法实例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#平衡杆游戏"><span class="nav-number">1.1.</span> <span class="nav-text">平衡杆游戏</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gym-平台"><span class="nav-number">1.2.</span> <span class="nav-text">Gym 平台</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#策略网络"><span class="nav-number">1.3.</span> <span class="nav-text">策略网络</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#策略梯度方法（Policy-Gradient-）"><span class="nav-number">2.</span> <span class="nav-text">策略梯度方法（Policy Gradient ）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#PPO-算法"><span class="nav-number">2.1.</span> <span class="nav-text">PPO 算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#值函数方法"><span class="nav-number">3.</span> <span class="nav-text">值函数方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#值函数"><span class="nav-number">3.1.</span> <span class="nav-text">值函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#值函数的估计"><span class="nav-number">3.2.</span> <span class="nav-text">值函数的估计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#策略改进"><span class="nav-number">3.3.</span> <span class="nav-text">策略改进</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DQN-算法"><span class="nav-number">3.4.</span> <span class="nav-text">DQN 算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DQN-实战"><span class="nav-number">3.5.</span> <span class="nav-text">DQN 实战</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Actor-Critic-方法"><span class="nav-number">4.</span> <span class="nav-text">Actor-Critic 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Advantage-AC-算法"><span class="nav-number">4.1.</span> <span class="nav-text">Advantage AC 算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A3C-算法"><span class="nav-number">4.2.</span> <span class="nav-text">A3C 算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A3C-实战"><span class="nav-number">4.3.</span> <span class="nav-text">A3C 实战</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">ccclll777</p>
  <div class="site-description" itemprop="description">胸怀猛虎 细嗅蔷薇</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ccclll777" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ccclll777" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:sdu945860882@gmail.com" title="E-Mail → mailto:sdu945860882@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.weibo.com/6732062654" title="Weibo → https:&#x2F;&#x2F;www.weibo.com&#x2F;6732062654" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/baidu_41871794" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;baidu_41871794" rel="noopener" target="_blank"><i class="gratipay fa-fw"></i>CSDN</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ccclll777</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">431k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:32</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>



        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  















  

  

  

</body>
</html>
