<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Tensorflow与强化学习 | ccclll777's blogs</title><meta name="keywords" content="深度学习框架,python,tensorflow"><meta name="author" content="ccclll777"><meta name="copyright" content="ccclll777"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="使用tensorflow框架实现强化学习算法，其中包括Policy Gradient ，A3C，DQN等算法">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow与强化学习">
<meta property="og:url" content="http://yoursite.com/2020/12/08/Tensorflow/Tensorflow-and-Reinforcement-learning/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="使用tensorflow框架实现强化学习算法，其中包括Policy Gradient ，A3C，DQN等算法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg">
<meta property="article:published_time" content="2020-12-08T12:46:47.000Z">
<meta property="article:modified_time" content="2021-10-17T01:36:25.744Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="python">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg"><link rel="shortcut icon" href="/images/avatar.png"><link rel="canonical" href="http://yoursite.com/2020/12/08/Tensorflow/Tensorflow-and-Reinforcement-learning/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Tensorflow与强化学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-10-17 09:36:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ccclll777's blogs</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Tensorflow与强化学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-12-08T12:46:47.000Z" title="发表于 2020-12-08 20:46:47">2020-12-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-17T01:36:25.744Z" title="更新于 2021-10-17 09:36:25">2021-10-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>46分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Tensorflow与强化学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>使用tensorflow框架实现强化学习算法，其中包括Policy Gradient ，A3C，DQN等算法</p>
<span id="more"></span>
<h1 id="强化学习算法实例"><a class="markdownIt-Anchor" href="#强化学习算法实例"></a> 强化学习算法实例</h1>
<h2 id="平衡杆游戏"><a class="markdownIt-Anchor" href="#平衡杆游戏"></a> 平衡杆游戏</h2>
<p><img src="https://img-blog.csdnimg.cn/2020120810052028.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<ul>
<li>衡杆游戏系统包含了三个物体:滑轨、小车和杆。如图 ，小车可以自由在<br />
滑轨上移动，杆的一侧通过轴承固定在小车上。在初始状态，小车位于滑轨中央，杆竖直<br />
立在小车上，智能体通过控制小车的左右移动来控制杆的平衡，当杆与竖直方向的角度大<br />
于某个角度或者小车偏离滑轨中心位置一定距离后即视为游戏结束。游戏时间越长，游戏 给予的回报也就越多，智能体的操控水平也越高。</li>
<li>为了简化环境状态的表示，我们这里直接取高层的环境特征向量𝑠作为智能体的输入，它一共包含了四个高层特征，分别为:小车位置、小车速度、杆角度和杆的速度。智能体的输出动作𝑎为向左移动或者向右移动，动作施加在平衡杆系统上会产生一个新的状态， 同时系统也会返回一个奖励值，这个奖励值可以简单的记为 1，即时长加一。在每个时间 戳𝑡上面，智能体通过观察环境状态𝑠𝑡而产生动作𝑎𝑡，环境接收动作后状态改变为𝑠𝑡+1，并返回奖励𝑟 。</li>
</ul>
<h2 id="gym-平台"><a class="markdownIt-Anchor" href="#gym-平台"></a> Gym 平台</h2>
<p>一般来说，在 Gym 环境中创建游戏并进行交互主要包含了 5 个步骤:</p>
<ul>
<li>
<p>创建游戏。通过 gym.make(name)即可创建指定名称 name 的游戏，并返回游戏对象 env。</p>
</li>
<li>
<p>复位游戏状态。一般游戏环境都具有初始状态，通过调用 env.reset()即可复位游戏状 态，同时返回游戏的初始状态 observation。</p>
</li>
<li>
<p>显示游戏画面。通过调用 env.render()即可显示每个时间戳的游戏画面，一般用做测 试。在训练时渲染画面会引入一定的计算代价，因此训练时可不显示画面。</p>
</li>
<li>
<p>与游戏环境交互。通过 env.step(action)即可执行 action 动作，并返回新的状态 observation、当前奖励 reward、游戏是否结束标志 done 以及额外的信息载体 info。通 过循环此步骤即可持续与环境交互，直至游戏回合结束。</p>
</li>
<li>
<p>销毁游戏。调用 env.close()即可。</p>
</li>
<li>
<p>下面演示了一段平衡杆游戏 CartPole-v1 的交互代码，每次交互时在动作空间:{向左，向右}中随机采样一个动作，与环境进行交互，直至游戏结束。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym <span class="comment"># 导入 gym 游戏平台</span></span><br><span class="line">env = gym.make(<span class="string">&quot;CartPole-v1&quot;</span>) <span class="comment"># 创建平衡杆游戏环境</span></span><br><span class="line">observation = env.reset() <span class="comment"># 复位游戏，回到初始状态 </span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>): <span class="comment"># 循环交互 1000 次</span></span><br><span class="line">	env.render() <span class="comment"># 显示当前时间戳的游戏画面</span></span><br><span class="line">	action = env.action_space.sample() <span class="comment"># 随机生成一个动作 </span></span><br><span class="line">	<span class="comment"># 与环境交互，返回新的状态，奖励，是否结束标志，其他信息 </span></span><br><span class="line">	observation, reward, done, info = env.step(action) </span><br><span class="line">	<span class="keyword">if</span> done:<span class="comment">#游戏回合结束，复位状态</span></span><br><span class="line">		observation = env.reset() </span><br><span class="line">env.close() <span class="comment"># 销毁游戏环境</span></span><br></pre></td></tr></table></figure>
<h2 id="策略网络"><a class="markdownIt-Anchor" href="#策略网络"></a> 策略网络</h2>
<p><img src="https://img-blog.csdnimg.cn/20201208111441250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<ul>
<li>将策略网络实现为一个 2 层的全连接网络，第一层将长度为 4 的向量转换为长度 为 128 的向量，第二层将 128 的向量转换为 2 的向量，即动作的概率分布</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policy</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># 策略网络，生成动作的概率分布</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Policy, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment"># 存储轨迹</span></span><br><span class="line">        <span class="comment"># 输入为长度为4的向量，输出为左、右2个动作</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">128</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        <span class="comment"># 网络优化器</span></span><br><span class="line">        self.optimizer = optimizers.Adam(lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 状态输入s的shape为向量：[4]</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = tf.nn.softmax(self.fc2(x), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>在交互时，我们将每个时间戳上的状态输入𝑠t ，动作分布输出𝑎t ，环境奖励𝑟t 和新状态 𝑠𝑡+1作为一个 4 元组 item 记录下来，用于策略网络的训练.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">put_data</span>(<span class="params">self, item</span>):</span></span><br><span class="line">    <span class="comment"># 记录r,log_P(a|s)z</span></span><br><span class="line">    self.data.append(item)</span><br></pre></td></tr></table></figure>
<ul>
<li>训练以及梯度更新</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_net</span>(<span class="params">self, tape</span>):</span></span><br><span class="line">    <span class="comment"># 计算梯度并更新策略网络参数。tape为梯度记录器</span></span><br><span class="line">    R = <span class="number">0</span> <span class="comment"># 终结状态的初始回报为0</span></span><br><span class="line">    <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::-<span class="number">1</span>]:<span class="comment">#逆序取</span></span><br><span class="line">        R = r + gamma * R <span class="comment"># 计算每个时间戳上的回报</span></span><br><span class="line">        <span class="comment"># 每个时间戳都计算一次梯度</span></span><br><span class="line">        <span class="comment"># grad_R=-log_P*R*grad_theta</span></span><br><span class="line">        loss = -log_prob * R</span><br><span class="line">        <span class="keyword">with</span> tape.stop_recording():</span><br><span class="line">            <span class="comment"># 优化策略网络</span></span><br><span class="line">            grads = tape.gradient(loss, self.trainable_variables)</span><br><span class="line">            <span class="comment"># print(grads)  compute_gradients()返回的值作为输入参数对variable进行更新  防止梯度消失或者梯度爆炸</span></span><br><span class="line">            self.optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.trainable_variables))</span><br><span class="line">    self.data = [] <span class="comment"># 清空轨迹</span></span><br></pre></td></tr></table></figure>
<ul>
<li>训练 400 个回合，在回合的开始，复位游戏状态，通过送入输入状态来采样动作，从而与环境进行交互，并记录每一个时间戳的信息，直至游戏回合结束</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    pi = Policy() <span class="comment"># 创建策略网络</span></span><br><span class="line">    pi(tf.random.normal((<span class="number">4</span>,<span class="number">4</span>)))</span><br><span class="line">    pi.summary()</span><br><span class="line">    score = <span class="number">0.0</span> <span class="comment"># 计分</span></span><br><span class="line">    print_interval = <span class="number">20</span> <span class="comment"># 打印间隔</span></span><br><span class="line">    returns = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">400</span>):</span><br><span class="line">        s = env.reset() <span class="comment"># 回到游戏初始状态，返回s0</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">501</span>): <span class="comment"># CartPole-v1 forced to terminates at 500 step.</span></span><br><span class="line">                <span class="comment"># 送入状态向量，获取策略</span></span><br><span class="line">                s = tf.constant(s,dtype=tf.float32)</span><br><span class="line">                <span class="comment"># s: [4] =&gt; [1,4]  在第0个维度之前添加一个维度</span></span><br><span class="line">                s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">                prob = pi(s) <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">                <span class="comment"># 从类别分布中采样1个动作, shape: [1]</span></span><br><span class="line">                a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">                a = <span class="built_in">int</span>(a) <span class="comment"># Tensor转数字</span></span><br><span class="line">                s_prime, r, done, info = env.step(a)</span><br><span class="line">                <span class="comment"># 记录动作a和动作产生的奖励r</span></span><br><span class="line">                <span class="comment"># prob shape:[1,2] </span></span><br><span class="line">                pi.put_data((r, tf.math.log(prob[<span class="number">0</span>][a])))</span><br><span class="line">                s = s_prime <span class="comment"># 刷新状态</span></span><br><span class="line">                score += r <span class="comment"># 累积奖励</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> n_epi &gt;<span class="number">1000</span>:</span><br><span class="line">                    env.render()</span><br><span class="line">                    <span class="comment"># im = Image.fromarray(s)</span></span><br><span class="line">                    <span class="comment"># im.save(&quot;res/%d.jpg&quot; % info[&#x27;frames&#x27;][0])</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> done:  <span class="comment"># 当前episode终止</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># episode终止后，训练一次网络</span></span><br><span class="line">            pi.train_net(tape)</span><br><span class="line">        <span class="keyword">del</span> tape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n_epi%print_interval==<span class="number">0</span> <span class="keyword">and</span> n_epi!=<span class="number">0</span>:</span><br><span class="line">            returns.append(score/print_interval)</span><br><span class="line">            print(<span class="string">f&quot;# of episode :<span class="subst">&#123;n_epi&#125;</span>, avg score : <span class="subst">&#123;score/print_interval&#125;</span>&quot;</span>)</span><br><span class="line">            score = <span class="number">0.0</span></span><br><span class="line">    env.close() <span class="comment"># 关闭环境</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*print_interval, returns)</span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*print_interval, returns, <span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;回合数&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;总回报&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;reinforce-tf-cartpole.svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<ul>
<li>完整代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> 	gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span> 	matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Default parameters for plots</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.titlesize&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;KaiTi&#x27;</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> 	tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"><span class="keyword">from</span>    PIL <span class="keyword">import</span> Image</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># 创建游戏环境</span></span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">2333</span>)</span><br><span class="line">np.random.seed(<span class="number">2333</span>)</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;2&#x27;</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">&#x27;2.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma         = <span class="number">0.98</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policy</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># 策略网络，生成动作的概率分布</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Policy, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment"># 存储轨迹</span></span><br><span class="line">        <span class="comment"># 输入为长度为4的向量，输出为左、右2个动作</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">128</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        <span class="comment"># 网络优化器</span></span><br><span class="line">        self.optimizer = optimizers.Adam(lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 状态输入s的shape为向量：[4]</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = tf.nn.softmax(self.fc2(x), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put_data</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        <span class="comment"># 记录r,log_P(a|s)z</span></span><br><span class="line">        self.data.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_net</span>(<span class="params">self, tape</span>):</span></span><br><span class="line">        <span class="comment"># 计算梯度并更新策略网络参数。tape为梯度记录器</span></span><br><span class="line">        R = <span class="number">0</span> <span class="comment"># 终结状态的初始回报为0</span></span><br><span class="line">        <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::-<span class="number">1</span>]:<span class="comment">#逆序取</span></span><br><span class="line">            R = r + gamma * R <span class="comment"># 计算每个时间戳上的回报</span></span><br><span class="line">            <span class="comment"># 每个时间戳都计算一次梯度</span></span><br><span class="line">            <span class="comment"># grad_R=-log_P*R*grad_theta</span></span><br><span class="line">            loss = -log_prob * R</span><br><span class="line">            <span class="keyword">with</span> tape.stop_recording():</span><br><span class="line">                <span class="comment"># 优化策略网络</span></span><br><span class="line">                grads = tape.gradient(loss, self.trainable_variables)</span><br><span class="line">                <span class="comment"># print(grads)  compute_gradients()返回的值作为输入参数对variable进行更新  防止梯度消失或者梯度爆炸</span></span><br><span class="line">                self.optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.trainable_variables))</span><br><span class="line">        self.data = [] <span class="comment"># 清空轨迹</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    pi = Policy() <span class="comment"># 创建策略网络</span></span><br><span class="line">    pi(tf.random.normal((<span class="number">4</span>,<span class="number">4</span>)))</span><br><span class="line">    pi.summary()</span><br><span class="line">    score = <span class="number">0.0</span> <span class="comment"># 计分</span></span><br><span class="line">    print_interval = <span class="number">20</span> <span class="comment"># 打印间隔</span></span><br><span class="line">    returns = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">400</span>):</span><br><span class="line">        s = env.reset() <span class="comment"># 回到游戏初始状态，返回s0</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">501</span>): <span class="comment"># CartPole-v1 forced to terminates at 500 step.</span></span><br><span class="line">                <span class="comment"># 送入状态向量，获取策略</span></span><br><span class="line">                s = tf.constant(s,dtype=tf.float32)</span><br><span class="line">                <span class="comment"># s: [4] =&gt; [1,4]  在第0个维度之前添加一个维度</span></span><br><span class="line">                s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">                prob = pi(s) <span class="comment"># 动作分布:[1,2]</span></span><br><span class="line">                <span class="comment"># 从类别分布中采样1个动作, shape: [1]</span></span><br><span class="line">                a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">                a = <span class="built_in">int</span>(a) <span class="comment"># Tensor转数字</span></span><br><span class="line">                s_prime, r, done, info = env.step(a)</span><br><span class="line">                <span class="comment"># 记录动作a和动作产生的奖励r</span></span><br><span class="line">                <span class="comment"># prob shape:[1,2]</span></span><br><span class="line">                pi.put_data((r, tf.math.log(prob[<span class="number">0</span>][a])))</span><br><span class="line">                s = s_prime <span class="comment"># 刷新状态</span></span><br><span class="line">                score += r <span class="comment"># 累积奖励</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> n_epi &gt;<span class="number">1000</span>:</span><br><span class="line">                    env.render()</span><br><span class="line">                    <span class="comment"># im = Image.fromarray(s)</span></span><br><span class="line">                    <span class="comment"># im.save(&quot;res/%d.jpg&quot; % info[&#x27;frames&#x27;][0])</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> done:  <span class="comment"># 当前episode终止</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># episode终止后，训练一次网络</span></span><br><span class="line">            pi.train_net(tape)</span><br><span class="line">        <span class="keyword">del</span> tape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n_epi%print_interval==<span class="number">0</span> <span class="keyword">and</span> n_epi!=<span class="number">0</span>:</span><br><span class="line">            returns.append(score/print_interval)</span><br><span class="line">            print(<span class="string">f&quot;# of episode :<span class="subst">&#123;n_epi&#125;</span>, avg score : <span class="subst">&#123;score/print_interval&#125;</span>&quot;</span>)</span><br><span class="line">            score = <span class="number">0.0</span></span><br><span class="line">    env.close() <span class="comment"># 关闭环境</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*print_interval, returns)</span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*print_interval, returns, <span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;回合数&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;总回报&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;reinforce-tf-cartpole.svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h1 id="策略梯度方法policy-gradient"><a class="markdownIt-Anchor" href="#策略梯度方法policy-gradient"></a> 策略梯度方法（Policy Gradient ）</h1>
<h2 id="ppo-算法"><a class="markdownIt-Anchor" href="#ppo-算法"></a> PPO 算法</h2>
<p><img src="https://img-blog.csdnimg.cn/20201208142626810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<ul>
<li><strong>策略网络</strong>：Actor 网络，策略网络的输入为状态𝑠𝑡，4 个输入节点，输出为动作𝑎𝑡 的概率分布𝜋𝜃(𝑎𝑡|𝑠𝑡)，采用 2 层的全连接层网络实现<br />
。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># 策略网络，也叫Actor网络，输出为概率分布pi(a|s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">	    <span class="comment"># 策略网络前向传播</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># 输出2个动作的概率分布</span></span><br><span class="line">        x = tf.nn.softmax(x, axis=<span class="number">1</span>) <span class="comment"># 转换成概率</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>基准线𝑏值网络</strong>： Critic 网络，或 V 值函数网络。网络的输入为状态𝑠𝑡，4 个输入 节点，输出为标量值𝑏，采用 2 层全连接层来估计𝑏。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        <span class="comment"># 偏置b的估值网络，也叫Critic网络，输出为v(s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">1</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)<span class="comment">#输出基准线b的估计</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>策略网络、值函数网络的创建工作，同时分别创建两个优化器，用于优化 策略网络和值函数网络的参数，我们创建在 PPO 算法主体类的初始化方法中</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PPO</span>():</span></span><br><span class="line">    <span class="comment"># PPO算法主体</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PPO, self).__init__()</span><br><span class="line">        self.actor = Actor() <span class="comment"># 创建Actor网络</span></span><br><span class="line">        self.critic = Critic() <span class="comment"># 创建Critic网络</span></span><br><span class="line">        self.buffer = [] <span class="comment"># 数据缓冲池</span></span><br><span class="line">        self.actor_optimizer = optimizers.Adam(<span class="number">1e-3</span>) <span class="comment"># Actor优化器</span></span><br><span class="line">        self.critic_optimizer = optimizers.Adam(<span class="number">3e-3</span>) <span class="comment"># Critic优化器</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>动作采样</strong> 通过select_action 函数可以计算出当前状态的动作分布𝜋𝜃(𝑎𝑡|𝑠𝑡)，并根据概率随机采样动作，返回动作及其概率</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, s</span>):</span></span><br><span class="line">    <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">    s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># s: [4] =&gt; [1,4]   在第0个纬度之前插入一个纬度</span></span><br><span class="line">    s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 获取策略分布: [1, 2]</span></span><br><span class="line">    prob = self.actor(s)</span><br><span class="line">    <span class="comment"># 从类别分布中采样1个动作, shape: [1]   tf.random.categorical 返回的是下标的列表</span></span><br><span class="line">    a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    a = <span class="built_in">int</span>(a)  <span class="comment"># Tensor转数字</span></span><br><span class="line">    <span class="keyword">return</span> a, <span class="built_in">float</span>(prob[<span class="number">0</span>][a]) <span class="comment"># 返回动作及其概率</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>环境交互</strong> 在主函数 main 中，与环境交互 500 个回合，每个回合通过 select_action 函 数采样策略，并保存进缓冲池，在间隔一段时间调用 agent.optimizer()函数优化策略。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    agent = PPO()</span><br><span class="line">    returns = [] <span class="comment"># 统计总回报</span></span><br><span class="line">    total = <span class="number">0</span> <span class="comment"># 一段时间内平均回报</span></span><br><span class="line">    <span class="keyword">for</span> i_epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># 训练回合数</span></span><br><span class="line">        state = env.reset() <span class="comment"># 复位环境</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># 最多考虑500步</span></span><br><span class="line">            <span class="comment"># 通过最新策略与环境交互</span></span><br><span class="line">            action, action_prob = agent.select_action(state)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            <span class="comment"># 构建样本并存储  &#x27;state&#x27;, &#x27;action&#x27;, &#x27;a_log_prob 动作出现的概率&#x27;, &#x27;reward&#x27;, &#x27;next_state&#x27;</span></span><br><span class="line">            trans = Transition(state, action, action_prob, reward, next_state)</span><br><span class="line">            <span class="comment">#存储状态</span></span><br><span class="line">            agent.store_transition(trans)</span><br><span class="line">            state = next_state <span class="comment"># 刷新状态</span></span><br><span class="line">            total += reward <span class="comment"># 累积激励</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done: <span class="comment"># 合适的时间点训练网络</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(agent.buffer) &gt;= batch_size:</span><br><span class="line">                    <span class="comment"># 交互一定轮次之后进行网络的训练</span></span><br><span class="line">                    agent.optimize() <span class="comment"># 训练网络</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i_epoch % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># 每20个回合统计一次平均回报</span></span><br><span class="line">            returns.append(total/<span class="number">20</span>)</span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line">            print(i_epoch, returns[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>网络优化</strong> 当缓冲池达到一定容量后，通过 optimizer()构建策略网络的误差和值网络的误差，优化网络的参数。首先将数据根据类别转换为 Tensor 类型，然后通过 MC 方法计算 累积回报𝑅(𝜏𝑡:𝑇 )。</li>
</ul>
<blockquote>
<p>MC：蒙特卡罗法，蒙特卡罗法是一种不基于模型的强化问题求解方法。它可以 避免动态规划求解过于复杂，同时还可以不事先知道环境转化模 型，因此可以用于海量数据和复杂模型。但是它也有自己的缺点， 这就是它每次采样都需要一个完整的状态序列</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="comment"># 优化网络主函数</span></span><br><span class="line">    <span class="comment"># 从缓存中取出样本数据，转换成Tensor</span></span><br><span class="line">    <span class="comment">#状态</span></span><br><span class="line">    state = tf.constant([t.state <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    <span class="comment">#动作</span></span><br><span class="line">    action = tf.constant([t.action <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.int32)</span><br><span class="line">    <span class="comment">#转化成列向量</span></span><br><span class="line">    action = tf.reshape(action,[-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment">#奖励</span></span><br><span class="line">    reward = [t.reward <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer]</span><br><span class="line">    <span class="comment">#选择动作的概率</span></span><br><span class="line">    old_action_log_prob = tf.constant([t.a_log_prob <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    old_action_log_prob = tf.reshape(old_action_log_prob, [-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 通过MC方法循环计算R(st)</span></span><br><span class="line">    R = <span class="number">0</span></span><br><span class="line">    <span class="comment">#存放累计回报的张量</span></span><br><span class="line">    Rs = []</span><br><span class="line">    <span class="comment">#从最后一个开始循环</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> reward[::-<span class="number">1</span>]:</span><br><span class="line">        R = r + gamma * R</span><br><span class="line">        Rs.insert(<span class="number">0</span>, R)</span><br><span class="line">    <span class="comment">#构成张量</span></span><br><span class="line">    Rs = tf.constant(Rs, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<ul>
<li>对缓存池中的数据按 Batch Size 取出，迭代训练 10 遍。对于策略网络，根据 PPO2 算法的误差函数计算;对于值网络，通过均方差计算值网络的预测与𝑅(𝜏t，r )之间的距离，使得值网络的估计越来越准确。<br />
<img src="https://img-blog.csdnimg.cn/20201208142526539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
<img src="https://img-blog.csdnimg.cn/20201208142537282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="comment"># 优化网络主函数</span></span><br><span class="line">    <span class="comment"># 从缓存中取出样本数据，转换成Tensor</span></span><br><span class="line">    <span class="comment">#状态</span></span><br><span class="line">    state = tf.constant([t.state <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    <span class="comment">#动作</span></span><br><span class="line">    action = tf.constant([t.action <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.int32)</span><br><span class="line">    <span class="comment">#转化成列向量</span></span><br><span class="line">    action = tf.reshape(action,[-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment">#奖励</span></span><br><span class="line">    reward = [t.reward <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer]</span><br><span class="line">    <span class="comment">#选择动作的概率</span></span><br><span class="line">    old_action_log_prob = tf.constant([t.a_log_prob <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    old_action_log_prob = tf.reshape(old_action_log_prob, [-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 通过MC方法循环计算R(st)</span></span><br><span class="line">    R = <span class="number">0</span></span><br><span class="line">    <span class="comment">#存放累计回报的张量</span></span><br><span class="line">    Rs = []</span><br><span class="line">    <span class="comment">#从最后一个开始循环</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> reward[::-<span class="number">1</span>]:</span><br><span class="line">        R = r + gamma * R</span><br><span class="line">        Rs.insert(<span class="number">0</span>, R)</span><br><span class="line">    <span class="comment">#构成张量</span></span><br><span class="line">    Rs = tf.constant(Rs, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># 对缓冲池数据大致迭代10遍</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">round</span>(<span class="number">10</span>*<span class="built_in">len</span>(self.buffer)/batch_size)):</span><br><span class="line">        <span class="comment"># 随机从缓冲池采样batch size大小样本</span></span><br><span class="line">        index = np.random.choice(np.arange(<span class="built_in">len</span>(self.buffer)), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 构建梯度跟踪环境</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1, tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">            <span class="comment"># 取出R(st)，[b,1]  tf.gather 取出index对应的数据   然后扩展一个维度</span></span><br><span class="line">            v_target = tf.expand_dims(tf.gather(Rs, index, axis=<span class="number">0</span>), axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 计算v(s)预测值，也就是偏置b，我们后面会介绍为什么写成v  计算偏置b</span></span><br><span class="line">            v = self.critic(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">            delta = v_target - v <span class="comment"># 计算优势值</span></span><br><span class="line">            advantage = tf.stop_gradient(delta) <span class="comment"># 断开梯度连接 </span></span><br><span class="line">            <span class="comment"># 由于TF的gather_nd与pytorch的gather功能不一样，需要构造</span></span><br><span class="line">            <span class="comment"># gather_nd需要的坐标参数，indices:[b, 2]</span></span><br><span class="line">            <span class="comment"># pi_a = pi.gather(1, a) # pytorch只需要一行即可实现</span></span><br><span class="line">            a = tf.gather(action, index, axis=<span class="number">0</span>) <span class="comment"># 取出batch的动作at</span></span><br><span class="line">            <span class="comment"># batch的动作分布pi(a|st)  每个动作出现的概率</span></span><br><span class="line">            pi = self.actor(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">            <span class="comment">#创建序列  扩展维度</span></span><br><span class="line">            indices = tf.expand_dims(tf.<span class="built_in">range</span>(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment">#与a进行拼接</span></span><br><span class="line">            indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">            pi_a = tf.gather_nd(pi, indices)  <span class="comment"># 动作的概率值pi(at|st), [b]  #指定每次采样点的多维坐标来实现采样多个点的目的</span></span><br><span class="line">            pi_a = tf.expand_dims(pi_a, axis=<span class="number">1</span>)  <span class="comment"># [b]=&gt; [b,1] </span></span><br><span class="line">            <span class="comment"># 重要性采样  不从原分布𝑝中进行采样，而通过另一个分布𝑞中进 行采样，只需要乘以𝑝(𝜏)/𝑞(𝜏)比率即可</span></span><br><span class="line">            ratio = (pi_a / tf.gather(old_action_log_prob, index, axis=<span class="number">0</span>))</span><br><span class="line">            surr1 = ratio * advantage</span><br><span class="line">            <span class="comment">#实现上下限幅</span></span><br><span class="line">            surr2 = tf.clip_by_value(ratio, <span class="number">1</span> - epsilon, <span class="number">1</span> + epsilon) * advantage</span><br><span class="line">            <span class="comment"># PPO误差函数</span></span><br><span class="line">            policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))</span><br><span class="line">            <span class="comment"># 对于偏置v来说，希望与MC估计的R(st)越接近越好</span></span><br><span class="line">            value_loss = losses.MSE(v_target, v)</span><br><span class="line">        <span class="comment"># 优化策略网络</span></span><br><span class="line">        grads = tape1.gradient(policy_loss, self.actor.trainable_variables)</span><br><span class="line">        self.actor_optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.actor.trainable_variables))</span><br><span class="line">        <span class="comment"># 优化偏置值网络</span></span><br><span class="line">        grads = tape2.gradient(value_loss, self.critic.trainable_variables)</span><br><span class="line">        self.critic_optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.critic.trainable_variables))</span><br><span class="line"></span><br><span class="line">    self.buffer = []  <span class="comment"># 清空已训练数据</span></span><br></pre></td></tr></table></figure>
<ul>
<li>整体代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span> 	matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.titlesize&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;KaiTi&#x27;</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span>  gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"><span class="keyword">from</span>    collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span>    torch.utils.data <span class="keyword">import</span> SubsetRandomSampler,BatchSampler</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># 创建游戏环境</span></span><br><span class="line">env.seed(<span class="number">2222</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">2222</span>)</span><br><span class="line">np.random.seed(<span class="number">2222</span>)</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;2&#x27;</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">&#x27;2.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.98</span> <span class="comment"># 激励衰减因子</span></span><br><span class="line">epsilon = <span class="number">0.2</span> <span class="comment"># PPO误差超参数0.8~1.2</span></span><br><span class="line">batch_size = <span class="number">32</span> <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建游戏环境</span></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>).unwrapped</span><br><span class="line">Transition = namedtuple(<span class="string">&#x27;Transition&#x27;</span>, [<span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;action&#x27;</span>, <span class="string">&#x27;a_log_prob&#x27;</span>, <span class="string">&#x27;reward&#x27;</span>, <span class="string">&#x27;next_state&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># 策略网络，也叫Actor网络，输出为概率分布pi(a|s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = tf.nn.softmax(x, axis=<span class="number">1</span>) <span class="comment"># 转换成概率</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        <span class="comment"># 偏置b的估值网络，也叫Critic网络，输出为v(s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">1</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PPO</span>():</span></span><br><span class="line">    <span class="comment"># PPO算法主体</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PPO, self).__init__()</span><br><span class="line">        self.actor = Actor() <span class="comment"># 创建Actor网络</span></span><br><span class="line">        self.critic = Critic() <span class="comment"># 创建Critic网络</span></span><br><span class="line">        self.buffer = [] <span class="comment"># 数据缓冲池</span></span><br><span class="line">        self.actor_optimizer = optimizers.Adam(<span class="number">1e-3</span>) <span class="comment"># Actor优化器</span></span><br><span class="line">        self.critic_optimizer = optimizers.Adam(<span class="number">3e-3</span>) <span class="comment"># Critic优化器</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, s</span>):</span></span><br><span class="line">        <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">        s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># s: [4] =&gt; [1,4]   在第0个纬度之前插入一个纬度</span></span><br><span class="line">        s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 获取策略分布: [1, 2]</span></span><br><span class="line">        prob = self.actor(s)</span><br><span class="line">        <span class="comment"># 从类别分布中采样1个动作, shape: [1]   tf.random.categorical 返回的是下标的列表</span></span><br><span class="line">        a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        a = <span class="built_in">int</span>(a)  <span class="comment"># Tensor转数字</span></span><br><span class="line">        <span class="keyword">return</span> a, <span class="built_in">float</span>(prob[<span class="number">0</span>][a]) <span class="comment"># 返回动作及其概率</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span>(<span class="params">self, s</span>):</span></span><br><span class="line">        <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">        s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># s: [4] =&gt; [1,4]</span></span><br><span class="line">        s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 获取策略分布: [1, 2]</span></span><br><span class="line">        v = self.critic(s)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(v) <span class="comment"># 返回v(s)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        <span class="comment"># 存储采样数据</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimize</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 优化网络主函数</span></span><br><span class="line">        <span class="comment"># 从缓存中取出样本数据，转换成Tensor</span></span><br><span class="line">        <span class="comment">#状态</span></span><br><span class="line">        state = tf.constant([t.state <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">        <span class="comment">#动作</span></span><br><span class="line">        action = tf.constant([t.action <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.int32)</span><br><span class="line">        <span class="comment">#转化成列向量</span></span><br><span class="line">        action = tf.reshape(action,[-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">        <span class="comment">#奖励</span></span><br><span class="line">        reward = [t.reward <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer]</span><br><span class="line">        <span class="comment">#选择动作的概率</span></span><br><span class="line">        old_action_log_prob = tf.constant([t.a_log_prob <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">        old_action_log_prob = tf.reshape(old_action_log_prob, [-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 通过MC方法循环计算R(st)</span></span><br><span class="line">        R = <span class="number">0</span></span><br><span class="line">        <span class="comment">#存放累计回报的张量</span></span><br><span class="line">        Rs = []</span><br><span class="line">        <span class="comment">#从最后一个开始循环</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> reward[::-<span class="number">1</span>]:</span><br><span class="line">            R = r + gamma * R</span><br><span class="line">            Rs.insert(<span class="number">0</span>, R)</span><br><span class="line">        <span class="comment">#构成张量</span></span><br><span class="line">        Rs = tf.constant(Rs, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># 对缓冲池数据大致迭代10遍</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">round</span>(<span class="number">10</span>*<span class="built_in">len</span>(self.buffer)/batch_size)):</span><br><span class="line">            <span class="comment"># 随机从缓冲池采样batch size大小样本</span></span><br><span class="line">            index = np.random.choice(np.arange(<span class="built_in">len</span>(self.buffer)), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line">            <span class="comment"># 构建梯度跟踪环境</span></span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1, tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">                <span class="comment"># 取出R(st)，[b,1]  tf.gather 取出index对应的数据   然后扩展一个维度</span></span><br><span class="line">                v_target = tf.expand_dims(tf.gather(Rs, index, axis=<span class="number">0</span>), axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># 计算v(s)预测值，也就是偏置b，我们后面会介绍为什么写成v  计算偏置b</span></span><br><span class="line">                v = self.critic(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">                delta = v_target - v <span class="comment"># 计算优势值</span></span><br><span class="line">                advantage = tf.stop_gradient(delta) <span class="comment"># 断开梯度连接 </span></span><br><span class="line">                <span class="comment"># 由于TF的gather_nd与pytorch的gather功能不一样，需要构造</span></span><br><span class="line">                <span class="comment"># gather_nd需要的坐标参数，indices:[b, 2]</span></span><br><span class="line">                <span class="comment"># pi_a = pi.gather(1, a) # pytorch只需要一行即可实现</span></span><br><span class="line">                a = tf.gather(action, index, axis=<span class="number">0</span>) <span class="comment"># 取出batch的动作at</span></span><br><span class="line">                <span class="comment"># batch的动作分布pi(a|st)  每个动作出现的概率</span></span><br><span class="line">                pi = self.actor(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">                <span class="comment">#创建序列  扩展维度</span></span><br><span class="line">                indices = tf.expand_dims(tf.<span class="built_in">range</span>(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment">#与a进行拼接</span></span><br><span class="line">                indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">                pi_a = tf.gather_nd(pi, indices)  <span class="comment"># 动作的概率值pi(at|st), [b]  #指定每次采样点的多维坐标来实现采样多个点的目的</span></span><br><span class="line">                pi_a = tf.expand_dims(pi_a, axis=<span class="number">1</span>)  <span class="comment"># [b]=&gt; [b,1] </span></span><br><span class="line">                <span class="comment"># 重要性采样  不从原分布𝑝中进行采样，而通过另一个分布𝑞中进 行采样，只需要乘以𝑝(𝜏)/𝑞(𝜏)比率即可</span></span><br><span class="line">                ratio = (pi_a / tf.gather(old_action_log_prob, index, axis=<span class="number">0</span>))</span><br><span class="line">                surr1 = ratio * advantage</span><br><span class="line">                <span class="comment">#实现上下限幅</span></span><br><span class="line">                surr2 = tf.clip_by_value(ratio, <span class="number">1</span> - epsilon, <span class="number">1</span> + epsilon) * advantage</span><br><span class="line">                <span class="comment"># PPO误差函数</span></span><br><span class="line">                policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))</span><br><span class="line">                <span class="comment"># 对于偏置v来说，希望与MC估计的R(st)越接近越好</span></span><br><span class="line">                value_loss = losses.MSE(v_target, v)</span><br><span class="line">            <span class="comment"># 优化策略网络</span></span><br><span class="line">            grads = tape1.gradient(policy_loss, self.actor.trainable_variables)</span><br><span class="line">            self.actor_optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.actor.trainable_variables))</span><br><span class="line">            <span class="comment"># 优化偏置值网络</span></span><br><span class="line">            grads = tape2.gradient(value_loss, self.critic.trainable_variables)</span><br><span class="line">            self.critic_optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.critic.trainable_variables))</span><br><span class="line"></span><br><span class="line">        self.buffer = []  <span class="comment"># 清空已训练数据</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    agent = PPO()</span><br><span class="line">    returns = [] <span class="comment"># 统计总回报</span></span><br><span class="line">    total = <span class="number">0</span> <span class="comment"># 一段时间内平均回报</span></span><br><span class="line">    <span class="keyword">for</span> i_epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># 训练回合数</span></span><br><span class="line">        state = env.reset() <span class="comment"># 复位环境</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># 最多考虑500步</span></span><br><span class="line">            <span class="comment"># 通过最新策略与环境交互</span></span><br><span class="line">            action, action_prob = agent.select_action(state)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            <span class="comment"># 构建样本并存储  &#x27;state&#x27;, &#x27;action&#x27;, &#x27;a_log_prob 动作出现的概率&#x27;, &#x27;reward&#x27;, &#x27;next_state&#x27;</span></span><br><span class="line">            trans = Transition(state, action, action_prob, reward, next_state)</span><br><span class="line">            <span class="comment">#存储状态</span></span><br><span class="line">            agent.store_transition(trans)</span><br><span class="line">            state = next_state <span class="comment"># 刷新状态</span></span><br><span class="line">            total += reward <span class="comment"># 累积激励</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done: <span class="comment"># 合适的时间点训练网络</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(agent.buffer) &gt;= batch_size:</span><br><span class="line">                    <span class="comment"># 交互一定轮次之后进行网络的训练</span></span><br><span class="line">                    agent.optimize() <span class="comment"># 训练网络</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i_epoch % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># 每20个回合统计一次平均回报</span></span><br><span class="line">            returns.append(total/<span class="number">20</span>)</span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line">            print(i_epoch, returns[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    print(np.array(returns))</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*<span class="number">20</span>, np.array(returns))</span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*<span class="number">20</span>, np.array(returns), <span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;回合数&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;总回报&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;ppo-tf-cartpole.svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line">    print(<span class="string">&quot;end&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="值函数方法"><a class="markdownIt-Anchor" href="#值函数方法"></a> 值函数方法</h1>
<p>策略梯度方法通过直接参数化策略网络，来优化得到更好的策略模型。在强化学习领 域，除了策略方法外，还有另外一类通过建模值函数而间接获得策略的方法，我们把它统 称为值函数方法。</p>
<h2 id="值函数"><a class="markdownIt-Anchor" href="#值函数"></a> 值函数</h2>
<p>状态值函数和状态-动作值函数，两者均表示在策略𝜋下的期望回报，轨迹起点定义不一样。</p>
<ul>
<li><strong>状态值函数(State Value Function，简称 V 函数)</strong>：从状态𝑠𝑡开始，在策略𝜋控 制下能获得的期望回报值:<br />
<img src="https://img-blog.csdnimg.cn/20201208145941156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></li>
<li><strong>状态-动作值函数(State-Action Value Function，简称 Q 函数)</strong>：从状态𝑠𝑡并执行 动作𝑎𝑡的双重设定下，在策略𝜋控制下能获得的期望回报值<br />
<img src="https://img-blog.csdnimg.cn/20201208150024114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></li>
</ul>
<h2 id="值函数的估计"><a class="markdownIt-Anchor" href="#值函数的估计"></a> 值函数的估计</h2>
<ul>
<li><strong>蒙特卡罗方法</strong><br />
<img src="https://img-blog.csdnimg.cn/20201208150610547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></li>
<li><strong>时序差分方法</strong><br />
<img src="https://img-blog.csdnimg.cn/20201208150629891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></li>
</ul>
<h2 id="策略改进"><a class="markdownIt-Anchor" href="#策略改进"></a> 策略改进</h2>
<ul>
<li><strong>ε-贪心法</strong><br />
<img src="https://img-blog.csdnimg.cn/20201208153008119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></li>
</ul>
<h2 id="dqn-算法"><a class="markdownIt-Anchor" href="#dqn-算法"></a> DQN 算法</h2>
<p><img src="https://img-blog.csdnimg.cn/20201208153049582.png" alt="在这里插入图片描述" /><br />
<img src="https://img-blog.csdnimg.cn/20201208153104220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<h2 id="dqn-实战"><a class="markdownIt-Anchor" href="#dqn-实战"></a> DQN 实战</h2>
<ul>
<li><strong>Q 网络</strong>平衡杆游戏的状态是长度为 4 的向量，因此 Q 网络的输入设计为 4 个节点， 经过256 − 256 − 2的全连接层，得到输出节点数为 2 的 Q 函数估值的分布𝑄(𝑠, 𝑎)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 创建Q网络，输入为状态向量，输出为动作的Q值</span></span><br><span class="line">        <span class="built_in">super</span>(Qnet, self).__init__()</span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc3 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(x))</span><br><span class="line">        x = tf.nn.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>经验回放池</strong>在 DQN 算法中使用了经验回放池来减轻数据之间的强相关性，我们利用 ReplayBuffer 类中的 Deque 对象来实现缓存池的功能。在训练时，通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象，并通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime, done_mask = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">            done_mask_lst.append([done_mask])</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(s_lst, dtype=tf.float32),\</span><br><span class="line">                      tf.constant(a_lst, dtype=tf.int32), \</span><br><span class="line">                      tf.constant(r_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(s_prime_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(done_mask_lst, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>策略改进</strong> 这里实现了ε-贪心法。在采样动作时，有1 − ε的概率选择arg max 𝑄𝜋 (𝑠, 𝑎)，有ε的概率随机选择一个动作。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_action</span>(<span class="params">self, s, epsilon</span>):</span></span><br><span class="line">    <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">    s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># s: [4] =&gt; [1,4]</span></span><br><span class="line">    s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">    out = self(s)[<span class="number">0</span>]</span><br><span class="line">    coin = random.random()</span><br><span class="line">    <span class="comment"># 策略改进：e-贪心方式</span></span><br><span class="line">    <span class="keyword">if</span> coin &lt; epsilon:</span><br><span class="line">        <span class="comment"># epsilon大的概率随机选取</span></span><br><span class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 选择Q值最大的动作</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">int</span>(tf.argmax(out))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>网络主流程</strong>网络最多训练 10000 个回合，在回合开始时，首先复位游戏，得到初始状 态𝑠，并从当前 Q 网络中间采样一个动作，与环境进行交互，得到数据对(𝑠, 𝑎, 𝑟, 𝑠′)，并存 入经验回放池。如果当前经验回放池样本数量足够多，则采样一个 Batch 数据，根据 TD 误差优化 Q 网络的估值，直至游戏回合结束。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):  <span class="comment"># 训练次数</span></span><br><span class="line">    <span class="comment"># epsilon概率也会8%到1%衰减，越到后面越使用Q值最大的动作</span></span><br><span class="line">    epsilon = <span class="built_in">max</span>(<span class="number">0.01</span>, <span class="number">0.08</span> - <span class="number">0.01</span> * (n_epi / <span class="number">200</span>))</span><br><span class="line">    s = env.reset()  <span class="comment"># 复位环境</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">600</span>):  <span class="comment"># 一个回合最大时间戳</span></span><br><span class="line">        <span class="comment"># if n_epi&gt;1000:</span></span><br><span class="line">        <span class="comment">#     env.render()</span></span><br><span class="line">        <span class="comment"># 根据当前Q网络提取策略，并改进策略</span></span><br><span class="line">        a = q.sample_action(s, epsilon)</span><br><span class="line">        <span class="comment"># 使用改进的策略与环境交互</span></span><br><span class="line">        s_prime, r, done, info = env.step(a)</span><br><span class="line">        done_mask = <span class="number">0.0</span> <span class="keyword">if</span> done <span class="keyword">else</span> <span class="number">1.0</span>  <span class="comment"># 结束标志掩码</span></span><br><span class="line">        <span class="comment"># 保存5元组</span></span><br><span class="line">        memory.put((s, a, r / <span class="number">100.0</span>, s_prime, done_mask))</span><br><span class="line">        s = s_prime  <span class="comment"># 刷新状态</span></span><br><span class="line">        score += r  <span class="comment"># 记录总回报</span></span><br><span class="line">        <span class="keyword">if</span> done:  <span class="comment"># 回合结束</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> memory.size() &gt; <span class="number">2000</span>:  <span class="comment"># 缓冲池只有大于2000就可以训练</span></span><br><span class="line">        train(q, q_target, memory, optimizer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span> <span class="keyword">and</span> n_epi != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> src, dest <span class="keyword">in</span> <span class="built_in">zip</span>(q.variables, q_target.variables):</span><br><span class="line">            dest.assign(src)  <span class="comment"># 影子网络权值来自Q</span></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/202012081649069.png" alt="在这里插入图片描述" /></p>
<ul>
<li><strong>优化 Q 网络</strong><br />
<img src="https://img-blog.csdnimg.cn/20201208164928927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">q, q_target, memory, optimizer</span>):</span></span><br><span class="line">    <span class="comment"># 通过Q网络和影子网络来构造贝尔曼方程的误差，</span></span><br><span class="line">    <span class="comment"># 并只更新Q网络，影子网络的更新会滞后Q网络</span></span><br><span class="line">    <span class="comment">#Smooth L1 误差可以通过 Huber 误差类实现</span></span><br><span class="line">    huber = losses.Huber()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># 训练10次</span></span><br><span class="line">        <span class="comment"># 从缓冲池采样</span></span><br><span class="line">        s, a, r, s_prime, done_mask = memory.sample(batch_size)</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="comment"># s: [b, 4]</span></span><br><span class="line">            q_out = q(s)  <span class="comment"># 得到Q(s,a)的分布</span></span><br><span class="line">            <span class="comment"># 由于TF的gather_nd与pytorch的gather功能不一样，需要构造</span></span><br><span class="line">            <span class="comment"># gather_nd需要的坐标参数，indices:[b, 2]</span></span><br><span class="line">            <span class="comment"># pi_a = pi.gather(1, a) # pytorch只需要一行即可实现</span></span><br><span class="line">            indices = tf.expand_dims(tf.<span class="built_in">range</span>(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">            indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">            q_a = tf.gather_nd(q_out, indices) <span class="comment"># 动作的概率值, [b]</span></span><br><span class="line">            q_a = tf.expand_dims(q_a, axis=<span class="number">1</span>) <span class="comment"># [b]=&gt; [b,1]</span></span><br><span class="line">            <span class="comment"># 得到Q(s&#x27;,a)的最大值，它来自影子网络！ [b,4]=&gt;[b,2]=&gt;[b,1]</span></span><br><span class="line">            max_q_prime = tf.reduce_max(q_target(s_prime),axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># 构造Q(s,a_t)的目标值，来自贝尔曼方程</span></span><br><span class="line">            target = r + gamma * max_q_prime * done_mask</span><br><span class="line">            <span class="comment"># 计算Q(s,a_t)与目标值的误差</span></span><br><span class="line">            loss = huber(q_a, target)</span><br><span class="line">        <span class="comment"># 更新网络，使得Q(s,a_t)估计符合贝尔曼方程</span></span><br><span class="line">        grads = tape.gradient(loss, q.trainable_variables)</span><br><span class="line">        <span class="comment"># for p in grads:</span></span><br><span class="line">        <span class="comment">#     print(tf.norm(p))</span></span><br><span class="line">        <span class="comment"># print(grads)</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grads, q.trainable_variables))</span><br></pre></td></tr></table></figure>
<ul>
<li>完整代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># 创建游戏环境</span></span><br><span class="line">env.seed(<span class="number">1234</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">1234</span>)</span><br><span class="line">np.random.seed(<span class="number">1234</span>)</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;2&#x27;</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">&#x27;2.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma = <span class="number">0.99</span></span><br><span class="line">buffer_limit = <span class="number">50000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># 经验回放池</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 双向队列</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#通过 put(transition)方法 将最新的(𝑠, 𝑎, 𝑟, 𝑠′)数据存入 Deque 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#通过 sample(n)方法从 Deque 对象中随机采样出 n 个(𝑠, 𝑎, 𝑟, 𝑠′)数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># 从回放池采样n个5元组</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []</span><br><span class="line">        <span class="comment"># 按类别进行整理</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime, done_mask = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">            done_mask_lst.append([done_mask])</span><br><span class="line">        <span class="comment"># 转换成Tensor</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(s_lst, dtype=tf.float32),\</span><br><span class="line">                      tf.constant(a_lst, dtype=tf.int32), \</span><br><span class="line">                      tf.constant(r_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(s_prime_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(done_mask_lst, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 创建Q网络，输入为状态向量，输出为动作的Q值</span></span><br><span class="line">        <span class="built_in">super</span>(Qnet, self).__init__()</span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc3 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(x))</span><br><span class="line">        x = tf.nn.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample_action</span>(<span class="params">self, s, epsilon</span>):</span></span><br><span class="line">        <span class="comment"># 送入状态向量，获取策略: [4]</span></span><br><span class="line">        s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># s: [4] =&gt; [1,4]</span></span><br><span class="line">        s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">        out = self(s)[<span class="number">0</span>]</span><br><span class="line">        coin = random.random()</span><br><span class="line">        <span class="comment"># 策略改进：e-贪心方式</span></span><br><span class="line">        <span class="keyword">if</span> coin &lt; epsilon:</span><br><span class="line">            <span class="comment"># epsilon大的概率随机选取</span></span><br><span class="line">            <span class="keyword">return</span> random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 选择Q值最大的动作</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">int</span>(tf.argmax(out))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">q, q_target, memory, optimizer</span>):</span></span><br><span class="line">    <span class="comment"># 通过Q网络和影子网络来构造贝尔曼方程的误差，</span></span><br><span class="line">    <span class="comment"># 并只更新Q网络，影子网络的更新会滞后Q网络</span></span><br><span class="line">    <span class="comment">#Smooth L1 误差可以通过 Huber 误差类实现</span></span><br><span class="line">    huber = losses.Huber()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># 训练10次</span></span><br><span class="line">        <span class="comment"># 从缓冲池采样</span></span><br><span class="line">        s, a, r, s_prime, done_mask = memory.sample(batch_size)</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="comment"># s: [b, 4]</span></span><br><span class="line">            q_out = q(s)  <span class="comment"># 得到Q(s,a)的分布</span></span><br><span class="line">            <span class="comment"># 由于TF的gather_nd与pytorch的gather功能不一样，需要构造</span></span><br><span class="line">            <span class="comment"># gather_nd需要的坐标参数，indices:[b, 2]</span></span><br><span class="line">            <span class="comment"># pi_a = pi.gather(1, a) # pytorch只需要一行即可实现</span></span><br><span class="line">            indices = tf.expand_dims(tf.<span class="built_in">range</span>(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">            indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">            q_a = tf.gather_nd(q_out, indices) <span class="comment"># 动作的概率值, [b]</span></span><br><span class="line">            q_a = tf.expand_dims(q_a, axis=<span class="number">1</span>) <span class="comment"># [b]=&gt; [b,1]</span></span><br><span class="line">            <span class="comment"># 得到Q(s&#x27;,a)的最大值，它来自影子网络！ [b,4]=&gt;[b,2]=&gt;[b,1]</span></span><br><span class="line">            max_q_prime = tf.reduce_max(q_target(s_prime),axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># 构造Q(s,a_t)的目标值，来自贝尔曼方程</span></span><br><span class="line">            target = r + gamma * max_q_prime * done_mask</span><br><span class="line">            <span class="comment"># 计算Q(s,a_t)与目标值的误差</span></span><br><span class="line">            loss = huber(q_a, target)</span><br><span class="line">        <span class="comment"># 更新网络，使得Q(s,a_t)估计符合贝尔曼方程</span></span><br><span class="line">        grads = tape.gradient(loss, q.trainable_variables)</span><br><span class="line">        <span class="comment"># for p in grads:</span></span><br><span class="line">        <span class="comment">#     print(tf.norm(p))</span></span><br><span class="line">        <span class="comment"># print(grads)</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grads, q.trainable_variables))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># 创建环境</span></span><br><span class="line">    q = Qnet()  <span class="comment"># 创建Q网络</span></span><br><span class="line">    q_target = Qnet()  <span class="comment"># 创建影子网络</span></span><br><span class="line">    q.build(input_shape=(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">    q_target.build(input_shape=(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> src, dest <span class="keyword">in</span> <span class="built_in">zip</span>(q.variables, q_target.variables):</span><br><span class="line">        dest.assign(src) <span class="comment"># 影子网络权值来自Q</span></span><br><span class="line">    memory = ReplayBuffer()  <span class="comment"># 创建回放池</span></span><br><span class="line"></span><br><span class="line">    print_interval = <span class="number">20</span></span><br><span class="line">    score = <span class="number">0.0</span></span><br><span class="line">    optimizer = optimizers.Adam(lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):  <span class="comment"># 训练次数</span></span><br><span class="line">        <span class="comment"># epsilon概率也会8%到1%衰减，越到后面越使用Q值最大的动作</span></span><br><span class="line">        epsilon = <span class="built_in">max</span>(<span class="number">0.01</span>, <span class="number">0.08</span> - <span class="number">0.01</span> * (n_epi / <span class="number">200</span>))</span><br><span class="line">        s = env.reset()  <span class="comment"># 复位环境</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">600</span>):  <span class="comment"># 一个回合最大时间戳</span></span><br><span class="line">            <span class="comment"># if n_epi&gt;1000:</span></span><br><span class="line">            <span class="comment">#     env.render()</span></span><br><span class="line">            <span class="comment"># 根据当前Q网络提取策略，并改进策略</span></span><br><span class="line">            a = q.sample_action(s, epsilon)</span><br><span class="line">            <span class="comment"># 使用改进的策略与环境交互</span></span><br><span class="line">            s_prime, r, done, info = env.step(a)</span><br><span class="line">            done_mask = <span class="number">0.0</span> <span class="keyword">if</span> done <span class="keyword">else</span> <span class="number">1.0</span>  <span class="comment"># 结束标志掩码</span></span><br><span class="line">            <span class="comment"># 保存5元组</span></span><br><span class="line">            memory.put((s, a, r / <span class="number">100.0</span>, s_prime, done_mask))</span><br><span class="line">            s = s_prime  <span class="comment"># 刷新状态</span></span><br><span class="line">            score += r  <span class="comment"># 记录总回报</span></span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># 回合结束</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> memory.size() &gt; <span class="number">2000</span>:  <span class="comment"># 缓冲池只有大于2000就可以训练</span></span><br><span class="line">            train(q, q_target, memory, optimizer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span> <span class="keyword">and</span> n_epi != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> src, dest <span class="keyword">in</span> <span class="built_in">zip</span>(q.variables, q_target.variables):</span><br><span class="line">                dest.assign(src)  <span class="comment"># 影子网络权值来自Q</span></span><br><span class="line">            print(<span class="string">&quot;# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, &quot;</span> \</span><br><span class="line">                  <span class="string">&quot;epsilon : &#123;:.1f&#125;%&quot;</span> \</span><br><span class="line">                  .<span class="built_in">format</span>(n_epi, score / print_interval, memory.size(), epsilon * <span class="number">100</span>))</span><br><span class="line">            score = <span class="number">0.0</span></span><br><span class="line">    env.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h1 id="actor-critic-方法"><a class="markdownIt-Anchor" href="#actor-critic-方法"></a> Actor-Critic 方法</h1>
<p><img src="https://img-blog.csdnimg.cn/20201208193539146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<h2 id="advantage-ac-算法"><a class="markdownIt-Anchor" href="#advantage-ac-算法"></a> Advantage AC 算法</h2>
<p><img src="https://img-blog.csdnimg.cn/20201208193622351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<h2 id="a3c-算法"><a class="markdownIt-Anchor" href="#a3c-算法"></a> A3C 算法</h2>
<p><img src="https://img-blog.csdnimg.cn/20201208193638743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<ul>
<li>A3C 算法全称为 Asynchronous Advantage Actor-Critic 算法，是 DeepMind 基于Advantage Actor-Critic 算法提出来的异步版本 [8]，将 Actor-Critic 网络部署在多个线程中<br />
同时进行训练，并通过全局网络来同步参数。这种异步训练的模式大大提升了训练效率，训练速度更快，并且算法性能也更好。</li>
<li>如图 ，算法会新建一个全局网络 Global Network 和 M 个 Worker 线程， Global Network 包含了Actor 和 Critic 网络，每个线程均新建一个交互环境和 Actor 和 Critic 网络。初始化阶段 Global Network 随机初始化参数𝜃 和𝜙 ，Worker 中的 Actor-Critic 网络从 Global Network中同步拉取参数来初始化网络。在训练时，Worker 中的 Actor-Critic 网络首先从 Global Network拉取最新参数，然后在最新策略𝜋𝜃(𝑎𝑡|𝑠𝑡)才采样动作与私有环 境进行交互，并根据 Advantage Actor-Critic 算法方法计算参数𝜃 和𝜙的梯度信息。完成梯 度计算后，各个 Worker 将梯度信息提交到 Global Network 中，利用 Global Network 的优化 器完成 Global Network的网络参数更新。在算法测试阶段，只使用Global Network 与环境交互即可。</li>
</ul>
<h2 id="a3c-实战"><a class="markdownIt-Anchor" href="#a3c-实战"></a> A3C 实战</h2>
<ul>
<li>异步的 A3C 算法。和普通的 Advantage AC 算法一样，需要创建 ActorCritic 网络大类，它包含了一个 Actor 子网络和一个 Critic 子网络，有时 Actor 和 Critic 会共享前面网络数层，减少网络的参数量。平衡杆游戏比较简单，我们使用一个 2 层 全连接网络来参数化 Actor 网络，使用另一个 2 层全连接网络来参数化 Critic 网络。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorCritic</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># Actor-Critic模型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_size, action_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ActorCritic, self).__init__()</span><br><span class="line">        self.state_size = state_size <span class="comment"># 状态向量长度</span></span><br><span class="line">        self.action_size = action_size <span class="comment"># 动作数量</span></span><br><span class="line">        <span class="comment"># 策略网络Actor</span></span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.policy_logits = layers.Dense(action_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># V网络Critic</span></span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.values = layers.Dense(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>Actor-Critic 的前向传播过程分别计算策略分布𝜋𝜃(𝑎𝑡|𝑠𝑡)和 V 函数估计𝑉𝜋(𝑠𝑡)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">    <span class="comment"># 获得策略分布Pi(a|s)</span></span><br><span class="line">    x = self.dense1(inputs)</span><br><span class="line">    logits = self.policy_logits(x)</span><br><span class="line">    <span class="comment"># 获得v(s)</span></span><br><span class="line">    v = self.dense2(inputs)</span><br><span class="line">    values = self.values(v)</span><br><span class="line">    <span class="keyword">return</span> logits, values</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Worker 线程类</strong> 在 Worker 线程中，实现和 Advantage AC 算法一样的计算流程，只是 计算产生的参数𝜃 和𝜙的梯度信息并不直接用于更新 Worker 的 Actor-Critic 网络，而是提 交到 Global Network 更新。具体地，在 Worker 类初始化阶段，获得 Global Network 传入的 server 对象和 opt 对象，分别代表了 Global Network 模型和优化器;并创建私有的 ActorCritic 网络类 client 和交互环境 env。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span>(<span class="params">threading.Thread</span>):</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,  server, opt, result_queue, idx</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Worker, self).__init__()</span><br><span class="line">        self.result_queue = result_queue <span class="comment"># 共享队列</span></span><br><span class="line">        self.server = server <span class="comment"># 中央模型</span></span><br><span class="line">        self.opt = opt <span class="comment"># 中央优化器</span></span><br><span class="line">        self.client = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># 线程私有网络</span></span><br><span class="line">        self.worker_idx = idx <span class="comment"># 线程id</span></span><br><span class="line">        self.env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>).unwrapped</span><br><span class="line">        self.ep_loss = <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<ul>
<li>在线程运行阶段，每个线程最多与环境交互 400 个回合，在回合开始，利用 client 网 络采样动作与环境进行交互，并保存至 Memory 对象。在回合结束，训练 Actor 网络和 Critic 网络，得到参数𝜃 和𝜙的梯度信息，调用 Global Network 的 opt 优化器对象更新 Global Network。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span> </span><br><span class="line">    mem = Memory() <span class="comment"># 每个worker自己维护一个memory</span></span><br><span class="line">    <span class="keyword">for</span> epi_counter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># 未达到最大回合数</span></span><br><span class="line">        current_state = self.env.reset() <span class="comment"># 复位client游戏状态</span></span><br><span class="line">        mem.clear()</span><br><span class="line">        ep_reward = <span class="number">0.</span></span><br><span class="line">        ep_steps = <span class="number">0</span>  </span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            <span class="comment"># 获得Pi(a|s),未经softmax</span></span><br><span class="line">            logits, _ = self.client(tf.constant(current_state[<span class="literal">None</span>, :],</span><br><span class="line">                                     dtype=tf.float32))</span><br><span class="line">            probs = tf.nn.softmax(logits)</span><br><span class="line">            <span class="comment"># 随机采样动作</span></span><br><span class="line">            action = np.random.choice(<span class="number">2</span>, p=probs.numpy()[<span class="number">0</span>])</span><br><span class="line">            new_state, reward, done, _ = self.env.step(action) <span class="comment"># 交互 </span></span><br><span class="line">            ep_reward += reward <span class="comment"># 累加奖励</span></span><br><span class="line">            mem.store(current_state, action, reward) <span class="comment"># 记录</span></span><br><span class="line">            ep_steps += <span class="number">1</span> <span class="comment"># 计算回合步数</span></span><br><span class="line">            current_state = new_state <span class="comment"># 刷新状态 </span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ep_steps &gt;= <span class="number">500</span> <span class="keyword">or</span> done: <span class="comment"># 最长步数500</span></span><br><span class="line">                <span class="comment"># 计算当前client上的误差</span></span><br><span class="line">                <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                    total_loss = self.compute_loss(done, new_state, mem) </span><br><span class="line">                <span class="comment"># 计算误差</span></span><br><span class="line">                grads = tape.gradient(total_loss, self.client.trainable_weights)</span><br><span class="line">                <span class="comment"># 梯度提交到server，在server上更新梯度</span></span><br><span class="line">                self.opt.apply_gradients(<span class="built_in">zip</span>(grads,</span><br><span class="line">                                             self.server.trainable_weights))</span><br><span class="line">                <span class="comment"># 从server拉取最新的梯度</span></span><br><span class="line">                self.client.set_weights(self.server.get_weights())</span><br><span class="line">                mem.clear() <span class="comment"># 清空Memory </span></span><br><span class="line">                <span class="comment"># 统计此回合回报</span></span><br><span class="line">                self.result_queue.put(ep_reward)</span><br><span class="line">                print(self.worker_idx, ep_reward)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    self.result_queue.put(<span class="literal">None</span>) <span class="comment"># 结束线程</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Actor-Critic 误差计算</strong><br />
<img src="https://img-blog.csdnimg.cn/2020120819533547.png" alt=" " /></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 done,</span></span></span><br><span class="line"><span class="function"><span class="params">                 new_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                 memory,</span></span></span><br><span class="line"><span class="function"><span class="params">                 gamma=<span class="number">0.99</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        reward_sum = <span class="number">0.</span> <span class="comment"># 终止状态的v(终止)=0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        reward_sum = self.client(tf.constant(new_state[<span class="literal">None</span>, :],</span><br><span class="line">                                 dtype=tf.float32))[-<span class="number">1</span>].numpy()[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 统计折扣回报</span></span><br><span class="line">    discounted_rewards = []</span><br><span class="line">    <span class="keyword">for</span> reward <span class="keyword">in</span> memory.rewards[::-<span class="number">1</span>]:  <span class="comment"># reverse buffer r</span></span><br><span class="line">        reward_sum = reward + gamma * reward_sum</span><br><span class="line">        discounted_rewards.append(reward_sum)</span><br><span class="line">    discounted_rewards.reverse()</span><br><span class="line">    <span class="comment"># 获取状态的Pi(a|s)和v(s)</span></span><br><span class="line">    logits, values = self.client(tf.constant(np.vstack(memory.states),</span><br><span class="line">                             dtype=tf.float32))</span><br><span class="line">    <span class="comment"># 计算advantage = R() - v(s)</span></span><br><span class="line">    advantage = tf.constant(np.array(discounted_rewards)[:, <span class="literal">None</span>],</span><br><span class="line">                                     dtype=tf.float32) - values</span><br><span class="line">    <span class="comment"># Critic网络损失</span></span><br><span class="line">    value_loss = advantage ** <span class="number">2</span></span><br><span class="line">    <span class="comment"># 策略损失</span></span><br><span class="line">    policy = tf.nn.softmax(logits)</span><br><span class="line">    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">                    labels=memory.actions, logits=logits)</span><br><span class="line">    <span class="comment"># 计算策略网络损失时，并不会计算V网络</span></span><br><span class="line">    policy_loss = policy_loss * tf.stop_gradient(advantage)</span><br><span class="line">    <span class="comment"># Entropy Bonus  labels标签值（真实值）logits模型的输出</span></span><br><span class="line">    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy,</span><br><span class="line">                                                      logits=logits)</span><br><span class="line">    policy_loss = policy_loss - <span class="number">0.01</span> * entropy</span><br><span class="line">    <span class="comment"># 聚合各个误差</span></span><br><span class="line">    total_loss = tf.reduce_mean((<span class="number">0.5</span> * value_loss + policy_loss))</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>智能体</strong>负责整个 A3C 算法的训练。在智能体类初始化阶段，新建 Global Network 全局网络对象 server 和它的优化器对象 opt。<br />
在训练开始时，创建各个 Worker 线程对象，并启动各个线程对象与环境交互，每个 Worker 对象在交互时均会从 Global Network 中拉取最新的网络参数，并利用最新策略与环 境交互，计算各自损失函数，最后提交梯度信息给 Global Network，调用 opt 对象完成 Global Network 的优化更新。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>:</span></span><br><span class="line">    <span class="comment"># 智能体，包含了中央参数网络server</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># server优化器，client不需要，直接从server拉取参数</span></span><br><span class="line">        self.opt = optimizers.Adam(<span class="number">1e-3</span>)</span><br><span class="line">        <span class="comment"># 中央模型，类似于参数服务器</span></span><br><span class="line">        self.server = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># 状态向量，动作数量</span></span><br><span class="line">        self.server(tf.random.normal((<span class="number">2</span>, <span class="number">4</span>)))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        res_queue = Queue() <span class="comment"># 共享队列</span></span><br><span class="line">        <span class="comment"># 创建各个交互环境</span></span><br><span class="line">        workers = [Worker(self.server, self.opt, res_queue, i)</span><br><span class="line">                   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(multiprocessing.cpu_count())]</span><br><span class="line">        <span class="keyword">for</span> i, worker <span class="keyword">in</span> <span class="built_in">enumerate</span>(workers):</span><br><span class="line">            print(<span class="string">&quot;Starting worker &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">            worker.start()</span><br><span class="line">        <span class="comment"># 统计并绘制总回报曲线</span></span><br><span class="line">        returns = []</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            reward = res_queue.get()</span><br><span class="line">            <span class="keyword">if</span> reward <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                returns.append(reward)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 结束标志</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        [w.join() <span class="keyword">for</span> w <span class="keyword">in</span> workers] <span class="comment"># 等待线程退出 </span></span><br><span class="line"></span><br><span class="line">        print(returns)</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(returns)), returns)</span><br><span class="line">        <span class="comment"># plt.plot(np.arange(len(moving_average_rewards)), np.array(moving_average_rewards), &#x27;s&#x27;)</span></span><br><span class="line">        plt.xlabel(<span class="string">&#x27;回合数&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;总回报&#x27;</span>)</span><br><span class="line">        plt.savefig(<span class="string">&#x27;a3c-tf-cartpole.svg&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>完整代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span>    matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.titlesize&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;KaiTi&#x27;</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span>  threading</span><br><span class="line"><span class="keyword">import</span>  gym</span><br><span class="line"><span class="keyword">import</span>  multiprocessing</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span>    queue <span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">import</span>  matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">1231</span>)</span><br><span class="line">np.random.seed(<span class="number">1231</span>)</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;2&#x27;</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">&#x27;2.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorCritic</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># Actor-Critic模型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_size, action_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ActorCritic, self).__init__()</span><br><span class="line">        self.state_size = state_size <span class="comment"># 状态向量长度</span></span><br><span class="line">        self.action_size = action_size <span class="comment"># 动作数量</span></span><br><span class="line">        <span class="comment"># 策略网络Actor</span></span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.policy_logits = layers.Dense(action_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># V网络Critic</span></span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.values = layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># 获得策略分布Pi(a|s)</span></span><br><span class="line">        x = self.dense1(inputs)</span><br><span class="line">        logits = self.policy_logits(x)</span><br><span class="line">        <span class="comment"># 获得v(s)</span></span><br><span class="line">        v = self.dense2(inputs)</span><br><span class="line">        values = self.values(v)</span><br><span class="line">        <span class="keyword">return</span> logits, values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">record</span>(<span class="params">episode,</span></span></span><br><span class="line"><span class="function"><span class="params">           episode_reward,</span></span></span><br><span class="line"><span class="function"><span class="params">           worker_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">           global_ep_reward,</span></span></span><br><span class="line"><span class="function"><span class="params">           result_queue,</span></span></span><br><span class="line"><span class="function"><span class="params">           total_loss,</span></span></span><br><span class="line"><span class="function"><span class="params">           num_steps</span>):</span></span><br><span class="line">    <span class="comment"># 统计工具函数</span></span><br><span class="line">    <span class="keyword">if</span> global_ep_reward == <span class="number">0</span>:</span><br><span class="line">        global_ep_reward = episode_reward</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        global_ep_reward = global_ep_reward * <span class="number">0.99</span> + episode_reward * <span class="number">0.01</span></span><br><span class="line">    print(</span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;episode&#125;</span> | &quot;</span></span><br><span class="line">        <span class="string">f&quot;Average Reward: <span class="subst">&#123;<span class="built_in">int</span>(global_ep_reward)&#125;</span> | &quot;</span></span><br><span class="line">        <span class="string">f&quot;Episode Reward: <span class="subst">&#123;<span class="built_in">int</span>(episode_reward)&#125;</span> | &quot;</span></span><br><span class="line">        <span class="string">f&quot;Loss: <span class="subst">&#123;<span class="built_in">int</span>(total_loss / <span class="built_in">float</span>(num_steps) * <span class="number">1000</span>) / <span class="number">1000</span>&#125;</span> | &quot;</span></span><br><span class="line">        <span class="string">f&quot;Steps: <span class="subst">&#123;num_steps&#125;</span> | &quot;</span></span><br><span class="line">        <span class="string">f&quot;Worker: <span class="subst">&#123;worker_idx&#125;</span>&quot;</span></span><br><span class="line">    )</span><br><span class="line">    result_queue.put(global_ep_reward) <span class="comment"># 保存回报，传给主线程</span></span><br><span class="line">    <span class="keyword">return</span> global_ep_reward</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Memory</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.states = []</span><br><span class="line">        self.actions = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, state, action, reward</span>):</span></span><br><span class="line">        self.states.append(state)</span><br><span class="line">        self.actions.append(action)</span><br><span class="line">        self.rewards.append(reward)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.states = []</span><br><span class="line">        self.actions = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>:</span></span><br><span class="line">    <span class="comment"># 智能体，包含了中央参数网络server</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># server优化器，client不需要，直接从server拉取参数</span></span><br><span class="line">        self.opt = optimizers.Adam(<span class="number">1e-3</span>)</span><br><span class="line">        <span class="comment"># 中央模型，类似于参数服务器</span></span><br><span class="line">        self.server = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># 状态向量，动作数量</span></span><br><span class="line">        self.server(tf.random.normal((<span class="number">2</span>, <span class="number">4</span>)))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        res_queue = Queue() <span class="comment"># 共享队列</span></span><br><span class="line">        <span class="comment"># 创建各个交互环境</span></span><br><span class="line">        workers = [Worker(self.server, self.opt, res_queue, i)</span><br><span class="line">                   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(multiprocessing.cpu_count())]</span><br><span class="line">        <span class="keyword">for</span> i, worker <span class="keyword">in</span> <span class="built_in">enumerate</span>(workers):</span><br><span class="line">            print(<span class="string">&quot;Starting worker &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">            worker.start()</span><br><span class="line">        <span class="comment"># 统计并绘制总回报曲线</span></span><br><span class="line">        returns = []</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            reward = res_queue.get()</span><br><span class="line">            <span class="keyword">if</span> reward <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                returns.append(reward)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 结束标志</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        [w.join() <span class="keyword">for</span> w <span class="keyword">in</span> workers] <span class="comment"># 等待线程退出 </span></span><br><span class="line"></span><br><span class="line">        print(returns)</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(returns)), returns)</span><br><span class="line">        <span class="comment"># plt.plot(np.arange(len(moving_average_rewards)), np.array(moving_average_rewards), &#x27;s&#x27;)</span></span><br><span class="line">        plt.xlabel(<span class="string">&#x27;回合数&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;总回报&#x27;</span>)</span><br><span class="line">        plt.savefig(<span class="string">&#x27;a3c-tf-cartpole.svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span>(<span class="params">threading.Thread</span>):</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,  server, opt, result_queue, idx</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Worker, self).__init__()</span><br><span class="line">        self.result_queue = result_queue <span class="comment"># 共享队列</span></span><br><span class="line">        self.server = server <span class="comment"># 中央模型</span></span><br><span class="line">        self.opt = opt <span class="comment"># 中央优化器</span></span><br><span class="line">        self.client = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># 线程私有网络</span></span><br><span class="line">        self.worker_idx = idx <span class="comment"># 线程id</span></span><br><span class="line">        self.env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>).unwrapped</span><br><span class="line">        self.ep_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span> </span><br><span class="line">        mem = Memory() <span class="comment"># 每个worker自己维护一个memory</span></span><br><span class="line">        <span class="keyword">for</span> epi_counter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># 未达到最大回合数</span></span><br><span class="line">            current_state = self.env.reset() <span class="comment"># 复位client游戏状态</span></span><br><span class="line">            mem.clear()</span><br><span class="line">            ep_reward = <span class="number">0.</span></span><br><span class="line">            ep_steps = <span class="number">0</span>  </span><br><span class="line">            done = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">                <span class="comment"># 获得Pi(a|s),未经softmax</span></span><br><span class="line">                logits, _ = self.client(tf.constant(current_state[<span class="literal">None</span>, :],</span><br><span class="line">                                         dtype=tf.float32))</span><br><span class="line">                probs = tf.nn.softmax(logits)</span><br><span class="line">                <span class="comment"># 随机采样动作</span></span><br><span class="line">                action = np.random.choice(<span class="number">2</span>, p=probs.numpy()[<span class="number">0</span>])</span><br><span class="line">                new_state, reward, done, _ = self.env.step(action) <span class="comment"># 交互 </span></span><br><span class="line">                ep_reward += reward <span class="comment"># 累加奖励</span></span><br><span class="line">                mem.store(current_state, action, reward) <span class="comment"># 记录</span></span><br><span class="line">                ep_steps += <span class="number">1</span> <span class="comment"># 计算回合步数</span></span><br><span class="line">                current_state = new_state <span class="comment"># 刷新状态 </span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> ep_steps &gt;= <span class="number">500</span> <span class="keyword">or</span> done: <span class="comment"># 最长步数500</span></span><br><span class="line">                    <span class="comment"># 计算当前client上的误差</span></span><br><span class="line">                    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                        total_loss = self.compute_loss(done, new_state, mem) </span><br><span class="line">                    <span class="comment"># 计算误差</span></span><br><span class="line">                    grads = tape.gradient(total_loss, self.client.trainable_weights)</span><br><span class="line">                    <span class="comment"># 梯度提交到server，在server上更新梯度</span></span><br><span class="line">                    self.opt.apply_gradients(<span class="built_in">zip</span>(grads,</span><br><span class="line">                                                 self.server.trainable_weights))</span><br><span class="line">                    <span class="comment"># 从server拉取最新的梯度</span></span><br><span class="line">                    self.client.set_weights(self.server.get_weights())</span><br><span class="line">                    mem.clear() <span class="comment"># 清空Memory </span></span><br><span class="line">                    <span class="comment"># 统计此回合回报</span></span><br><span class="line">                    self.result_queue.put(ep_reward)</span><br><span class="line">                    print(self.worker_idx, ep_reward)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        self.result_queue.put(<span class="literal">None</span>) <span class="comment"># 结束线程</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                     done,</span></span></span><br><span class="line"><span class="function"><span class="params">                     new_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                     memory,</span></span></span><br><span class="line"><span class="function"><span class="params">                     gamma=<span class="number">0.99</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward_sum = <span class="number">0.</span> <span class="comment"># 终止状态的v(终止)=0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reward_sum = self.client(tf.constant(new_state[<span class="literal">None</span>, :],</span><br><span class="line">                                     dtype=tf.float32))[-<span class="number">1</span>].numpy()[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 统计折扣回报</span></span><br><span class="line">        discounted_rewards = []</span><br><span class="line">        <span class="keyword">for</span> reward <span class="keyword">in</span> memory.rewards[::-<span class="number">1</span>]:  <span class="comment"># reverse buffer r</span></span><br><span class="line">            reward_sum = reward + gamma * reward_sum</span><br><span class="line">            discounted_rewards.append(reward_sum)</span><br><span class="line">        discounted_rewards.reverse()</span><br><span class="line">        <span class="comment"># 获取状态的Pi(a|s)和v(s)</span></span><br><span class="line">        logits, values = self.client(tf.constant(np.vstack(memory.states),</span><br><span class="line">                                 dtype=tf.float32))</span><br><span class="line">        <span class="comment"># 计算advantage = R() - v(s)</span></span><br><span class="line">        advantage = tf.constant(np.array(discounted_rewards)[:, <span class="literal">None</span>],</span><br><span class="line">                                         dtype=tf.float32) - values</span><br><span class="line">        <span class="comment"># Critic网络损失</span></span><br><span class="line">        value_loss = advantage ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 策略损失</span></span><br><span class="line">        policy = tf.nn.softmax(logits)</span><br><span class="line">        policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">                        labels=memory.actions, logits=logits)</span><br><span class="line">        <span class="comment"># 计算策略网络损失时，并不会计算V网络</span></span><br><span class="line">        policy_loss = policy_loss * tf.stop_gradient(advantage)</span><br><span class="line">        <span class="comment"># Entropy Bonus  labels标签值（真实值）logits模型的输出</span></span><br><span class="line">        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy,</span><br><span class="line">                                                          logits=logits)</span><br><span class="line">        policy_loss = policy_loss - <span class="number">0.01</span> * entropy</span><br><span class="line">        <span class="comment"># 聚合各个误差</span></span><br><span class="line">        total_loss = tf.reduce_mean((<span class="number">0.5</span> * value_loss + policy_loss))</span><br><span class="line">        <span class="keyword">return</span> total_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    master = Agent()</span><br><span class="line">    master.train()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/tensorflow/">tensorflow</a></div><div class="post_share"><div class="social-share" data-image="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/12/09/Pytorch/Pytorch-Introductory-knowledge/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">PyTorch入门知识</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/07/ReinforcementLearning/Reinforcement-Learning-Basic-Theory/"><img class="next-cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">强化学习基础理论</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/12/06/Tensorflow/Tensorflow-and-Encoder-Decoder/" title="Tensorflow与自编码器"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-06</div><div class="title">Tensorflow与自编码器</div></div></a></div><div><a href="/2020/12/06/Tensorflow/Tensorflow-and-Recurrent-neural-network/" title="Tensorflow与循环神经网络"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-06</div><div class="title">Tensorflow与循环神经网络</div></div></a></div><div><a href="/2020/12/04/Tensorflow/Tensorflow-and-Convolutional-Neural-Network/" title="Tensorflow与卷积神经网络"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-04</div><div class="title">Tensorflow与卷积神经网络</div></div></a></div><div><a href="/2020/12/04/Tensorflow/Tensorflow-Keras-high-level-interface/" title="Tensorflow中Keras 高层接口"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-04</div><div class="title">Tensorflow中Keras 高层接口</div></div></a></div><div><a href="/2020/12/04/Tensorflow/Tensorflow-and-Neural-Networks/" title="Tensorflow构建简单神经网络"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-04</div><div class="title">Tensorflow构建简单神经网络</div></div></a></div><div><a href="/2020/12/03/Tensorflow/Tensorflow-advanced-knowledge/" title="Tensorflow2.0进阶知识"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-03</div><div class="title">Tensorflow2.0进阶知识</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ccclll777</div><div class="author-info__description">胸怀猛虎 细嗅蔷薇</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ccclll777"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccclll777" target="_blank" title="fab fa-github"><i class="GitHub"></i></a><a class="social-icon" href="mailto:sdu945860882@gmail.com" target="_blank" title="fa fa-envelope"><i class="E-Mail"></i></a><a class="social-icon" href="https://www.weibo.com/6732062654" target="_blank" title="fab fa-weibo"><i class="Weibo"></i></a><a class="social-icon" href="https://blog.csdn.net/baidu_41871794" target="_blank" title="gratipay"><i class="CSDN"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E4%BE%8B"><span class="toc-number">1.</span> <span class="toc-text"> 强化学习算法实例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B3%E8%A1%A1%E6%9D%86%E6%B8%B8%E6%88%8F"><span class="toc-number">1.1.</span> <span class="toc-text"> 平衡杆游戏</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gym-%E5%B9%B3%E5%8F%B0"><span class="toc-number">1.2.</span> <span class="toc-text"> Gym 平台</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.</span> <span class="toc-text"> 策略网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95policy-gradient"><span class="toc-number">2.</span> <span class="toc-text"> 策略梯度方法（Policy Gradient ）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#ppo-%E7%AE%97%E6%B3%95"><span class="toc-number">2.1.</span> <span class="toc-text"> PPO 算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text"> 值函数方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text"> 值函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%B0%E8%AE%A1"><span class="toc-number">3.2.</span> <span class="toc-text"> 值函数的估计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B"><span class="toc-number">3.3.</span> <span class="toc-text"> 策略改进</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dqn-%E7%AE%97%E6%B3%95"><span class="toc-number">3.4.</span> <span class="toc-text"> DQN 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dqn-%E5%AE%9E%E6%88%98"><span class="toc-number">3.5.</span> <span class="toc-text"> DQN 实战</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#actor-critic-%E6%96%B9%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text"> Actor-Critic 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#advantage-ac-%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text"> Advantage AC 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#a3c-%E7%AE%97%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text"> A3C 算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#a3c-%E5%AE%9E%E6%88%98"><span class="toc-number">4.3.</span> <span class="toc-text"> A3C 实战</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符"><img src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Leetcode 282. 给表达式添加运算符"/></a><div class="content"><a class="title" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符">Leetcode 282. 给表达式添加运算符</a><time datetime="2021-10-16T15:35:16.000Z" title="发表于 2021-10-16 23:35:16">2021-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现"><img src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch强化学习算法实现"/></a><div class="content"><a class="title" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现">Pytorch强化学习算法实现</a><time datetime="2020-12-12T02:54:37.000Z" title="发表于 2020-12-12 10:54:37">2020-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyTorch常用工具模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块">PyTorch常用工具模块</a><time datetime="2020-12-09T13:32:23.000Z" title="发表于 2020-12-09 21:32:23">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中神经网络工具箱nn模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块">Pytorch中神经网络工具箱nn模块</a><time datetime="2020-12-09T13:25:38.000Z" title="发表于 2020-12-09 21:25:38">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中的Autograd"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd">Pytorch中的Autograd</a><time datetime="2020-12-09T13:25:19.000Z" title="发表于 2020-12-09 21:25:19">2020-12-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By ccclll777</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>