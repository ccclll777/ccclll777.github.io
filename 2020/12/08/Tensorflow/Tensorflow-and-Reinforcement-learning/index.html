<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Tensorflowä¸å¼ºåŒ–å­¦ä¹  | ccclll777's blogs</title><meta name="keywords" content="æ·±åº¦å­¦ä¹ æ¡†æ¶,python,tensorflow"><meta name="author" content="ccclll777"><meta name="copyright" content="ccclll777"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ä½¿ç”¨tensorflowæ¡†æ¶å®ç°å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬Policy Gradient ï¼ŒA3Cï¼ŒDQNç­‰ç®—æ³•">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflowä¸å¼ºåŒ–å­¦ä¹ ">
<meta property="og:url" content="http://yoursite.com/2020/12/08/Tensorflow/Tensorflow-and-Reinforcement-learning/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="ä½¿ç”¨tensorflowæ¡†æ¶å®ç°å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬Policy Gradient ï¼ŒA3Cï¼ŒDQNç­‰ç®—æ³•">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg">
<meta property="article:published_time" content="2020-12-08T12:46:47.000Z">
<meta property="article:modified_time" content="2021-10-17T01:36:25.744Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="æ·±åº¦å­¦ä¹ æ¡†æ¶">
<meta property="article:tag" content="python">
<meta property="article:tag" content="tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg"><link rel="shortcut icon" href="/images/avatar.png"><link rel="canonical" href="http://yoursite.com/2020/12/08/Tensorflow/Tensorflow-and-Reinforcement-learning/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"ç¹","msgToSimplifiedChinese":"ç°¡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'å¤©',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Tensorflowä¸å¼ºåŒ–å­¦ä¹ ',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-10-17 09:36:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">45</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">26</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">9</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ccclll777's blogs</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é¡µ</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ—¶é—´è½´</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ ‡ç­¾</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†ç±»</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> å…³äº</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Tensorflowä¸å¼ºåŒ–å­¦ä¹ </h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2020-12-08T12:46:47.000Z" title="å‘è¡¨äº 2020-12-08 20:46:47">2020-12-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2021-10-17T01:36:25.744Z" title="æ›´æ–°äº 2021-10-17 09:36:25">2021-10-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">å¼ºåŒ–å­¦ä¹ </a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">10.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>46åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Tensorflowä¸å¼ºåŒ–å­¦ä¹ "><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>ä½¿ç”¨tensorflowæ¡†æ¶å®ç°å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬Policy Gradient ï¼ŒA3Cï¼ŒDQNç­‰ç®—æ³•</p>
<span id="more"></span>
<h1 id="å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ä¾‹"><a class="markdownIt-Anchor" href="#å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ä¾‹"></a> å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ä¾‹</h1>
<h2 id="å¹³è¡¡æ†æ¸¸æˆ"><a class="markdownIt-Anchor" href="#å¹³è¡¡æ†æ¸¸æˆ"></a> å¹³è¡¡æ†æ¸¸æˆ</h2>
<p><img src="https://img-blog.csdnimg.cn/2020120810052028.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></p>
<ul>
<li>è¡¡æ†æ¸¸æˆç³»ç»ŸåŒ…å«äº†ä¸‰ä¸ªç‰©ä½“:æ»‘è½¨ã€å°è½¦å’Œæ†ã€‚å¦‚å›¾ ï¼Œå°è½¦å¯ä»¥è‡ªç”±åœ¨<br />
æ»‘è½¨ä¸Šç§»åŠ¨ï¼Œæ†çš„ä¸€ä¾§é€šè¿‡è½´æ‰¿å›ºå®šåœ¨å°è½¦ä¸Šã€‚åœ¨åˆå§‹çŠ¶æ€ï¼Œå°è½¦ä½äºæ»‘è½¨ä¸­å¤®ï¼Œæ†ç«–ç›´<br />
ç«‹åœ¨å°è½¦ä¸Šï¼Œæ™ºèƒ½ä½“é€šè¿‡æ§åˆ¶å°è½¦çš„å·¦å³ç§»åŠ¨æ¥æ§åˆ¶æ†çš„å¹³è¡¡ï¼Œå½“æ†ä¸ç«–ç›´æ–¹å‘çš„è§’åº¦å¤§<br />
äºæŸä¸ªè§’åº¦æˆ–è€…å°è½¦åç¦»æ»‘è½¨ä¸­å¿ƒä½ç½®ä¸€å®šè·ç¦»åå³è§†ä¸ºæ¸¸æˆç»“æŸã€‚æ¸¸æˆæ—¶é—´è¶Šé•¿ï¼Œæ¸¸æˆ ç»™äºˆçš„å›æŠ¥ä¹Ÿå°±è¶Šå¤šï¼Œæ™ºèƒ½ä½“çš„æ“æ§æ°´å¹³ä¹Ÿè¶Šé«˜ã€‚</li>
<li>ä¸ºäº†ç®€åŒ–ç¯å¢ƒçŠ¶æ€çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬è¿™é‡Œç›´æ¥å–é«˜å±‚çš„ç¯å¢ƒç‰¹å¾å‘é‡ğ‘ ä½œä¸ºæ™ºèƒ½ä½“çš„è¾“å…¥ï¼Œå®ƒä¸€å…±åŒ…å«äº†å››ä¸ªé«˜å±‚ç‰¹å¾ï¼Œåˆ†åˆ«ä¸º:å°è½¦ä½ç½®ã€å°è½¦é€Ÿåº¦ã€æ†è§’åº¦å’Œæ†çš„é€Ÿåº¦ã€‚æ™ºèƒ½ä½“çš„è¾“å‡ºåŠ¨ä½œğ‘ä¸ºå‘å·¦ç§»åŠ¨æˆ–è€…å‘å³ç§»åŠ¨ï¼ŒåŠ¨ä½œæ–½åŠ åœ¨å¹³è¡¡æ†ç³»ç»Ÿä¸Šä¼šäº§ç”Ÿä¸€ä¸ªæ–°çš„çŠ¶æ€ï¼Œ åŒæ—¶ç³»ç»Ÿä¹Ÿä¼šè¿”å›ä¸€ä¸ªå¥–åŠ±å€¼ï¼Œè¿™ä¸ªå¥–åŠ±å€¼å¯ä»¥ç®€å•çš„è®°ä¸º 1ï¼Œå³æ—¶é•¿åŠ ä¸€ã€‚åœ¨æ¯ä¸ªæ—¶é—´ æˆ³ğ‘¡ä¸Šé¢ï¼Œæ™ºèƒ½ä½“é€šè¿‡è§‚å¯Ÿç¯å¢ƒçŠ¶æ€ğ‘ ğ‘¡è€Œäº§ç”ŸåŠ¨ä½œğ‘ğ‘¡ï¼Œç¯å¢ƒæ¥æ”¶åŠ¨ä½œåçŠ¶æ€æ”¹å˜ä¸ºğ‘ ğ‘¡+1ï¼Œå¹¶è¿”å›å¥–åŠ±ğ‘Ÿ ã€‚</li>
</ul>
<h2 id="gym-å¹³å°"><a class="markdownIt-Anchor" href="#gym-å¹³å°"></a> Gym å¹³å°</h2>
<p>ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨ Gym ç¯å¢ƒä¸­åˆ›å»ºæ¸¸æˆå¹¶è¿›è¡Œäº¤äº’ä¸»è¦åŒ…å«äº† 5 ä¸ªæ­¥éª¤:</p>
<ul>
<li>
<p>åˆ›å»ºæ¸¸æˆã€‚é€šè¿‡ gym.make(name)å³å¯åˆ›å»ºæŒ‡å®šåç§° name çš„æ¸¸æˆï¼Œå¹¶è¿”å›æ¸¸æˆå¯¹è±¡ envã€‚</p>
</li>
<li>
<p>å¤ä½æ¸¸æˆçŠ¶æ€ã€‚ä¸€èˆ¬æ¸¸æˆç¯å¢ƒéƒ½å…·æœ‰åˆå§‹çŠ¶æ€ï¼Œé€šè¿‡è°ƒç”¨ env.reset()å³å¯å¤ä½æ¸¸æˆçŠ¶ æ€ï¼ŒåŒæ—¶è¿”å›æ¸¸æˆçš„åˆå§‹çŠ¶æ€ observationã€‚</p>
</li>
<li>
<p>æ˜¾ç¤ºæ¸¸æˆç”»é¢ã€‚é€šè¿‡è°ƒç”¨ env.render()å³å¯æ˜¾ç¤ºæ¯ä¸ªæ—¶é—´æˆ³çš„æ¸¸æˆç”»é¢ï¼Œä¸€èˆ¬ç”¨åšæµ‹ è¯•ã€‚åœ¨è®­ç»ƒæ—¶æ¸²æŸ“ç”»é¢ä¼šå¼•å…¥ä¸€å®šçš„è®¡ç®—ä»£ä»·ï¼Œå› æ­¤è®­ç»ƒæ—¶å¯ä¸æ˜¾ç¤ºç”»é¢ã€‚</p>
</li>
<li>
<p>ä¸æ¸¸æˆç¯å¢ƒäº¤äº’ã€‚é€šè¿‡ env.step(action)å³å¯æ‰§è¡Œ action åŠ¨ä½œï¼Œå¹¶è¿”å›æ–°çš„çŠ¶æ€ observationã€å½“å‰å¥–åŠ± rewardã€æ¸¸æˆæ˜¯å¦ç»“æŸæ ‡å¿— done ä»¥åŠé¢å¤–çš„ä¿¡æ¯è½½ä½“ infoã€‚é€š è¿‡å¾ªç¯æ­¤æ­¥éª¤å³å¯æŒç»­ä¸ç¯å¢ƒäº¤äº’ï¼Œç›´è‡³æ¸¸æˆå›åˆç»“æŸã€‚</p>
</li>
<li>
<p>é”€æ¯æ¸¸æˆã€‚è°ƒç”¨ env.close()å³å¯ã€‚</p>
</li>
<li>
<p>ä¸‹é¢æ¼”ç¤ºäº†ä¸€æ®µå¹³è¡¡æ†æ¸¸æˆ CartPole-v1 çš„äº¤äº’ä»£ç ï¼Œæ¯æ¬¡äº¤äº’æ—¶åœ¨åŠ¨ä½œç©ºé—´:{å‘å·¦ï¼Œå‘å³}ä¸­éšæœºé‡‡æ ·ä¸€ä¸ªåŠ¨ä½œï¼Œä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œç›´è‡³æ¸¸æˆç»“æŸã€‚</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym <span class="comment"># å¯¼å…¥ gym æ¸¸æˆå¹³å°</span></span><br><span class="line">env = gym.make(<span class="string">&quot;CartPole-v1&quot;</span>) <span class="comment"># åˆ›å»ºå¹³è¡¡æ†æ¸¸æˆç¯å¢ƒ</span></span><br><span class="line">observation = env.reset() <span class="comment"># å¤ä½æ¸¸æˆï¼Œå›åˆ°åˆå§‹çŠ¶æ€ </span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>): <span class="comment"># å¾ªç¯äº¤äº’ 1000 æ¬¡</span></span><br><span class="line">	env.render() <span class="comment"># æ˜¾ç¤ºå½“å‰æ—¶é—´æˆ³çš„æ¸¸æˆç”»é¢</span></span><br><span class="line">	action = env.action_space.sample() <span class="comment"># éšæœºç”Ÿæˆä¸€ä¸ªåŠ¨ä½œ </span></span><br><span class="line">	<span class="comment"># ä¸ç¯å¢ƒäº¤äº’ï¼Œè¿”å›æ–°çš„çŠ¶æ€ï¼Œå¥–åŠ±ï¼Œæ˜¯å¦ç»“æŸæ ‡å¿—ï¼Œå…¶ä»–ä¿¡æ¯ </span></span><br><span class="line">	observation, reward, done, info = env.step(action) </span><br><span class="line">	<span class="keyword">if</span> done:<span class="comment">#æ¸¸æˆå›åˆç»“æŸï¼Œå¤ä½çŠ¶æ€</span></span><br><span class="line">		observation = env.reset() </span><br><span class="line">env.close() <span class="comment"># é”€æ¯æ¸¸æˆç¯å¢ƒ</span></span><br></pre></td></tr></table></figure>
<h2 id="ç­–ç•¥ç½‘ç»œ"><a class="markdownIt-Anchor" href="#ç­–ç•¥ç½‘ç»œ"></a> ç­–ç•¥ç½‘ç»œ</h2>
<p><img src="https://img-blog.csdnimg.cn/20201208111441250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></p>
<ul>
<li>å°†ç­–ç•¥ç½‘ç»œå®ç°ä¸ºä¸€ä¸ª 2 å±‚çš„å…¨è¿æ¥ç½‘ç»œï¼Œç¬¬ä¸€å±‚å°†é•¿åº¦ä¸º 4 çš„å‘é‡è½¬æ¢ä¸ºé•¿åº¦ ä¸º 128 çš„å‘é‡ï¼Œç¬¬äºŒå±‚å°† 128 çš„å‘é‡è½¬æ¢ä¸º 2 çš„å‘é‡ï¼Œå³åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policy</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># ç­–ç•¥ç½‘ç»œï¼Œç”ŸæˆåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Policy, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment"># å­˜å‚¨è½¨è¿¹</span></span><br><span class="line">        <span class="comment"># è¾“å…¥ä¸ºé•¿åº¦ä¸º4çš„å‘é‡ï¼Œè¾“å‡ºä¸ºå·¦ã€å³2ä¸ªåŠ¨ä½œ</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">128</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        <span class="comment"># ç½‘ç»œä¼˜åŒ–å™¨</span></span><br><span class="line">        self.optimizer = optimizers.Adam(lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># çŠ¶æ€è¾“å…¥sçš„shapeä¸ºå‘é‡ï¼š[4]</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = tf.nn.softmax(self.fc2(x), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>åœ¨äº¤äº’æ—¶ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæ—¶é—´æˆ³ä¸Šçš„çŠ¶æ€è¾“å…¥ğ‘ t ï¼ŒåŠ¨ä½œåˆ†å¸ƒè¾“å‡ºğ‘t ï¼Œç¯å¢ƒå¥–åŠ±ğ‘Ÿt å’Œæ–°çŠ¶æ€ ğ‘ ğ‘¡+1ä½œä¸ºä¸€ä¸ª 4 å…ƒç»„ item è®°å½•ä¸‹æ¥ï¼Œç”¨äºç­–ç•¥ç½‘ç»œçš„è®­ç»ƒ.</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">put_data</span>(<span class="params">self, item</span>):</span></span><br><span class="line">    <span class="comment"># è®°å½•r,log_P(a|s)z</span></span><br><span class="line">    self.data.append(item)</span><br></pre></td></tr></table></figure>
<ul>
<li>è®­ç»ƒä»¥åŠæ¢¯åº¦æ›´æ–°</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_net</span>(<span class="params">self, tape</span>):</span></span><br><span class="line">    <span class="comment"># è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç­–ç•¥ç½‘ç»œå‚æ•°ã€‚tapeä¸ºæ¢¯åº¦è®°å½•å™¨</span></span><br><span class="line">    R = <span class="number">0</span> <span class="comment"># ç»ˆç»“çŠ¶æ€çš„åˆå§‹å›æŠ¥ä¸º0</span></span><br><span class="line">    <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::-<span class="number">1</span>]:<span class="comment">#é€†åºå–</span></span><br><span class="line">        R = r + gamma * R <span class="comment"># è®¡ç®—æ¯ä¸ªæ—¶é—´æˆ³ä¸Šçš„å›æŠ¥</span></span><br><span class="line">        <span class="comment"># æ¯ä¸ªæ—¶é—´æˆ³éƒ½è®¡ç®—ä¸€æ¬¡æ¢¯åº¦</span></span><br><span class="line">        <span class="comment"># grad_R=-log_P*R*grad_theta</span></span><br><span class="line">        loss = -log_prob * R</span><br><span class="line">        <span class="keyword">with</span> tape.stop_recording():</span><br><span class="line">            <span class="comment"># ä¼˜åŒ–ç­–ç•¥ç½‘ç»œ</span></span><br><span class="line">            grads = tape.gradient(loss, self.trainable_variables)</span><br><span class="line">            <span class="comment"># print(grads)  compute_gradients()è¿”å›çš„å€¼ä½œä¸ºè¾“å…¥å‚æ•°å¯¹variableè¿›è¡Œæ›´æ–°  é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±æˆ–è€…æ¢¯åº¦çˆ†ç‚¸</span></span><br><span class="line">            self.optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.trainable_variables))</span><br><span class="line">    self.data = [] <span class="comment"># æ¸…ç©ºè½¨è¿¹</span></span><br></pre></td></tr></table></figure>
<ul>
<li>è®­ç»ƒ 400 ä¸ªå›åˆï¼Œåœ¨å›åˆçš„å¼€å§‹ï¼Œå¤ä½æ¸¸æˆçŠ¶æ€ï¼Œé€šè¿‡é€å…¥è¾“å…¥çŠ¶æ€æ¥é‡‡æ ·åŠ¨ä½œï¼Œä»è€Œä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå¹¶è®°å½•æ¯ä¸€ä¸ªæ—¶é—´æˆ³çš„ä¿¡æ¯ï¼Œç›´è‡³æ¸¸æˆå›åˆç»“æŸ</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    pi = Policy() <span class="comment"># åˆ›å»ºç­–ç•¥ç½‘ç»œ</span></span><br><span class="line">    pi(tf.random.normal((<span class="number">4</span>,<span class="number">4</span>)))</span><br><span class="line">    pi.summary()</span><br><span class="line">    score = <span class="number">0.0</span> <span class="comment"># è®¡åˆ†</span></span><br><span class="line">    print_interval = <span class="number">20</span> <span class="comment"># æ‰“å°é—´éš”</span></span><br><span class="line">    returns = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">400</span>):</span><br><span class="line">        s = env.reset() <span class="comment"># å›åˆ°æ¸¸æˆåˆå§‹çŠ¶æ€ï¼Œè¿”å›s0</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">501</span>): <span class="comment"># CartPole-v1 forced to terminates at 500 step.</span></span><br><span class="line">                <span class="comment"># é€å…¥çŠ¶æ€å‘é‡ï¼Œè·å–ç­–ç•¥</span></span><br><span class="line">                s = tf.constant(s,dtype=tf.float32)</span><br><span class="line">                <span class="comment"># s: [4] =&gt; [1,4]  åœ¨ç¬¬0ä¸ªç»´åº¦ä¹‹å‰æ·»åŠ ä¸€ä¸ªç»´åº¦</span></span><br><span class="line">                s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">                prob = pi(s) <span class="comment"># åŠ¨ä½œåˆ†å¸ƒ:[1,2]</span></span><br><span class="line">                <span class="comment"># ä»ç±»åˆ«åˆ†å¸ƒä¸­é‡‡æ ·1ä¸ªåŠ¨ä½œ, shape: [1]</span></span><br><span class="line">                a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">                a = <span class="built_in">int</span>(a) <span class="comment"># Tensorè½¬æ•°å­—</span></span><br><span class="line">                s_prime, r, done, info = env.step(a)</span><br><span class="line">                <span class="comment"># è®°å½•åŠ¨ä½œaå’ŒåŠ¨ä½œäº§ç”Ÿçš„å¥–åŠ±r</span></span><br><span class="line">                <span class="comment"># prob shape:[1,2] </span></span><br><span class="line">                pi.put_data((r, tf.math.log(prob[<span class="number">0</span>][a])))</span><br><span class="line">                s = s_prime <span class="comment"># åˆ·æ–°çŠ¶æ€</span></span><br><span class="line">                score += r <span class="comment"># ç´¯ç§¯å¥–åŠ±</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> n_epi &gt;<span class="number">1000</span>:</span><br><span class="line">                    env.render()</span><br><span class="line">                    <span class="comment"># im = Image.fromarray(s)</span></span><br><span class="line">                    <span class="comment"># im.save(&quot;res/%d.jpg&quot; % info[&#x27;frames&#x27;][0])</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> done:  <span class="comment"># å½“å‰episodeç»ˆæ­¢</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># episodeç»ˆæ­¢åï¼Œè®­ç»ƒä¸€æ¬¡ç½‘ç»œ</span></span><br><span class="line">            pi.train_net(tape)</span><br><span class="line">        <span class="keyword">del</span> tape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n_epi%print_interval==<span class="number">0</span> <span class="keyword">and</span> n_epi!=<span class="number">0</span>:</span><br><span class="line">            returns.append(score/print_interval)</span><br><span class="line">            print(<span class="string">f&quot;# of episode :<span class="subst">&#123;n_epi&#125;</span>, avg score : <span class="subst">&#123;score/print_interval&#125;</span>&quot;</span>)</span><br><span class="line">            score = <span class="number">0.0</span></span><br><span class="line">    env.close() <span class="comment"># å…³é—­ç¯å¢ƒ</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*print_interval, returns)</span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*print_interval, returns, <span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;å›åˆæ•°&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;æ€»å›æŠ¥&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;reinforce-tf-cartpole.svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<ul>
<li>å®Œæ•´ä»£ç </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> 	gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span> 	matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Default parameters for plots</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.titlesize&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;KaiTi&#x27;</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span> </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> 	tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"><span class="keyword">from</span>    PIL <span class="keyword">import</span> Image</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># åˆ›å»ºæ¸¸æˆç¯å¢ƒ</span></span><br><span class="line">env.seed(<span class="number">2333</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">2333</span>)</span><br><span class="line">np.random.seed(<span class="number">2333</span>)</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;2&#x27;</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">&#x27;2.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma         = <span class="number">0.98</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policy</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># ç­–ç•¥ç½‘ç»œï¼Œç”ŸæˆåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Policy, self).__init__()</span><br><span class="line">        self.data = [] <span class="comment"># å­˜å‚¨è½¨è¿¹</span></span><br><span class="line">        <span class="comment"># è¾“å…¥ä¸ºé•¿åº¦ä¸º4çš„å‘é‡ï¼Œè¾“å‡ºä¸ºå·¦ã€å³2ä¸ªåŠ¨ä½œ</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">128</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        <span class="comment"># ç½‘ç»œä¼˜åŒ–å™¨</span></span><br><span class="line">        self.optimizer = optimizers.Adam(lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># çŠ¶æ€è¾“å…¥sçš„shapeä¸ºå‘é‡ï¼š[4]</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = tf.nn.softmax(self.fc2(x), axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put_data</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        <span class="comment"># è®°å½•r,log_P(a|s)z</span></span><br><span class="line">        self.data.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_net</span>(<span class="params">self, tape</span>):</span></span><br><span class="line">        <span class="comment"># è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç­–ç•¥ç½‘ç»œå‚æ•°ã€‚tapeä¸ºæ¢¯åº¦è®°å½•å™¨</span></span><br><span class="line">        R = <span class="number">0</span> <span class="comment"># ç»ˆç»“çŠ¶æ€çš„åˆå§‹å›æŠ¥ä¸º0</span></span><br><span class="line">        <span class="keyword">for</span> r, log_prob <span class="keyword">in</span> self.data[::-<span class="number">1</span>]:<span class="comment">#é€†åºå–</span></span><br><span class="line">            R = r + gamma * R <span class="comment"># è®¡ç®—æ¯ä¸ªæ—¶é—´æˆ³ä¸Šçš„å›æŠ¥</span></span><br><span class="line">            <span class="comment"># æ¯ä¸ªæ—¶é—´æˆ³éƒ½è®¡ç®—ä¸€æ¬¡æ¢¯åº¦</span></span><br><span class="line">            <span class="comment"># grad_R=-log_P*R*grad_theta</span></span><br><span class="line">            loss = -log_prob * R</span><br><span class="line">            <span class="keyword">with</span> tape.stop_recording():</span><br><span class="line">                <span class="comment"># ä¼˜åŒ–ç­–ç•¥ç½‘ç»œ</span></span><br><span class="line">                grads = tape.gradient(loss, self.trainable_variables)</span><br><span class="line">                <span class="comment"># print(grads)  compute_gradients()è¿”å›çš„å€¼ä½œä¸ºè¾“å…¥å‚æ•°å¯¹variableè¿›è¡Œæ›´æ–°  é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±æˆ–è€…æ¢¯åº¦çˆ†ç‚¸</span></span><br><span class="line">                self.optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.trainable_variables))</span><br><span class="line">        self.data = [] <span class="comment"># æ¸…ç©ºè½¨è¿¹</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    pi = Policy() <span class="comment"># åˆ›å»ºç­–ç•¥ç½‘ç»œ</span></span><br><span class="line">    pi(tf.random.normal((<span class="number">4</span>,<span class="number">4</span>)))</span><br><span class="line">    pi.summary()</span><br><span class="line">    score = <span class="number">0.0</span> <span class="comment"># è®¡åˆ†</span></span><br><span class="line">    print_interval = <span class="number">20</span> <span class="comment"># æ‰“å°é—´éš”</span></span><br><span class="line">    returns = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">400</span>):</span><br><span class="line">        s = env.reset() <span class="comment"># å›åˆ°æ¸¸æˆåˆå§‹çŠ¶æ€ï¼Œè¿”å›s0</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">501</span>): <span class="comment"># CartPole-v1 forced to terminates at 500 step.</span></span><br><span class="line">                <span class="comment"># é€å…¥çŠ¶æ€å‘é‡ï¼Œè·å–ç­–ç•¥</span></span><br><span class="line">                s = tf.constant(s,dtype=tf.float32)</span><br><span class="line">                <span class="comment"># s: [4] =&gt; [1,4]  åœ¨ç¬¬0ä¸ªç»´åº¦ä¹‹å‰æ·»åŠ ä¸€ä¸ªç»´åº¦</span></span><br><span class="line">                s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">                prob = pi(s) <span class="comment"># åŠ¨ä½œåˆ†å¸ƒ:[1,2]</span></span><br><span class="line">                <span class="comment"># ä»ç±»åˆ«åˆ†å¸ƒä¸­é‡‡æ ·1ä¸ªåŠ¨ä½œ, shape: [1]</span></span><br><span class="line">                a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">                a = <span class="built_in">int</span>(a) <span class="comment"># Tensorè½¬æ•°å­—</span></span><br><span class="line">                s_prime, r, done, info = env.step(a)</span><br><span class="line">                <span class="comment"># è®°å½•åŠ¨ä½œaå’ŒåŠ¨ä½œäº§ç”Ÿçš„å¥–åŠ±r</span></span><br><span class="line">                <span class="comment"># prob shape:[1,2]</span></span><br><span class="line">                pi.put_data((r, tf.math.log(prob[<span class="number">0</span>][a])))</span><br><span class="line">                s = s_prime <span class="comment"># åˆ·æ–°çŠ¶æ€</span></span><br><span class="line">                score += r <span class="comment"># ç´¯ç§¯å¥–åŠ±</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> n_epi &gt;<span class="number">1000</span>:</span><br><span class="line">                    env.render()</span><br><span class="line">                    <span class="comment"># im = Image.fromarray(s)</span></span><br><span class="line">                    <span class="comment"># im.save(&quot;res/%d.jpg&quot; % info[&#x27;frames&#x27;][0])</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> done:  <span class="comment"># å½“å‰episodeç»ˆæ­¢</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># episodeç»ˆæ­¢åï¼Œè®­ç»ƒä¸€æ¬¡ç½‘ç»œ</span></span><br><span class="line">            pi.train_net(tape)</span><br><span class="line">        <span class="keyword">del</span> tape</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n_epi%print_interval==<span class="number">0</span> <span class="keyword">and</span> n_epi!=<span class="number">0</span>:</span><br><span class="line">            returns.append(score/print_interval)</span><br><span class="line">            print(<span class="string">f&quot;# of episode :<span class="subst">&#123;n_epi&#125;</span>, avg score : <span class="subst">&#123;score/print_interval&#125;</span>&quot;</span>)</span><br><span class="line">            score = <span class="number">0.0</span></span><br><span class="line">    env.close() <span class="comment"># å…³é—­ç¯å¢ƒ</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*print_interval, returns)</span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*print_interval, returns, <span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;å›åˆæ•°&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;æ€»å›æŠ¥&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;reinforce-tf-cartpole.svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h1 id="ç­–ç•¥æ¢¯åº¦æ–¹æ³•policy-gradient"><a class="markdownIt-Anchor" href="#ç­–ç•¥æ¢¯åº¦æ–¹æ³•policy-gradient"></a> ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆPolicy Gradient ï¼‰</h1>
<h2 id="ppo-ç®—æ³•"><a class="markdownIt-Anchor" href="#ppo-ç®—æ³•"></a> PPO ç®—æ³•</h2>
<p><img src="https://img-blog.csdnimg.cn/20201208142626810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></p>
<ul>
<li><strong>ç­–ç•¥ç½‘ç»œ</strong>ï¼šActor ç½‘ç»œï¼Œç­–ç•¥ç½‘ç»œçš„è¾“å…¥ä¸ºçŠ¶æ€ğ‘ ğ‘¡ï¼Œ4 ä¸ªè¾“å…¥èŠ‚ç‚¹ï¼Œè¾“å‡ºä¸ºåŠ¨ä½œğ‘ğ‘¡ çš„æ¦‚ç‡åˆ†å¸ƒğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡)ï¼Œé‡‡ç”¨ 2 å±‚çš„å…¨è¿æ¥å±‚ç½‘ç»œå®ç°<br />
ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># ç­–ç•¥ç½‘ç»œï¼Œä¹Ÿå«Actorç½‘ç»œï¼Œè¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒpi(a|s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">	    <span class="comment"># ç­–ç•¥ç½‘ç»œå‰å‘ä¼ æ’­</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># è¾“å‡º2ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ</span></span><br><span class="line">        x = tf.nn.softmax(x, axis=<span class="number">1</span>) <span class="comment"># è½¬æ¢æˆæ¦‚ç‡</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>åŸºå‡†çº¿ğ‘å€¼ç½‘ç»œ</strong>ï¼š Critic ç½‘ç»œï¼Œæˆ– V å€¼å‡½æ•°ç½‘ç»œã€‚ç½‘ç»œçš„è¾“å…¥ä¸ºçŠ¶æ€ğ‘ ğ‘¡ï¼Œ4 ä¸ªè¾“å…¥ èŠ‚ç‚¹ï¼Œè¾“å‡ºä¸ºæ ‡é‡å€¼ğ‘ï¼Œé‡‡ç”¨ 2 å±‚å…¨è¿æ¥å±‚æ¥ä¼°è®¡ğ‘ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        <span class="comment"># åç½®bçš„ä¼°å€¼ç½‘ç»œï¼Œä¹Ÿå«Criticç½‘ç»œï¼Œè¾“å‡ºä¸ºv(s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">1</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)<span class="comment">#è¾“å‡ºåŸºå‡†çº¿bçš„ä¼°è®¡</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>ç­–ç•¥ç½‘ç»œã€å€¼å‡½æ•°ç½‘ç»œçš„åˆ›å»ºå·¥ä½œï¼ŒåŒæ—¶åˆ†åˆ«åˆ›å»ºä¸¤ä¸ªä¼˜åŒ–å™¨ï¼Œç”¨äºä¼˜åŒ– ç­–ç•¥ç½‘ç»œå’Œå€¼å‡½æ•°ç½‘ç»œçš„å‚æ•°ï¼Œæˆ‘ä»¬åˆ›å»ºåœ¨ PPO ç®—æ³•ä¸»ä½“ç±»çš„åˆå§‹åŒ–æ–¹æ³•ä¸­</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PPO</span>():</span></span><br><span class="line">    <span class="comment"># PPOç®—æ³•ä¸»ä½“</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PPO, self).__init__()</span><br><span class="line">        self.actor = Actor() <span class="comment"># åˆ›å»ºActorç½‘ç»œ</span></span><br><span class="line">        self.critic = Critic() <span class="comment"># åˆ›å»ºCriticç½‘ç»œ</span></span><br><span class="line">        self.buffer = [] <span class="comment"># æ•°æ®ç¼“å†²æ± </span></span><br><span class="line">        self.actor_optimizer = optimizers.Adam(<span class="number">1e-3</span>) <span class="comment"># Actorä¼˜åŒ–å™¨</span></span><br><span class="line">        self.critic_optimizer = optimizers.Adam(<span class="number">3e-3</span>) <span class="comment"># Criticä¼˜åŒ–å™¨</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>åŠ¨ä½œé‡‡æ ·</strong> é€šè¿‡select_action å‡½æ•°å¯ä»¥è®¡ç®—å‡ºå½“å‰çŠ¶æ€çš„åŠ¨ä½œåˆ†å¸ƒğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡)ï¼Œå¹¶æ ¹æ®æ¦‚ç‡éšæœºé‡‡æ ·åŠ¨ä½œï¼Œè¿”å›åŠ¨ä½œåŠå…¶æ¦‚ç‡</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, s</span>):</span></span><br><span class="line">    <span class="comment"># é€å…¥çŠ¶æ€å‘é‡ï¼Œè·å–ç­–ç•¥: [4]</span></span><br><span class="line">    s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># s: [4] =&gt; [1,4]   åœ¨ç¬¬0ä¸ªçº¬åº¦ä¹‹å‰æ’å…¥ä¸€ä¸ªçº¬åº¦</span></span><br><span class="line">    s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># è·å–ç­–ç•¥åˆ†å¸ƒ: [1, 2]</span></span><br><span class="line">    prob = self.actor(s)</span><br><span class="line">    <span class="comment"># ä»ç±»åˆ«åˆ†å¸ƒä¸­é‡‡æ ·1ä¸ªåŠ¨ä½œ, shape: [1]   tf.random.categorical è¿”å›çš„æ˜¯ä¸‹æ ‡çš„åˆ—è¡¨</span></span><br><span class="line">    a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    a = <span class="built_in">int</span>(a)  <span class="comment"># Tensorè½¬æ•°å­—</span></span><br><span class="line">    <span class="keyword">return</span> a, <span class="built_in">float</span>(prob[<span class="number">0</span>][a]) <span class="comment"># è¿”å›åŠ¨ä½œåŠå…¶æ¦‚ç‡</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ç¯å¢ƒäº¤äº’</strong> åœ¨ä¸»å‡½æ•° main ä¸­ï¼Œä¸ç¯å¢ƒäº¤äº’ 500 ä¸ªå›åˆï¼Œæ¯ä¸ªå›åˆé€šè¿‡ select_action å‡½ æ•°é‡‡æ ·ç­–ç•¥ï¼Œå¹¶ä¿å­˜è¿›ç¼“å†²æ± ï¼Œåœ¨é—´éš”ä¸€æ®µæ—¶é—´è°ƒç”¨ agent.optimizer()å‡½æ•°ä¼˜åŒ–ç­–ç•¥ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    agent = PPO()</span><br><span class="line">    returns = [] <span class="comment"># ç»Ÿè®¡æ€»å›æŠ¥</span></span><br><span class="line">    total = <span class="number">0</span> <span class="comment"># ä¸€æ®µæ—¶é—´å†…å¹³å‡å›æŠ¥</span></span><br><span class="line">    <span class="keyword">for</span> i_epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># è®­ç»ƒå›åˆæ•°</span></span><br><span class="line">        state = env.reset() <span class="comment"># å¤ä½ç¯å¢ƒ</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># æœ€å¤šè€ƒè™‘500æ­¥</span></span><br><span class="line">            <span class="comment"># é€šè¿‡æœ€æ–°ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’</span></span><br><span class="line">            action, action_prob = agent.select_action(state)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            <span class="comment"># æ„å»ºæ ·æœ¬å¹¶å­˜å‚¨  &#x27;state&#x27;, &#x27;action&#x27;, &#x27;a_log_prob åŠ¨ä½œå‡ºç°çš„æ¦‚ç‡&#x27;, &#x27;reward&#x27;, &#x27;next_state&#x27;</span></span><br><span class="line">            trans = Transition(state, action, action_prob, reward, next_state)</span><br><span class="line">            <span class="comment">#å­˜å‚¨çŠ¶æ€</span></span><br><span class="line">            agent.store_transition(trans)</span><br><span class="line">            state = next_state <span class="comment"># åˆ·æ–°çŠ¶æ€</span></span><br><span class="line">            total += reward <span class="comment"># ç´¯ç§¯æ¿€åŠ±</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done: <span class="comment"># åˆé€‚çš„æ—¶é—´ç‚¹è®­ç»ƒç½‘ç»œ</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(agent.buffer) &gt;= batch_size:</span><br><span class="line">                    <span class="comment"># äº¤äº’ä¸€å®šè½®æ¬¡ä¹‹åè¿›è¡Œç½‘ç»œçš„è®­ç»ƒ</span></span><br><span class="line">                    agent.optimize() <span class="comment"># è®­ç»ƒç½‘ç»œ</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i_epoch % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># æ¯20ä¸ªå›åˆç»Ÿè®¡ä¸€æ¬¡å¹³å‡å›æŠ¥</span></span><br><span class="line">            returns.append(total/<span class="number">20</span>)</span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line">            print(i_epoch, returns[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ç½‘ç»œä¼˜åŒ–</strong> å½“ç¼“å†²æ± è¾¾åˆ°ä¸€å®šå®¹é‡åï¼Œé€šè¿‡ optimizer()æ„å»ºç­–ç•¥ç½‘ç»œçš„è¯¯å·®å’Œå€¼ç½‘ç»œçš„è¯¯å·®ï¼Œä¼˜åŒ–ç½‘ç»œçš„å‚æ•°ã€‚é¦–å…ˆå°†æ•°æ®æ ¹æ®ç±»åˆ«è½¬æ¢ä¸º Tensor ç±»å‹ï¼Œç„¶åé€šè¿‡ MC æ–¹æ³•è®¡ç®— ç´¯ç§¯å›æŠ¥ğ‘…(ğœğ‘¡:ğ‘‡ )ã€‚</li>
</ul>
<blockquote>
<p>MCï¼šè’™ç‰¹å¡ç½—æ³•ï¼Œè’™ç‰¹å¡ç½—æ³•æ˜¯ä¸€ç§ä¸åŸºäºæ¨¡å‹çš„å¼ºåŒ–é—®é¢˜æ±‚è§£æ–¹æ³•ã€‚å®ƒå¯ä»¥ é¿å…åŠ¨æ€è§„åˆ’æ±‚è§£è¿‡äºå¤æ‚ï¼ŒåŒæ—¶è¿˜å¯ä»¥ä¸äº‹å…ˆçŸ¥é“ç¯å¢ƒè½¬åŒ–æ¨¡ å‹ï¼Œå› æ­¤å¯ä»¥ç”¨äºæµ·é‡æ•°æ®å’Œå¤æ‚æ¨¡å‹ã€‚ä½†æ˜¯å®ƒä¹Ÿæœ‰è‡ªå·±çš„ç¼ºç‚¹ï¼Œ è¿™å°±æ˜¯å®ƒæ¯æ¬¡é‡‡æ ·éƒ½éœ€è¦ä¸€ä¸ªå®Œæ•´çš„çŠ¶æ€åºåˆ—</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="comment"># ä¼˜åŒ–ç½‘ç»œä¸»å‡½æ•°</span></span><br><span class="line">    <span class="comment"># ä»ç¼“å­˜ä¸­å–å‡ºæ ·æœ¬æ•°æ®ï¼Œè½¬æ¢æˆTensor</span></span><br><span class="line">    <span class="comment">#çŠ¶æ€</span></span><br><span class="line">    state = tf.constant([t.state <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    <span class="comment">#åŠ¨ä½œ</span></span><br><span class="line">    action = tf.constant([t.action <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.int32)</span><br><span class="line">    <span class="comment">#è½¬åŒ–æˆåˆ—å‘é‡</span></span><br><span class="line">    action = tf.reshape(action,[-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment">#å¥–åŠ±</span></span><br><span class="line">    reward = [t.reward <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer]</span><br><span class="line">    <span class="comment">#é€‰æ‹©åŠ¨ä½œçš„æ¦‚ç‡</span></span><br><span class="line">    old_action_log_prob = tf.constant([t.a_log_prob <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    old_action_log_prob = tf.reshape(old_action_log_prob, [-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># é€šè¿‡MCæ–¹æ³•å¾ªç¯è®¡ç®—R(st)</span></span><br><span class="line">    R = <span class="number">0</span></span><br><span class="line">    <span class="comment">#å­˜æ”¾ç´¯è®¡å›æŠ¥çš„å¼ é‡</span></span><br><span class="line">    Rs = []</span><br><span class="line">    <span class="comment">#ä»æœ€åä¸€ä¸ªå¼€å§‹å¾ªç¯</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> reward[::-<span class="number">1</span>]:</span><br><span class="line">        R = r + gamma * R</span><br><span class="line">        Rs.insert(<span class="number">0</span>, R)</span><br><span class="line">    <span class="comment">#æ„æˆå¼ é‡</span></span><br><span class="line">    Rs = tf.constant(Rs, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<ul>
<li>å¯¹ç¼“å­˜æ± ä¸­çš„æ•°æ®æŒ‰ Batch Size å–å‡ºï¼Œè¿­ä»£è®­ç»ƒ 10 éã€‚å¯¹äºç­–ç•¥ç½‘ç»œï¼Œæ ¹æ® PPO2 ç®—æ³•çš„è¯¯å·®å‡½æ•°è®¡ç®—;å¯¹äºå€¼ç½‘ç»œï¼Œé€šè¿‡å‡æ–¹å·®è®¡ç®—å€¼ç½‘ç»œçš„é¢„æµ‹ä¸ğ‘…(ğœtï¼Œr )ä¹‹é—´çš„è·ç¦»ï¼Œä½¿å¾—å€¼ç½‘ç»œçš„ä¼°è®¡è¶Šæ¥è¶Šå‡†ç¡®ã€‚<br />
<img src="https://img-blog.csdnimg.cn/20201208142526539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /><br />
<img src="https://img-blog.csdnimg.cn/20201208142537282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="comment"># ä¼˜åŒ–ç½‘ç»œä¸»å‡½æ•°</span></span><br><span class="line">    <span class="comment"># ä»ç¼“å­˜ä¸­å–å‡ºæ ·æœ¬æ•°æ®ï¼Œè½¬æ¢æˆTensor</span></span><br><span class="line">    <span class="comment">#çŠ¶æ€</span></span><br><span class="line">    state = tf.constant([t.state <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    <span class="comment">#åŠ¨ä½œ</span></span><br><span class="line">    action = tf.constant([t.action <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.int32)</span><br><span class="line">    <span class="comment">#è½¬åŒ–æˆåˆ—å‘é‡</span></span><br><span class="line">    action = tf.reshape(action,[-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment">#å¥–åŠ±</span></span><br><span class="line">    reward = [t.reward <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer]</span><br><span class="line">    <span class="comment">#é€‰æ‹©åŠ¨ä½œçš„æ¦‚ç‡</span></span><br><span class="line">    old_action_log_prob = tf.constant([t.a_log_prob <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">    old_action_log_prob = tf.reshape(old_action_log_prob, [-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># é€šè¿‡MCæ–¹æ³•å¾ªç¯è®¡ç®—R(st)</span></span><br><span class="line">    R = <span class="number">0</span></span><br><span class="line">    <span class="comment">#å­˜æ”¾ç´¯è®¡å›æŠ¥çš„å¼ é‡</span></span><br><span class="line">    Rs = []</span><br><span class="line">    <span class="comment">#ä»æœ€åä¸€ä¸ªå¼€å§‹å¾ªç¯</span></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> reward[::-<span class="number">1</span>]:</span><br><span class="line">        R = r + gamma * R</span><br><span class="line">        Rs.insert(<span class="number">0</span>, R)</span><br><span class="line">    <span class="comment">#æ„æˆå¼ é‡</span></span><br><span class="line">    Rs = tf.constant(Rs, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># å¯¹ç¼“å†²æ± æ•°æ®å¤§è‡´è¿­ä»£10é</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">round</span>(<span class="number">10</span>*<span class="built_in">len</span>(self.buffer)/batch_size)):</span><br><span class="line">        <span class="comment"># éšæœºä»ç¼“å†²æ± é‡‡æ ·batch sizeå¤§å°æ ·æœ¬</span></span><br><span class="line">        index = np.random.choice(np.arange(<span class="built_in">len</span>(self.buffer)), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># æ„å»ºæ¢¯åº¦è·Ÿè¸ªç¯å¢ƒ</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1, tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">            <span class="comment"># å–å‡ºR(st)ï¼Œ[b,1]  tf.gather å–å‡ºindexå¯¹åº”çš„æ•°æ®   ç„¶åæ‰©å±•ä¸€ä¸ªç»´åº¦</span></span><br><span class="line">            v_target = tf.expand_dims(tf.gather(Rs, index, axis=<span class="number">0</span>), axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># è®¡ç®—v(s)é¢„æµ‹å€¼ï¼Œä¹Ÿå°±æ˜¯åç½®bï¼Œæˆ‘ä»¬åé¢ä¼šä»‹ç»ä¸ºä»€ä¹ˆå†™æˆv  è®¡ç®—åç½®b</span></span><br><span class="line">            v = self.critic(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">            delta = v_target - v <span class="comment"># è®¡ç®—ä¼˜åŠ¿å€¼</span></span><br><span class="line">            advantage = tf.stop_gradient(delta) <span class="comment"># æ–­å¼€æ¢¯åº¦è¿æ¥ </span></span><br><span class="line">            <span class="comment"># ç”±äºTFçš„gather_ndä¸pytorchçš„gatheråŠŸèƒ½ä¸ä¸€æ ·ï¼Œéœ€è¦æ„é€ </span></span><br><span class="line">            <span class="comment"># gather_ndéœ€è¦çš„åæ ‡å‚æ•°ï¼Œindices:[b, 2]</span></span><br><span class="line">            <span class="comment"># pi_a = pi.gather(1, a) # pytorchåªéœ€è¦ä¸€è¡Œå³å¯å®ç°</span></span><br><span class="line">            a = tf.gather(action, index, axis=<span class="number">0</span>) <span class="comment"># å–å‡ºbatchçš„åŠ¨ä½œat</span></span><br><span class="line">            <span class="comment"># batchçš„åŠ¨ä½œåˆ†å¸ƒpi(a|st)  æ¯ä¸ªåŠ¨ä½œå‡ºç°çš„æ¦‚ç‡</span></span><br><span class="line">            pi = self.actor(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">            <span class="comment">#åˆ›å»ºåºåˆ—  æ‰©å±•ç»´åº¦</span></span><br><span class="line">            indices = tf.expand_dims(tf.<span class="built_in">range</span>(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">            <span class="comment">#ä¸aè¿›è¡Œæ‹¼æ¥</span></span><br><span class="line">            indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">            pi_a = tf.gather_nd(pi, indices)  <span class="comment"># åŠ¨ä½œçš„æ¦‚ç‡å€¼pi(at|st), [b]  #æŒ‡å®šæ¯æ¬¡é‡‡æ ·ç‚¹çš„å¤šç»´åæ ‡æ¥å®ç°é‡‡æ ·å¤šä¸ªç‚¹çš„ç›®çš„</span></span><br><span class="line">            pi_a = tf.expand_dims(pi_a, axis=<span class="number">1</span>)  <span class="comment"># [b]=&gt; [b,1] </span></span><br><span class="line">            <span class="comment"># é‡è¦æ€§é‡‡æ ·  ä¸ä»åŸåˆ†å¸ƒğ‘ä¸­è¿›è¡Œé‡‡æ ·ï¼Œè€Œé€šè¿‡å¦ä¸€ä¸ªåˆ†å¸ƒğ‘ä¸­è¿› è¡Œé‡‡æ ·ï¼Œåªéœ€è¦ä¹˜ä»¥ğ‘(ğœ)/ğ‘(ğœ)æ¯”ç‡å³å¯</span></span><br><span class="line">            ratio = (pi_a / tf.gather(old_action_log_prob, index, axis=<span class="number">0</span>))</span><br><span class="line">            surr1 = ratio * advantage</span><br><span class="line">            <span class="comment">#å®ç°ä¸Šä¸‹é™å¹…</span></span><br><span class="line">            surr2 = tf.clip_by_value(ratio, <span class="number">1</span> - epsilon, <span class="number">1</span> + epsilon) * advantage</span><br><span class="line">            <span class="comment"># PPOè¯¯å·®å‡½æ•°</span></span><br><span class="line">            policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))</span><br><span class="line">            <span class="comment"># å¯¹äºåç½®væ¥è¯´ï¼Œå¸Œæœ›ä¸MCä¼°è®¡çš„R(st)è¶Šæ¥è¿‘è¶Šå¥½</span></span><br><span class="line">            value_loss = losses.MSE(v_target, v)</span><br><span class="line">        <span class="comment"># ä¼˜åŒ–ç­–ç•¥ç½‘ç»œ</span></span><br><span class="line">        grads = tape1.gradient(policy_loss, self.actor.trainable_variables)</span><br><span class="line">        self.actor_optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.actor.trainable_variables))</span><br><span class="line">        <span class="comment"># ä¼˜åŒ–åç½®å€¼ç½‘ç»œ</span></span><br><span class="line">        grads = tape2.gradient(value_loss, self.critic.trainable_variables)</span><br><span class="line">        self.critic_optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.critic.trainable_variables))</span><br><span class="line"></span><br><span class="line">    self.buffer = []  <span class="comment"># æ¸…ç©ºå·²è®­ç»ƒæ•°æ®</span></span><br></pre></td></tr></table></figure>
<ul>
<li>æ•´ä½“ä»£ç </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span> 	matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.titlesize&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;KaiTi&#x27;</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span>  gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"><span class="keyword">from</span>    collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span>    torch.utils.data <span class="keyword">import</span> SubsetRandomSampler,BatchSampler</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># åˆ›å»ºæ¸¸æˆç¯å¢ƒ</span></span><br><span class="line">env.seed(<span class="number">2222</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">2222</span>)</span><br><span class="line">np.random.seed(<span class="number">2222</span>)</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;2&#x27;</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">&#x27;2.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">0.98</span> <span class="comment"># æ¿€åŠ±è¡°å‡å› å­</span></span><br><span class="line">epsilon = <span class="number">0.2</span> <span class="comment"># PPOè¯¯å·®è¶…å‚æ•°0.8~1.2</span></span><br><span class="line">batch_size = <span class="number">32</span> <span class="comment"># batch size</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»ºæ¸¸æˆç¯å¢ƒ</span></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>).unwrapped</span><br><span class="line">Transition = namedtuple(<span class="string">&#x27;Transition&#x27;</span>, [<span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;action&#x27;</span>, <span class="string">&#x27;a_log_prob&#x27;</span>, <span class="string">&#x27;reward&#x27;</span>, <span class="string">&#x27;next_state&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        <span class="comment"># ç­–ç•¥ç½‘ç»œï¼Œä¹Ÿå«Actorç½‘ç»œï¼Œè¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒpi(a|s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = tf.nn.softmax(x, axis=<span class="number">1</span>) <span class="comment"># è½¬æ¢æˆæ¦‚ç‡</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Critic, self).__init__()</span><br><span class="line">        <span class="comment"># åç½®bçš„ä¼°å€¼ç½‘ç»œï¼Œä¹Ÿå«Criticç½‘ç»œï¼Œè¾“å‡ºä¸ºv(s)</span></span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">100</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">1</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(inputs))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PPO</span>():</span></span><br><span class="line">    <span class="comment"># PPOç®—æ³•ä¸»ä½“</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PPO, self).__init__()</span><br><span class="line">        self.actor = Actor() <span class="comment"># åˆ›å»ºActorç½‘ç»œ</span></span><br><span class="line">        self.critic = Critic() <span class="comment"># åˆ›å»ºCriticç½‘ç»œ</span></span><br><span class="line">        self.buffer = [] <span class="comment"># æ•°æ®ç¼“å†²æ± </span></span><br><span class="line">        self.actor_optimizer = optimizers.Adam(<span class="number">1e-3</span>) <span class="comment"># Actorä¼˜åŒ–å™¨</span></span><br><span class="line">        self.critic_optimizer = optimizers.Adam(<span class="number">3e-3</span>) <span class="comment"># Criticä¼˜åŒ–å™¨</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, s</span>):</span></span><br><span class="line">        <span class="comment"># é€å…¥çŠ¶æ€å‘é‡ï¼Œè·å–ç­–ç•¥: [4]</span></span><br><span class="line">        s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># s: [4] =&gt; [1,4]   åœ¨ç¬¬0ä¸ªçº¬åº¦ä¹‹å‰æ’å…¥ä¸€ä¸ªçº¬åº¦</span></span><br><span class="line">        s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># è·å–ç­–ç•¥åˆ†å¸ƒ: [1, 2]</span></span><br><span class="line">        prob = self.actor(s)</span><br><span class="line">        <span class="comment"># ä»ç±»åˆ«åˆ†å¸ƒä¸­é‡‡æ ·1ä¸ªåŠ¨ä½œ, shape: [1]   tf.random.categorical è¿”å›çš„æ˜¯ä¸‹æ ‡çš„åˆ—è¡¨</span></span><br><span class="line">        a = tf.random.categorical(tf.math.log(prob), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        a = <span class="built_in">int</span>(a)  <span class="comment"># Tensorè½¬æ•°å­—</span></span><br><span class="line">        <span class="keyword">return</span> a, <span class="built_in">float</span>(prob[<span class="number">0</span>][a]) <span class="comment"># è¿”å›åŠ¨ä½œåŠå…¶æ¦‚ç‡</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span>(<span class="params">self, s</span>):</span></span><br><span class="line">        <span class="comment"># é€å…¥çŠ¶æ€å‘é‡ï¼Œè·å–ç­–ç•¥: [4]</span></span><br><span class="line">        s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># s: [4] =&gt; [1,4]</span></span><br><span class="line">        s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># è·å–ç­–ç•¥åˆ†å¸ƒ: [1, 2]</span></span><br><span class="line">        v = self.critic(s)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">float</span>(v) <span class="comment"># è¿”å›v(s)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        <span class="comment"># å­˜å‚¨é‡‡æ ·æ•°æ®</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimize</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># ä¼˜åŒ–ç½‘ç»œä¸»å‡½æ•°</span></span><br><span class="line">        <span class="comment"># ä»ç¼“å­˜ä¸­å–å‡ºæ ·æœ¬æ•°æ®ï¼Œè½¬æ¢æˆTensor</span></span><br><span class="line">        <span class="comment">#çŠ¶æ€</span></span><br><span class="line">        state = tf.constant([t.state <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">        <span class="comment">#åŠ¨ä½œ</span></span><br><span class="line">        action = tf.constant([t.action <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.int32)</span><br><span class="line">        <span class="comment">#è½¬åŒ–æˆåˆ—å‘é‡</span></span><br><span class="line">        action = tf.reshape(action,[-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">        <span class="comment">#å¥–åŠ±</span></span><br><span class="line">        reward = [t.reward <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer]</span><br><span class="line">        <span class="comment">#é€‰æ‹©åŠ¨ä½œçš„æ¦‚ç‡</span></span><br><span class="line">        old_action_log_prob = tf.constant([t.a_log_prob <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=tf.float32)</span><br><span class="line">        old_action_log_prob = tf.reshape(old_action_log_prob, [-<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># é€šè¿‡MCæ–¹æ³•å¾ªç¯è®¡ç®—R(st)</span></span><br><span class="line">        R = <span class="number">0</span></span><br><span class="line">        <span class="comment">#å­˜æ”¾ç´¯è®¡å›æŠ¥çš„å¼ é‡</span></span><br><span class="line">        Rs = []</span><br><span class="line">        <span class="comment">#ä»æœ€åä¸€ä¸ªå¼€å§‹å¾ªç¯</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> reward[::-<span class="number">1</span>]:</span><br><span class="line">            R = r + gamma * R</span><br><span class="line">            Rs.insert(<span class="number">0</span>, R)</span><br><span class="line">        <span class="comment">#æ„æˆå¼ é‡</span></span><br><span class="line">        Rs = tf.constant(Rs, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># å¯¹ç¼“å†²æ± æ•°æ®å¤§è‡´è¿­ä»£10é</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">round</span>(<span class="number">10</span>*<span class="built_in">len</span>(self.buffer)/batch_size)):</span><br><span class="line">            <span class="comment"># éšæœºä»ç¼“å†²æ± é‡‡æ ·batch sizeå¤§å°æ ·æœ¬</span></span><br><span class="line">            index = np.random.choice(np.arange(<span class="built_in">len</span>(self.buffer)), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line">            <span class="comment"># æ„å»ºæ¢¯åº¦è·Ÿè¸ªç¯å¢ƒ</span></span><br><span class="line">            <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape1, tf.GradientTape() <span class="keyword">as</span> tape2:</span><br><span class="line">                <span class="comment"># å–å‡ºR(st)ï¼Œ[b,1]  tf.gather å–å‡ºindexå¯¹åº”çš„æ•°æ®   ç„¶åæ‰©å±•ä¸€ä¸ªç»´åº¦</span></span><br><span class="line">                v_target = tf.expand_dims(tf.gather(Rs, index, axis=<span class="number">0</span>), axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># è®¡ç®—v(s)é¢„æµ‹å€¼ï¼Œä¹Ÿå°±æ˜¯åç½®bï¼Œæˆ‘ä»¬åé¢ä¼šä»‹ç»ä¸ºä»€ä¹ˆå†™æˆv  è®¡ç®—åç½®b</span></span><br><span class="line">                v = self.critic(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">                delta = v_target - v <span class="comment"># è®¡ç®—ä¼˜åŠ¿å€¼</span></span><br><span class="line">                advantage = tf.stop_gradient(delta) <span class="comment"># æ–­å¼€æ¢¯åº¦è¿æ¥ </span></span><br><span class="line">                <span class="comment"># ç”±äºTFçš„gather_ndä¸pytorchçš„gatheråŠŸèƒ½ä¸ä¸€æ ·ï¼Œéœ€è¦æ„é€ </span></span><br><span class="line">                <span class="comment"># gather_ndéœ€è¦çš„åæ ‡å‚æ•°ï¼Œindices:[b, 2]</span></span><br><span class="line">                <span class="comment"># pi_a = pi.gather(1, a) # pytorchåªéœ€è¦ä¸€è¡Œå³å¯å®ç°</span></span><br><span class="line">                a = tf.gather(action, index, axis=<span class="number">0</span>) <span class="comment"># å–å‡ºbatchçš„åŠ¨ä½œat</span></span><br><span class="line">                <span class="comment"># batchçš„åŠ¨ä½œåˆ†å¸ƒpi(a|st)  æ¯ä¸ªåŠ¨ä½œå‡ºç°çš„æ¦‚ç‡</span></span><br><span class="line">                pi = self.actor(tf.gather(state, index, axis=<span class="number">0</span>))</span><br><span class="line">                <span class="comment">#åˆ›å»ºåºåˆ—  æ‰©å±•ç»´åº¦</span></span><br><span class="line">                indices = tf.expand_dims(tf.<span class="built_in">range</span>(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">                <span class="comment">#ä¸aè¿›è¡Œæ‹¼æ¥</span></span><br><span class="line">                indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">                pi_a = tf.gather_nd(pi, indices)  <span class="comment"># åŠ¨ä½œçš„æ¦‚ç‡å€¼pi(at|st), [b]  #æŒ‡å®šæ¯æ¬¡é‡‡æ ·ç‚¹çš„å¤šç»´åæ ‡æ¥å®ç°é‡‡æ ·å¤šä¸ªç‚¹çš„ç›®çš„</span></span><br><span class="line">                pi_a = tf.expand_dims(pi_a, axis=<span class="number">1</span>)  <span class="comment"># [b]=&gt; [b,1] </span></span><br><span class="line">                <span class="comment"># é‡è¦æ€§é‡‡æ ·  ä¸ä»åŸåˆ†å¸ƒğ‘ä¸­è¿›è¡Œé‡‡æ ·ï¼Œè€Œé€šè¿‡å¦ä¸€ä¸ªåˆ†å¸ƒğ‘ä¸­è¿› è¡Œé‡‡æ ·ï¼Œåªéœ€è¦ä¹˜ä»¥ğ‘(ğœ)/ğ‘(ğœ)æ¯”ç‡å³å¯</span></span><br><span class="line">                ratio = (pi_a / tf.gather(old_action_log_prob, index, axis=<span class="number">0</span>))</span><br><span class="line">                surr1 = ratio * advantage</span><br><span class="line">                <span class="comment">#å®ç°ä¸Šä¸‹é™å¹…</span></span><br><span class="line">                surr2 = tf.clip_by_value(ratio, <span class="number">1</span> - epsilon, <span class="number">1</span> + epsilon) * advantage</span><br><span class="line">                <span class="comment"># PPOè¯¯å·®å‡½æ•°</span></span><br><span class="line">                policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))</span><br><span class="line">                <span class="comment"># å¯¹äºåç½®væ¥è¯´ï¼Œå¸Œæœ›ä¸MCä¼°è®¡çš„R(st)è¶Šæ¥è¿‘è¶Šå¥½</span></span><br><span class="line">                value_loss = losses.MSE(v_target, v)</span><br><span class="line">            <span class="comment"># ä¼˜åŒ–ç­–ç•¥ç½‘ç»œ</span></span><br><span class="line">            grads = tape1.gradient(policy_loss, self.actor.trainable_variables)</span><br><span class="line">            self.actor_optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.actor.trainable_variables))</span><br><span class="line">            <span class="comment"># ä¼˜åŒ–åç½®å€¼ç½‘ç»œ</span></span><br><span class="line">            grads = tape2.gradient(value_loss, self.critic.trainable_variables)</span><br><span class="line">            self.critic_optimizer.apply_gradients(<span class="built_in">zip</span>(grads, self.critic.trainable_variables))</span><br><span class="line"></span><br><span class="line">        self.buffer = []  <span class="comment"># æ¸…ç©ºå·²è®­ç»ƒæ•°æ®</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    agent = PPO()</span><br><span class="line">    returns = [] <span class="comment"># ç»Ÿè®¡æ€»å›æŠ¥</span></span><br><span class="line">    total = <span class="number">0</span> <span class="comment"># ä¸€æ®µæ—¶é—´å†…å¹³å‡å›æŠ¥</span></span><br><span class="line">    <span class="keyword">for</span> i_epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># è®­ç»ƒå›åˆæ•°</span></span><br><span class="line">        state = env.reset() <span class="comment"># å¤ä½ç¯å¢ƒ</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># æœ€å¤šè€ƒè™‘500æ­¥</span></span><br><span class="line">            <span class="comment"># é€šè¿‡æœ€æ–°ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’</span></span><br><span class="line">            action, action_prob = agent.select_action(state)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            <span class="comment"># æ„å»ºæ ·æœ¬å¹¶å­˜å‚¨  &#x27;state&#x27;, &#x27;action&#x27;, &#x27;a_log_prob åŠ¨ä½œå‡ºç°çš„æ¦‚ç‡&#x27;, &#x27;reward&#x27;, &#x27;next_state&#x27;</span></span><br><span class="line">            trans = Transition(state, action, action_prob, reward, next_state)</span><br><span class="line">            <span class="comment">#å­˜å‚¨çŠ¶æ€</span></span><br><span class="line">            agent.store_transition(trans)</span><br><span class="line">            state = next_state <span class="comment"># åˆ·æ–°çŠ¶æ€</span></span><br><span class="line">            total += reward <span class="comment"># ç´¯ç§¯æ¿€åŠ±</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> done: <span class="comment"># åˆé€‚çš„æ—¶é—´ç‚¹è®­ç»ƒç½‘ç»œ</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(agent.buffer) &gt;= batch_size:</span><br><span class="line">                    <span class="comment"># äº¤äº’ä¸€å®šè½®æ¬¡ä¹‹åè¿›è¡Œç½‘ç»œçš„è®­ç»ƒ</span></span><br><span class="line">                    agent.optimize() <span class="comment"># è®­ç»ƒç½‘ç»œ</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i_epoch % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># æ¯20ä¸ªå›åˆç»Ÿè®¡ä¸€æ¬¡å¹³å‡å›æŠ¥</span></span><br><span class="line">            returns.append(total/<span class="number">20</span>)</span><br><span class="line">            total = <span class="number">0</span></span><br><span class="line">            print(i_epoch, returns[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    print(np.array(returns))</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*<span class="number">20</span>, np.array(returns))</span><br><span class="line">    plt.plot(np.arange(<span class="built_in">len</span>(returns))*<span class="number">20</span>, np.array(returns), <span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;å›åˆæ•°&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;æ€»å›æŠ¥&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;ppo-tf-cartpole.svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line">    print(<span class="string">&quot;end&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="å€¼å‡½æ•°æ–¹æ³•"><a class="markdownIt-Anchor" href="#å€¼å‡½æ•°æ–¹æ³•"></a> å€¼å‡½æ•°æ–¹æ³•</h1>
<p>ç­–ç•¥æ¢¯åº¦æ–¹æ³•é€šè¿‡ç›´æ¥å‚æ•°åŒ–ç­–ç•¥ç½‘ç»œï¼Œæ¥ä¼˜åŒ–å¾—åˆ°æ›´å¥½çš„ç­–ç•¥æ¨¡å‹ã€‚åœ¨å¼ºåŒ–å­¦ä¹ é¢† åŸŸï¼Œé™¤äº†ç­–ç•¥æ–¹æ³•å¤–ï¼Œè¿˜æœ‰å¦å¤–ä¸€ç±»é€šè¿‡å»ºæ¨¡å€¼å‡½æ•°è€Œé—´æ¥è·å¾—ç­–ç•¥çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æŠŠå®ƒç»Ÿ ç§°ä¸ºå€¼å‡½æ•°æ–¹æ³•ã€‚</p>
<h2 id="å€¼å‡½æ•°"><a class="markdownIt-Anchor" href="#å€¼å‡½æ•°"></a> å€¼å‡½æ•°</h2>
<p>çŠ¶æ€å€¼å‡½æ•°å’ŒçŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°ï¼Œä¸¤è€…å‡è¡¨ç¤ºåœ¨ç­–ç•¥ğœ‹ä¸‹çš„æœŸæœ›å›æŠ¥ï¼Œè½¨è¿¹èµ·ç‚¹å®šä¹‰ä¸ä¸€æ ·ã€‚</p>
<ul>
<li><strong>çŠ¶æ€å€¼å‡½æ•°(State Value Functionï¼Œç®€ç§° V å‡½æ•°)</strong>ï¼šä»çŠ¶æ€ğ‘ ğ‘¡å¼€å§‹ï¼Œåœ¨ç­–ç•¥ğœ‹æ§ åˆ¶ä¸‹èƒ½è·å¾—çš„æœŸæœ›å›æŠ¥å€¼:<br />
<img src="https://img-blog.csdnimg.cn/20201208145941156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></li>
<li><strong>çŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°(State-Action Value Functionï¼Œç®€ç§° Q å‡½æ•°)</strong>ï¼šä»çŠ¶æ€ğ‘ ğ‘¡å¹¶æ‰§è¡Œ åŠ¨ä½œğ‘ğ‘¡çš„åŒé‡è®¾å®šä¸‹ï¼Œåœ¨ç­–ç•¥ğœ‹æ§åˆ¶ä¸‹èƒ½è·å¾—çš„æœŸæœ›å›æŠ¥å€¼<br />
<img src="https://img-blog.csdnimg.cn/20201208150024114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></li>
</ul>
<h2 id="å€¼å‡½æ•°çš„ä¼°è®¡"><a class="markdownIt-Anchor" href="#å€¼å‡½æ•°çš„ä¼°è®¡"></a> å€¼å‡½æ•°çš„ä¼°è®¡</h2>
<ul>
<li><strong>è’™ç‰¹å¡ç½—æ–¹æ³•</strong><br />
<img src="https://img-blog.csdnimg.cn/20201208150610547.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></li>
<li><strong>æ—¶åºå·®åˆ†æ–¹æ³•</strong><br />
<img src="https://img-blog.csdnimg.cn/20201208150629891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></li>
</ul>
<h2 id="ç­–ç•¥æ”¹è¿›"><a class="markdownIt-Anchor" href="#ç­–ç•¥æ”¹è¿›"></a> ç­–ç•¥æ”¹è¿›</h2>
<ul>
<li><strong>Îµ-è´ªå¿ƒæ³•</strong><br />
<img src="https://img-blog.csdnimg.cn/20201208153008119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></li>
</ul>
<h2 id="dqn-ç®—æ³•"><a class="markdownIt-Anchor" href="#dqn-ç®—æ³•"></a> DQN ç®—æ³•</h2>
<p><img src="https://img-blog.csdnimg.cn/20201208153049582.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /><br />
<img src="https://img-blog.csdnimg.cn/20201208153104220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></p>
<h2 id="dqn-å®æˆ˜"><a class="markdownIt-Anchor" href="#dqn-å®æˆ˜"></a> DQN å®æˆ˜</h2>
<ul>
<li><strong>Q ç½‘ç»œ</strong>å¹³è¡¡æ†æ¸¸æˆçš„çŠ¶æ€æ˜¯é•¿åº¦ä¸º 4 çš„å‘é‡ï¼Œå› æ­¤ Q ç½‘ç»œçš„è¾“å…¥è®¾è®¡ä¸º 4 ä¸ªèŠ‚ç‚¹ï¼Œ ç»è¿‡256 âˆ’ 256 âˆ’ 2çš„å…¨è¿æ¥å±‚ï¼Œå¾—åˆ°è¾“å‡ºèŠ‚ç‚¹æ•°ä¸º 2 çš„ Q å‡½æ•°ä¼°å€¼çš„åˆ†å¸ƒğ‘„(ğ‘ , ğ‘)ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># åˆ›å»ºQç½‘ç»œï¼Œè¾“å…¥ä¸ºçŠ¶æ€å‘é‡ï¼Œè¾“å‡ºä¸ºåŠ¨ä½œçš„Qå€¼</span></span><br><span class="line">        <span class="built_in">super</span>(Qnet, self).__init__()</span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc3 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(x))</span><br><span class="line">        x = tf.nn.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ç»éªŒå›æ”¾æ± </strong>åœ¨ DQN ç®—æ³•ä¸­ä½¿ç”¨äº†ç»éªŒå›æ”¾æ± æ¥å‡è½»æ•°æ®ä¹‹é—´çš„å¼ºç›¸å…³æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨ ReplayBuffer ç±»ä¸­çš„ Deque å¯¹è±¡æ¥å®ç°ç¼“å­˜æ± çš„åŠŸèƒ½ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œé€šè¿‡ put(transition)æ–¹æ³• å°†æœ€æ–°çš„(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®å­˜å…¥ Deque å¯¹è±¡ï¼Œå¹¶é€šè¿‡ sample(n)æ–¹æ³•ä» Deque å¯¹è±¡ä¸­éšæœºé‡‡æ ·å‡º n ä¸ª(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># ç»éªŒå›æ”¾æ± </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># åŒå‘é˜Ÿåˆ—</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#é€šè¿‡ put(transition)æ–¹æ³• å°†æœ€æ–°çš„(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®å­˜å…¥ Deque å¯¹è±¡</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#é€šè¿‡ sample(n)æ–¹æ³•ä» Deque å¯¹è±¡ä¸­éšæœºé‡‡æ ·å‡º n ä¸ª(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># ä»å›æ”¾æ± é‡‡æ ·nä¸ª5å…ƒç»„</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []</span><br><span class="line">        <span class="comment"># æŒ‰ç±»åˆ«è¿›è¡Œæ•´ç†</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime, done_mask = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">            done_mask_lst.append([done_mask])</span><br><span class="line">        <span class="comment"># è½¬æ¢æˆTensor</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(s_lst, dtype=tf.float32),\</span><br><span class="line">                      tf.constant(a_lst, dtype=tf.int32), \</span><br><span class="line">                      tf.constant(r_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(s_prime_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(done_mask_lst, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ç­–ç•¥æ”¹è¿›</strong> è¿™é‡Œå®ç°äº†Îµ-è´ªå¿ƒæ³•ã€‚åœ¨é‡‡æ ·åŠ¨ä½œæ—¶ï¼Œæœ‰1 âˆ’ Îµçš„æ¦‚ç‡é€‰æ‹©arg max ğ‘„ğœ‹ (ğ‘ , ğ‘)ï¼Œæœ‰Îµçš„æ¦‚ç‡éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_action</span>(<span class="params">self, s, epsilon</span>):</span></span><br><span class="line">    <span class="comment"># é€å…¥çŠ¶æ€å‘é‡ï¼Œè·å–ç­–ç•¥: [4]</span></span><br><span class="line">    s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># s: [4] =&gt; [1,4]</span></span><br><span class="line">    s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">    out = self(s)[<span class="number">0</span>]</span><br><span class="line">    coin = random.random()</span><br><span class="line">    <span class="comment"># ç­–ç•¥æ”¹è¿›ï¼še-è´ªå¿ƒæ–¹å¼</span></span><br><span class="line">    <span class="keyword">if</span> coin &lt; epsilon:</span><br><span class="line">        <span class="comment"># epsilonå¤§çš„æ¦‚ç‡éšæœºé€‰å–</span></span><br><span class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">int</span>(tf.argmax(out))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ç½‘ç»œä¸»æµç¨‹</strong>ç½‘ç»œæœ€å¤šè®­ç»ƒ 10000 ä¸ªå›åˆï¼Œåœ¨å›åˆå¼€å§‹æ—¶ï¼Œé¦–å…ˆå¤ä½æ¸¸æˆï¼Œå¾—åˆ°åˆå§‹çŠ¶ æ€ğ‘ ï¼Œå¹¶ä»å½“å‰ Q ç½‘ç»œä¸­é—´é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œï¼Œä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå¾—åˆ°æ•°æ®å¯¹(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)ï¼Œå¹¶å­˜ å…¥ç»éªŒå›æ”¾æ± ã€‚å¦‚æœå½“å‰ç»éªŒå›æ”¾æ± æ ·æœ¬æ•°é‡è¶³å¤Ÿå¤šï¼Œåˆ™é‡‡æ ·ä¸€ä¸ª Batch æ•°æ®ï¼Œæ ¹æ® TD è¯¯å·®ä¼˜åŒ– Q ç½‘ç»œçš„ä¼°å€¼ï¼Œç›´è‡³æ¸¸æˆå›åˆç»“æŸã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):  <span class="comment"># è®­ç»ƒæ¬¡æ•°</span></span><br><span class="line">    <span class="comment"># epsilonæ¦‚ç‡ä¹Ÿä¼š8%åˆ°1%è¡°å‡ï¼Œè¶Šåˆ°åé¢è¶Šä½¿ç”¨Qå€¼æœ€å¤§çš„åŠ¨ä½œ</span></span><br><span class="line">    epsilon = <span class="built_in">max</span>(<span class="number">0.01</span>, <span class="number">0.08</span> - <span class="number">0.01</span> * (n_epi / <span class="number">200</span>))</span><br><span class="line">    s = env.reset()  <span class="comment"># å¤ä½ç¯å¢ƒ</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">600</span>):  <span class="comment"># ä¸€ä¸ªå›åˆæœ€å¤§æ—¶é—´æˆ³</span></span><br><span class="line">        <span class="comment"># if n_epi&gt;1000:</span></span><br><span class="line">        <span class="comment">#     env.render()</span></span><br><span class="line">        <span class="comment"># æ ¹æ®å½“å‰Qç½‘ç»œæå–ç­–ç•¥ï¼Œå¹¶æ”¹è¿›ç­–ç•¥</span></span><br><span class="line">        a = q.sample_action(s, epsilon)</span><br><span class="line">        <span class="comment"># ä½¿ç”¨æ”¹è¿›çš„ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’</span></span><br><span class="line">        s_prime, r, done, info = env.step(a)</span><br><span class="line">        done_mask = <span class="number">0.0</span> <span class="keyword">if</span> done <span class="keyword">else</span> <span class="number">1.0</span>  <span class="comment"># ç»“æŸæ ‡å¿—æ©ç </span></span><br><span class="line">        <span class="comment"># ä¿å­˜5å…ƒç»„</span></span><br><span class="line">        memory.put((s, a, r / <span class="number">100.0</span>, s_prime, done_mask))</span><br><span class="line">        s = s_prime  <span class="comment"># åˆ·æ–°çŠ¶æ€</span></span><br><span class="line">        score += r  <span class="comment"># è®°å½•æ€»å›æŠ¥</span></span><br><span class="line">        <span class="keyword">if</span> done:  <span class="comment"># å›åˆç»“æŸ</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> memory.size() &gt; <span class="number">2000</span>:  <span class="comment"># ç¼“å†²æ± åªæœ‰å¤§äº2000å°±å¯ä»¥è®­ç»ƒ</span></span><br><span class="line">        train(q, q_target, memory, optimizer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span> <span class="keyword">and</span> n_epi != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> src, dest <span class="keyword">in</span> <span class="built_in">zip</span>(q.variables, q_target.variables):</span><br><span class="line">            dest.assign(src)  <span class="comment"># å½±å­ç½‘ç»œæƒå€¼æ¥è‡ªQ</span></span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/202012081649069.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></p>
<ul>
<li><strong>ä¼˜åŒ– Q ç½‘ç»œ</strong><br />
<img src="https://img-blog.csdnimg.cn/20201208164928927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">q, q_target, memory, optimizer</span>):</span></span><br><span class="line">    <span class="comment"># é€šè¿‡Qç½‘ç»œå’Œå½±å­ç½‘ç»œæ¥æ„é€ è´å°”æ›¼æ–¹ç¨‹çš„è¯¯å·®ï¼Œ</span></span><br><span class="line">    <span class="comment"># å¹¶åªæ›´æ–°Qç½‘ç»œï¼Œå½±å­ç½‘ç»œçš„æ›´æ–°ä¼šæ»åQç½‘ç»œ</span></span><br><span class="line">    <span class="comment">#Smooth L1 è¯¯å·®å¯ä»¥é€šè¿‡ Huber è¯¯å·®ç±»å®ç°</span></span><br><span class="line">    huber = losses.Huber()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># è®­ç»ƒ10æ¬¡</span></span><br><span class="line">        <span class="comment"># ä»ç¼“å†²æ± é‡‡æ ·</span></span><br><span class="line">        s, a, r, s_prime, done_mask = memory.sample(batch_size)</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="comment"># s: [b, 4]</span></span><br><span class="line">            q_out = q(s)  <span class="comment"># å¾—åˆ°Q(s,a)çš„åˆ†å¸ƒ</span></span><br><span class="line">            <span class="comment"># ç”±äºTFçš„gather_ndä¸pytorchçš„gatheråŠŸèƒ½ä¸ä¸€æ ·ï¼Œéœ€è¦æ„é€ </span></span><br><span class="line">            <span class="comment"># gather_ndéœ€è¦çš„åæ ‡å‚æ•°ï¼Œindices:[b, 2]</span></span><br><span class="line">            <span class="comment"># pi_a = pi.gather(1, a) # pytorchåªéœ€è¦ä¸€è¡Œå³å¯å®ç°</span></span><br><span class="line">            indices = tf.expand_dims(tf.<span class="built_in">range</span>(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">            indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">            q_a = tf.gather_nd(q_out, indices) <span class="comment"># åŠ¨ä½œçš„æ¦‚ç‡å€¼, [b]</span></span><br><span class="line">            q_a = tf.expand_dims(q_a, axis=<span class="number">1</span>) <span class="comment"># [b]=&gt; [b,1]</span></span><br><span class="line">            <span class="comment"># å¾—åˆ°Q(s&#x27;,a)çš„æœ€å¤§å€¼ï¼Œå®ƒæ¥è‡ªå½±å­ç½‘ç»œï¼ [b,4]=&gt;[b,2]=&gt;[b,1]</span></span><br><span class="line">            max_q_prime = tf.reduce_max(q_target(s_prime),axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># æ„é€ Q(s,a_t)çš„ç›®æ ‡å€¼ï¼Œæ¥è‡ªè´å°”æ›¼æ–¹ç¨‹</span></span><br><span class="line">            target = r + gamma * max_q_prime * done_mask</span><br><span class="line">            <span class="comment"># è®¡ç®—Q(s,a_t)ä¸ç›®æ ‡å€¼çš„è¯¯å·®</span></span><br><span class="line">            loss = huber(q_a, target)</span><br><span class="line">        <span class="comment"># æ›´æ–°ç½‘ç»œï¼Œä½¿å¾—Q(s,a_t)ä¼°è®¡ç¬¦åˆè´å°”æ›¼æ–¹ç¨‹</span></span><br><span class="line">        grads = tape.gradient(loss, q.trainable_variables)</span><br><span class="line">        <span class="comment"># for p in grads:</span></span><br><span class="line">        <span class="comment">#     print(tf.norm(p))</span></span><br><span class="line">        <span class="comment"># print(grads)</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grads, q.trainable_variables))</span><br></pre></td></tr></table></figure>
<ul>
<li>å®Œæ•´ä»£ç </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> gym,os</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># åˆ›å»ºæ¸¸æˆç¯å¢ƒ</span></span><br><span class="line">env.seed(<span class="number">1234</span>)</span><br><span class="line">tf.random.set_seed(<span class="number">1234</span>)</span><br><span class="line">np.random.seed(<span class="number">1234</span>)</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;2&#x27;</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">&#x27;2.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">learning_rate = <span class="number">0.0002</span></span><br><span class="line">gamma = <span class="number">0.99</span></span><br><span class="line">buffer_limit = <span class="number">50000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>():</span></span><br><span class="line">    <span class="comment"># ç»éªŒå›æ”¾æ± </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># åŒå‘é˜Ÿåˆ—</span></span><br><span class="line">        self.buffer = collections.deque(maxlen=buffer_limit)</span><br><span class="line">        <span class="comment">#é€šè¿‡ put(transition)æ–¹æ³• å°†æœ€æ–°çš„(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®å­˜å…¥ Deque å¯¹è±¡</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    <span class="comment">#é€šè¿‡ sample(n)æ–¹æ³•ä» Deque å¯¹è±¡ä¸­éšæœºé‡‡æ ·å‡º n ä¸ª(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ â€²)æ•°æ®</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="comment"># ä»å›æ”¾æ± é‡‡æ ·nä¸ª5å…ƒç»„</span></span><br><span class="line">        mini_batch = random.sample(self.buffer, n)</span><br><span class="line">        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []</span><br><span class="line">        <span class="comment"># æŒ‰ç±»åˆ«è¿›è¡Œæ•´ç†</span></span><br><span class="line">        <span class="keyword">for</span> transition <span class="keyword">in</span> mini_batch:</span><br><span class="line">            s, a, r, s_prime, done_mask = transition</span><br><span class="line">            s_lst.append(s)</span><br><span class="line">            a_lst.append([a])</span><br><span class="line">            r_lst.append([r])</span><br><span class="line">            s_prime_lst.append(s_prime)</span><br><span class="line">            done_mask_lst.append([done_mask])</span><br><span class="line">        <span class="comment"># è½¬æ¢æˆTensor</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(s_lst, dtype=tf.float32),\</span><br><span class="line">                      tf.constant(a_lst, dtype=tf.int32), \</span><br><span class="line">                      tf.constant(r_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(s_prime_lst, dtype=tf.float32), \</span><br><span class="line">                      tf.constant(done_mask_lst, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qnet</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># åˆ›å»ºQç½‘ç»œï¼Œè¾“å…¥ä¸ºçŠ¶æ€å‘é‡ï¼Œè¾“å‡ºä¸ºåŠ¨ä½œçš„Qå€¼</span></span><br><span class="line">        <span class="built_in">super</span>(Qnet, self).__init__()</span><br><span class="line">        self.fc1 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc2 = layers.Dense(<span class="number">256</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line">        self.fc3 = layers.Dense(<span class="number">2</span>, kernel_initializer=<span class="string">&#x27;he_normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, training=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = tf.nn.relu(self.fc1(x))</span><br><span class="line">        x = tf.nn.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample_action</span>(<span class="params">self, s, epsilon</span>):</span></span><br><span class="line">        <span class="comment"># é€å…¥çŠ¶æ€å‘é‡ï¼Œè·å–ç­–ç•¥: [4]</span></span><br><span class="line">        s = tf.constant(s, dtype=tf.float32)</span><br><span class="line">        <span class="comment"># s: [4] =&gt; [1,4]</span></span><br><span class="line">        s = tf.expand_dims(s, axis=<span class="number">0</span>)</span><br><span class="line">        out = self(s)[<span class="number">0</span>]</span><br><span class="line">        coin = random.random()</span><br><span class="line">        <span class="comment"># ç­–ç•¥æ”¹è¿›ï¼še-è´ªå¿ƒæ–¹å¼</span></span><br><span class="line">        <span class="keyword">if</span> coin &lt; epsilon:</span><br><span class="line">            <span class="comment"># epsilonå¤§çš„æ¦‚ç‡éšæœºé€‰å–</span></span><br><span class="line">            <span class="keyword">return</span> random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">int</span>(tf.argmax(out))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">q, q_target, memory, optimizer</span>):</span></span><br><span class="line">    <span class="comment"># é€šè¿‡Qç½‘ç»œå’Œå½±å­ç½‘ç»œæ¥æ„é€ è´å°”æ›¼æ–¹ç¨‹çš„è¯¯å·®ï¼Œ</span></span><br><span class="line">    <span class="comment"># å¹¶åªæ›´æ–°Qç½‘ç»œï¼Œå½±å­ç½‘ç»œçš„æ›´æ–°ä¼šæ»åQç½‘ç»œ</span></span><br><span class="line">    <span class="comment">#Smooth L1 è¯¯å·®å¯ä»¥é€šè¿‡ Huber è¯¯å·®ç±»å®ç°</span></span><br><span class="line">    huber = losses.Huber()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># è®­ç»ƒ10æ¬¡</span></span><br><span class="line">        <span class="comment"># ä»ç¼“å†²æ± é‡‡æ ·</span></span><br><span class="line">        s, a, r, s_prime, done_mask = memory.sample(batch_size)</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="comment"># s: [b, 4]</span></span><br><span class="line">            q_out = q(s)  <span class="comment"># å¾—åˆ°Q(s,a)çš„åˆ†å¸ƒ</span></span><br><span class="line">            <span class="comment"># ç”±äºTFçš„gather_ndä¸pytorchçš„gatheråŠŸèƒ½ä¸ä¸€æ ·ï¼Œéœ€è¦æ„é€ </span></span><br><span class="line">            <span class="comment"># gather_ndéœ€è¦çš„åæ ‡å‚æ•°ï¼Œindices:[b, 2]</span></span><br><span class="line">            <span class="comment"># pi_a = pi.gather(1, a) # pytorchåªéœ€è¦ä¸€è¡Œå³å¯å®ç°</span></span><br><span class="line">            indices = tf.expand_dims(tf.<span class="built_in">range</span>(a.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line">            indices = tf.concat([indices, a], axis=<span class="number">1</span>)</span><br><span class="line">            q_a = tf.gather_nd(q_out, indices) <span class="comment"># åŠ¨ä½œçš„æ¦‚ç‡å€¼, [b]</span></span><br><span class="line">            q_a = tf.expand_dims(q_a, axis=<span class="number">1</span>) <span class="comment"># [b]=&gt; [b,1]</span></span><br><span class="line">            <span class="comment"># å¾—åˆ°Q(s&#x27;,a)çš„æœ€å¤§å€¼ï¼Œå®ƒæ¥è‡ªå½±å­ç½‘ç»œï¼ [b,4]=&gt;[b,2]=&gt;[b,1]</span></span><br><span class="line">            max_q_prime = tf.reduce_max(q_target(s_prime),axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># æ„é€ Q(s,a_t)çš„ç›®æ ‡å€¼ï¼Œæ¥è‡ªè´å°”æ›¼æ–¹ç¨‹</span></span><br><span class="line">            target = r + gamma * max_q_prime * done_mask</span><br><span class="line">            <span class="comment"># è®¡ç®—Q(s,a_t)ä¸ç›®æ ‡å€¼çš„è¯¯å·®</span></span><br><span class="line">            loss = huber(q_a, target)</span><br><span class="line">        <span class="comment"># æ›´æ–°ç½‘ç»œï¼Œä½¿å¾—Q(s,a_t)ä¼°è®¡ç¬¦åˆè´å°”æ›¼æ–¹ç¨‹</span></span><br><span class="line">        grads = tape.gradient(loss, q.trainable_variables)</span><br><span class="line">        <span class="comment"># for p in grads:</span></span><br><span class="line">        <span class="comment">#     print(tf.norm(p))</span></span><br><span class="line">        <span class="comment"># print(grads)</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grads, q.trainable_variables))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)  <span class="comment"># åˆ›å»ºç¯å¢ƒ</span></span><br><span class="line">    q = Qnet()  <span class="comment"># åˆ›å»ºQç½‘ç»œ</span></span><br><span class="line">    q_target = Qnet()  <span class="comment"># åˆ›å»ºå½±å­ç½‘ç»œ</span></span><br><span class="line">    q.build(input_shape=(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">    q_target.build(input_shape=(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> src, dest <span class="keyword">in</span> <span class="built_in">zip</span>(q.variables, q_target.variables):</span><br><span class="line">        dest.assign(src) <span class="comment"># å½±å­ç½‘ç»œæƒå€¼æ¥è‡ªQ</span></span><br><span class="line">    memory = ReplayBuffer()  <span class="comment"># åˆ›å»ºå›æ”¾æ± </span></span><br><span class="line"></span><br><span class="line">    print_interval = <span class="number">20</span></span><br><span class="line">    score = <span class="number">0.0</span></span><br><span class="line">    optimizer = optimizers.Adam(lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n_epi <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):  <span class="comment"># è®­ç»ƒæ¬¡æ•°</span></span><br><span class="line">        <span class="comment"># epsilonæ¦‚ç‡ä¹Ÿä¼š8%åˆ°1%è¡°å‡ï¼Œè¶Šåˆ°åé¢è¶Šä½¿ç”¨Qå€¼æœ€å¤§çš„åŠ¨ä½œ</span></span><br><span class="line">        epsilon = <span class="built_in">max</span>(<span class="number">0.01</span>, <span class="number">0.08</span> - <span class="number">0.01</span> * (n_epi / <span class="number">200</span>))</span><br><span class="line">        s = env.reset()  <span class="comment"># å¤ä½ç¯å¢ƒ</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">600</span>):  <span class="comment"># ä¸€ä¸ªå›åˆæœ€å¤§æ—¶é—´æˆ³</span></span><br><span class="line">            <span class="comment"># if n_epi&gt;1000:</span></span><br><span class="line">            <span class="comment">#     env.render()</span></span><br><span class="line">            <span class="comment"># æ ¹æ®å½“å‰Qç½‘ç»œæå–ç­–ç•¥ï¼Œå¹¶æ”¹è¿›ç­–ç•¥</span></span><br><span class="line">            a = q.sample_action(s, epsilon)</span><br><span class="line">            <span class="comment"># ä½¿ç”¨æ”¹è¿›çš„ç­–ç•¥ä¸ç¯å¢ƒäº¤äº’</span></span><br><span class="line">            s_prime, r, done, info = env.step(a)</span><br><span class="line">            done_mask = <span class="number">0.0</span> <span class="keyword">if</span> done <span class="keyword">else</span> <span class="number">1.0</span>  <span class="comment"># ç»“æŸæ ‡å¿—æ©ç </span></span><br><span class="line">            <span class="comment"># ä¿å­˜5å…ƒç»„</span></span><br><span class="line">            memory.put((s, a, r / <span class="number">100.0</span>, s_prime, done_mask))</span><br><span class="line">            s = s_prime  <span class="comment"># åˆ·æ–°çŠ¶æ€</span></span><br><span class="line">            score += r  <span class="comment"># è®°å½•æ€»å›æŠ¥</span></span><br><span class="line">            <span class="keyword">if</span> done:  <span class="comment"># å›åˆç»“æŸ</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> memory.size() &gt; <span class="number">2000</span>:  <span class="comment"># ç¼“å†²æ± åªæœ‰å¤§äº2000å°±å¯ä»¥è®­ç»ƒ</span></span><br><span class="line">            train(q, q_target, memory, optimizer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> n_epi % print_interval == <span class="number">0</span> <span class="keyword">and</span> n_epi != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> src, dest <span class="keyword">in</span> <span class="built_in">zip</span>(q.variables, q_target.variables):</span><br><span class="line">                dest.assign(src)  <span class="comment"># å½±å­ç½‘ç»œæƒå€¼æ¥è‡ªQ</span></span><br><span class="line">            print(<span class="string">&quot;# of episode :&#123;&#125;, avg score : &#123;:.1f&#125;, buffer size : &#123;&#125;, &quot;</span> \</span><br><span class="line">                  <span class="string">&quot;epsilon : &#123;:.1f&#125;%&quot;</span> \</span><br><span class="line">                  .<span class="built_in">format</span>(n_epi, score / print_interval, memory.size(), epsilon * <span class="number">100</span>))</span><br><span class="line">            score = <span class="number">0.0</span></span><br><span class="line">    env.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h1 id="actor-critic-æ–¹æ³•"><a class="markdownIt-Anchor" href="#actor-critic-æ–¹æ³•"></a> Actor-Critic æ–¹æ³•</h1>
<p><img src="https://img-blog.csdnimg.cn/20201208193539146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></p>
<h2 id="advantage-ac-ç®—æ³•"><a class="markdownIt-Anchor" href="#advantage-ac-ç®—æ³•"></a> Advantage AC ç®—æ³•</h2>
<p><img src="https://img-blog.csdnimg.cn/20201208193622351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></p>
<h2 id="a3c-ç®—æ³•"><a class="markdownIt-Anchor" href="#a3c-ç®—æ³•"></a> A3C ç®—æ³•</h2>
<p><img src="https://img-blog.csdnimg.cn/20201208193638743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" /></p>
<ul>
<li>A3C ç®—æ³•å…¨ç§°ä¸º Asynchronous Advantage Actor-Critic ç®—æ³•ï¼Œæ˜¯ DeepMind åŸºäºAdvantage Actor-Critic ç®—æ³•æå‡ºæ¥çš„å¼‚æ­¥ç‰ˆæœ¬ [8]ï¼Œå°† Actor-Critic ç½‘ç»œéƒ¨ç½²åœ¨å¤šä¸ªçº¿ç¨‹ä¸­<br />
åŒæ—¶è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡å…¨å±€ç½‘ç»œæ¥åŒæ­¥å‚æ•°ã€‚è¿™ç§å¼‚æ­¥è®­ç»ƒçš„æ¨¡å¼å¤§å¤§æå‡äº†è®­ç»ƒæ•ˆç‡ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”ç®—æ³•æ€§èƒ½ä¹Ÿæ›´å¥½ã€‚</li>
<li>å¦‚å›¾ ï¼Œç®—æ³•ä¼šæ–°å»ºä¸€ä¸ªå…¨å±€ç½‘ç»œ Global Network å’Œ M ä¸ª Worker çº¿ç¨‹ï¼Œ Global Network åŒ…å«äº†Actor å’Œ Critic ç½‘ç»œï¼Œæ¯ä¸ªçº¿ç¨‹å‡æ–°å»ºä¸€ä¸ªäº¤äº’ç¯å¢ƒå’Œ Actor å’Œ Critic ç½‘ç»œã€‚åˆå§‹åŒ–é˜¶æ®µ Global Network éšæœºåˆå§‹åŒ–å‚æ•°ğœƒ å’Œğœ™ ï¼ŒWorker ä¸­çš„ Actor-Critic ç½‘ç»œä» Global Networkä¸­åŒæ­¥æ‹‰å–å‚æ•°æ¥åˆå§‹åŒ–ç½‘ç»œã€‚åœ¨è®­ç»ƒæ—¶ï¼ŒWorker ä¸­çš„ Actor-Critic ç½‘ç»œé¦–å…ˆä» Global Networkæ‹‰å–æœ€æ–°å‚æ•°ï¼Œç„¶ååœ¨æœ€æ–°ç­–ç•¥ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡)æ‰é‡‡æ ·åŠ¨ä½œä¸ç§æœ‰ç¯ å¢ƒè¿›è¡Œäº¤äº’ï¼Œå¹¶æ ¹æ® Advantage Actor-Critic ç®—æ³•æ–¹æ³•è®¡ç®—å‚æ•°ğœƒ å’Œğœ™çš„æ¢¯åº¦ä¿¡æ¯ã€‚å®Œæˆæ¢¯ åº¦è®¡ç®—åï¼Œå„ä¸ª Worker å°†æ¢¯åº¦ä¿¡æ¯æäº¤åˆ° Global Network ä¸­ï¼Œåˆ©ç”¨ Global Network çš„ä¼˜åŒ– å™¨å®Œæˆ Global Networkçš„ç½‘ç»œå‚æ•°æ›´æ–°ã€‚åœ¨ç®—æ³•æµ‹è¯•é˜¶æ®µï¼Œåªä½¿ç”¨Global Network ä¸ç¯å¢ƒäº¤äº’å³å¯ã€‚</li>
</ul>
<h2 id="a3c-å®æˆ˜"><a class="markdownIt-Anchor" href="#a3c-å®æˆ˜"></a> A3C å®æˆ˜</h2>
<ul>
<li>å¼‚æ­¥çš„ A3C ç®—æ³•ã€‚å’Œæ™®é€šçš„ Advantage AC ç®—æ³•ä¸€æ ·ï¼Œéœ€è¦åˆ›å»º ActorCritic ç½‘ç»œå¤§ç±»ï¼Œå®ƒåŒ…å«äº†ä¸€ä¸ª Actor å­ç½‘ç»œå’Œä¸€ä¸ª Critic å­ç½‘ç»œï¼Œæœ‰æ—¶ Actor å’Œ Critic ä¼šå…±äº«å‰é¢ç½‘ç»œæ•°å±‚ï¼Œå‡å°‘ç½‘ç»œçš„å‚æ•°é‡ã€‚å¹³è¡¡æ†æ¸¸æˆæ¯”è¾ƒç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª 2 å±‚ å…¨è¿æ¥ç½‘ç»œæ¥å‚æ•°åŒ– Actor ç½‘ç»œï¼Œä½¿ç”¨å¦ä¸€ä¸ª 2 å±‚å…¨è¿æ¥ç½‘ç»œæ¥å‚æ•°åŒ– Critic ç½‘ç»œã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorCritic</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># Actor-Criticæ¨¡å‹</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_size, action_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ActorCritic, self).__init__()</span><br><span class="line">        self.state_size = state_size <span class="comment"># çŠ¶æ€å‘é‡é•¿åº¦</span></span><br><span class="line">        self.action_size = action_size <span class="comment"># åŠ¨ä½œæ•°é‡</span></span><br><span class="line">        <span class="comment"># ç­–ç•¥ç½‘ç»œActor</span></span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.policy_logits = layers.Dense(action_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Vç½‘ç»œCritic</span></span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.values = layers.Dense(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>Actor-Critic çš„å‰å‘ä¼ æ’­è¿‡ç¨‹åˆ†åˆ«è®¡ç®—ç­–ç•¥åˆ†å¸ƒğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡)å’Œ V å‡½æ•°ä¼°è®¡ğ‘‰ğœ‹(ğ‘ ğ‘¡)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">    <span class="comment"># è·å¾—ç­–ç•¥åˆ†å¸ƒPi(a|s)</span></span><br><span class="line">    x = self.dense1(inputs)</span><br><span class="line">    logits = self.policy_logits(x)</span><br><span class="line">    <span class="comment"># è·å¾—v(s)</span></span><br><span class="line">    v = self.dense2(inputs)</span><br><span class="line">    values = self.values(v)</span><br><span class="line">    <span class="keyword">return</span> logits, values</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Worker çº¿ç¨‹ç±»</strong> åœ¨ Worker çº¿ç¨‹ä¸­ï¼Œå®ç°å’Œ Advantage AC ç®—æ³•ä¸€æ ·çš„è®¡ç®—æµç¨‹ï¼Œåªæ˜¯ è®¡ç®—äº§ç”Ÿçš„å‚æ•°ğœƒ å’Œğœ™çš„æ¢¯åº¦ä¿¡æ¯å¹¶ä¸ç›´æ¥ç”¨äºæ›´æ–° Worker çš„ Actor-Critic ç½‘ç»œï¼Œè€Œæ˜¯æ äº¤åˆ° Global Network æ›´æ–°ã€‚å…·ä½“åœ°ï¼Œåœ¨ Worker ç±»åˆå§‹åŒ–é˜¶æ®µï¼Œè·å¾— Global Network ä¼ å…¥çš„ server å¯¹è±¡å’Œ opt å¯¹è±¡ï¼Œåˆ†åˆ«ä»£è¡¨äº† Global Network æ¨¡å‹å’Œä¼˜åŒ–å™¨;å¹¶åˆ›å»ºç§æœ‰çš„ ActorCritic ç½‘ç»œç±» client å’Œäº¤äº’ç¯å¢ƒ envã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span>(<span class="params">threading.Thread</span>):</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,  server, opt, result_queue, idx</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Worker, self).__init__()</span><br><span class="line">        self.result_queue = result_queue <span class="comment"># å…±äº«é˜Ÿåˆ—</span></span><br><span class="line">        self.server = server <span class="comment"># ä¸­å¤®æ¨¡å‹</span></span><br><span class="line">        self.opt = opt <span class="comment"># ä¸­å¤®ä¼˜åŒ–å™¨</span></span><br><span class="line">        self.client = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># çº¿ç¨‹ç§æœ‰ç½‘ç»œ</span></span><br><span class="line">        self.worker_idx = idx <span class="comment"># çº¿ç¨‹id</span></span><br><span class="line">        self.env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>).unwrapped</span><br><span class="line">        self.ep_loss = <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<ul>
<li>åœ¨çº¿ç¨‹è¿è¡Œé˜¶æ®µï¼Œæ¯ä¸ªçº¿ç¨‹æœ€å¤šä¸ç¯å¢ƒäº¤äº’ 400 ä¸ªå›åˆï¼Œåœ¨å›åˆå¼€å§‹ï¼Œåˆ©ç”¨ client ç½‘ ç»œé‡‡æ ·åŠ¨ä½œä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå¹¶ä¿å­˜è‡³ Memory å¯¹è±¡ã€‚åœ¨å›åˆç»“æŸï¼Œè®­ç»ƒ Actor ç½‘ç»œå’Œ Critic ç½‘ç»œï¼Œå¾—åˆ°å‚æ•°ğœƒ å’Œğœ™çš„æ¢¯åº¦ä¿¡æ¯ï¼Œè°ƒç”¨ Global Network çš„ opt ä¼˜åŒ–å™¨å¯¹è±¡æ›´æ–° Global Networkã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span> </span><br><span class="line">    mem = Memory() <span class="comment"># æ¯ä¸ªworkerè‡ªå·±ç»´æŠ¤ä¸€ä¸ªmemory</span></span><br><span class="line">    <span class="keyword">for</span> epi_counter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># æœªè¾¾åˆ°æœ€å¤§å›åˆæ•°</span></span><br><span class="line">        current_state = self.env.reset() <span class="comment"># å¤ä½clientæ¸¸æˆçŠ¶æ€</span></span><br><span class="line">        mem.clear()</span><br><span class="line">        ep_reward = <span class="number">0.</span></span><br><span class="line">        ep_steps = <span class="number">0</span>  </span><br><span class="line">        done = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">            <span class="comment"># è·å¾—Pi(a|s),æœªç»softmax</span></span><br><span class="line">            logits, _ = self.client(tf.constant(current_state[<span class="literal">None</span>, :],</span><br><span class="line">                                     dtype=tf.float32))</span><br><span class="line">            probs = tf.nn.softmax(logits)</span><br><span class="line">            <span class="comment"># éšæœºé‡‡æ ·åŠ¨ä½œ</span></span><br><span class="line">            action = np.random.choice(<span class="number">2</span>, p=probs.numpy()[<span class="number">0</span>])</span><br><span class="line">            new_state, reward, done, _ = self.env.step(action) <span class="comment"># äº¤äº’ </span></span><br><span class="line">            ep_reward += reward <span class="comment"># ç´¯åŠ å¥–åŠ±</span></span><br><span class="line">            mem.store(current_state, action, reward) <span class="comment"># è®°å½•</span></span><br><span class="line">            ep_steps += <span class="number">1</span> <span class="comment"># è®¡ç®—å›åˆæ­¥æ•°</span></span><br><span class="line">            current_state = new_state <span class="comment"># åˆ·æ–°çŠ¶æ€ </span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ep_steps &gt;= <span class="number">500</span> <span class="keyword">or</span> done: <span class="comment"># æœ€é•¿æ­¥æ•°500</span></span><br><span class="line">                <span class="comment"># è®¡ç®—å½“å‰clientä¸Šçš„è¯¯å·®</span></span><br><span class="line">                <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                    total_loss = self.compute_loss(done, new_state, mem) </span><br><span class="line">                <span class="comment"># è®¡ç®—è¯¯å·®</span></span><br><span class="line">                grads = tape.gradient(total_loss, self.client.trainable_weights)</span><br><span class="line">                <span class="comment"># æ¢¯åº¦æäº¤åˆ°serverï¼Œåœ¨serverä¸Šæ›´æ–°æ¢¯åº¦</span></span><br><span class="line">                self.opt.apply_gradients(<span class="built_in">zip</span>(grads,</span><br><span class="line">                                             self.server.trainable_weights))</span><br><span class="line">                <span class="comment"># ä»serveræ‹‰å–æœ€æ–°çš„æ¢¯åº¦</span></span><br><span class="line">                self.client.set_weights(self.server.get_weights())</span><br><span class="line">                mem.clear() <span class="comment"># æ¸…ç©ºMemory </span></span><br><span class="line">                <span class="comment"># ç»Ÿè®¡æ­¤å›åˆå›æŠ¥</span></span><br><span class="line">                self.result_queue.put(ep_reward)</span><br><span class="line">                print(self.worker_idx, ep_reward)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    self.result_queue.put(<span class="literal">None</span>) <span class="comment"># ç»“æŸçº¿ç¨‹</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Actor-Critic è¯¯å·®è®¡ç®—</strong><br />
<img src="https://img-blog.csdnimg.cn/2020120819533547.png" alt=" " /></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 done,</span></span></span><br><span class="line"><span class="function"><span class="params">                 new_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                 memory,</span></span></span><br><span class="line"><span class="function"><span class="params">                 gamma=<span class="number">0.99</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        reward_sum = <span class="number">0.</span> <span class="comment"># ç»ˆæ­¢çŠ¶æ€çš„v(ç»ˆæ­¢)=0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        reward_sum = self.client(tf.constant(new_state[<span class="literal">None</span>, :],</span><br><span class="line">                                 dtype=tf.float32))[-<span class="number">1</span>].numpy()[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># ç»Ÿè®¡æŠ˜æ‰£å›æŠ¥</span></span><br><span class="line">    discounted_rewards = []</span><br><span class="line">    <span class="keyword">for</span> reward <span class="keyword">in</span> memory.rewards[::-<span class="number">1</span>]:  <span class="comment"># reverse buffer r</span></span><br><span class="line">        reward_sum = reward + gamma * reward_sum</span><br><span class="line">        discounted_rewards.append(reward_sum)</span><br><span class="line">    discounted_rewards.reverse()</span><br><span class="line">    <span class="comment"># è·å–çŠ¶æ€çš„Pi(a|s)å’Œv(s)</span></span><br><span class="line">    logits, values = self.client(tf.constant(np.vstack(memory.states),</span><br><span class="line">                             dtype=tf.float32))</span><br><span class="line">    <span class="comment"># è®¡ç®—advantage = R() - v(s)</span></span><br><span class="line">    advantage = tf.constant(np.array(discounted_rewards)[:, <span class="literal">None</span>],</span><br><span class="line">                                     dtype=tf.float32) - values</span><br><span class="line">    <span class="comment"># Criticç½‘ç»œæŸå¤±</span></span><br><span class="line">    value_loss = advantage ** <span class="number">2</span></span><br><span class="line">    <span class="comment"># ç­–ç•¥æŸå¤±</span></span><br><span class="line">    policy = tf.nn.softmax(logits)</span><br><span class="line">    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">                    labels=memory.actions, logits=logits)</span><br><span class="line">    <span class="comment"># è®¡ç®—ç­–ç•¥ç½‘ç»œæŸå¤±æ—¶ï¼Œå¹¶ä¸ä¼šè®¡ç®—Vç½‘ç»œ</span></span><br><span class="line">    policy_loss = policy_loss * tf.stop_gradient(advantage)</span><br><span class="line">    <span class="comment"># Entropy Bonus  labelsæ ‡ç­¾å€¼ï¼ˆçœŸå®å€¼ï¼‰logitsæ¨¡å‹çš„è¾“å‡º</span></span><br><span class="line">    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy,</span><br><span class="line">                                                      logits=logits)</span><br><span class="line">    policy_loss = policy_loss - <span class="number">0.01</span> * entropy</span><br><span class="line">    <span class="comment"># èšåˆå„ä¸ªè¯¯å·®</span></span><br><span class="line">    total_loss = tf.reduce_mean((<span class="number">0.5</span> * value_loss + policy_loss))</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>æ™ºèƒ½ä½“</strong>è´Ÿè´£æ•´ä¸ª A3C ç®—æ³•çš„è®­ç»ƒã€‚åœ¨æ™ºèƒ½ä½“ç±»åˆå§‹åŒ–é˜¶æ®µï¼Œæ–°å»º Global Network å…¨å±€ç½‘ç»œå¯¹è±¡ server å’Œå®ƒçš„ä¼˜åŒ–å™¨å¯¹è±¡ optã€‚<br />
åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œåˆ›å»ºå„ä¸ª Worker çº¿ç¨‹å¯¹è±¡ï¼Œå¹¶å¯åŠ¨å„ä¸ªçº¿ç¨‹å¯¹è±¡ä¸ç¯å¢ƒäº¤äº’ï¼Œæ¯ä¸ª Worker å¯¹è±¡åœ¨äº¤äº’æ—¶å‡ä¼šä» Global Network ä¸­æ‹‰å–æœ€æ–°çš„ç½‘ç»œå‚æ•°ï¼Œå¹¶åˆ©ç”¨æœ€æ–°ç­–ç•¥ä¸ç¯ å¢ƒäº¤äº’ï¼Œè®¡ç®—å„è‡ªæŸå¤±å‡½æ•°ï¼Œæœ€åæäº¤æ¢¯åº¦ä¿¡æ¯ç»™ Global Networkï¼Œè°ƒç”¨ opt å¯¹è±¡å®Œæˆ Global Network çš„ä¼˜åŒ–æ›´æ–°ã€‚</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>:</span></span><br><span class="line">    <span class="comment"># æ™ºèƒ½ä½“ï¼ŒåŒ…å«äº†ä¸­å¤®å‚æ•°ç½‘ç»œserver</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># serverä¼˜åŒ–å™¨ï¼Œclientä¸éœ€è¦ï¼Œç›´æ¥ä»serveræ‹‰å–å‚æ•°</span></span><br><span class="line">        self.opt = optimizers.Adam(<span class="number">1e-3</span>)</span><br><span class="line">        <span class="comment"># ä¸­å¤®æ¨¡å‹ï¼Œç±»ä¼¼äºå‚æ•°æœåŠ¡å™¨</span></span><br><span class="line">        self.server = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># çŠ¶æ€å‘é‡ï¼ŒåŠ¨ä½œæ•°é‡</span></span><br><span class="line">        self.server(tf.random.normal((<span class="number">2</span>, <span class="number">4</span>)))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        res_queue = Queue() <span class="comment"># å…±äº«é˜Ÿåˆ—</span></span><br><span class="line">        <span class="comment"># åˆ›å»ºå„ä¸ªäº¤äº’ç¯å¢ƒ</span></span><br><span class="line">        workers = [Worker(self.server, self.opt, res_queue, i)</span><br><span class="line">                   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(multiprocessing.cpu_count())]</span><br><span class="line">        <span class="keyword">for</span> i, worker <span class="keyword">in</span> <span class="built_in">enumerate</span>(workers):</span><br><span class="line">            print(<span class="string">&quot;Starting worker &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">            worker.start()</span><br><span class="line">        <span class="comment"># ç»Ÿè®¡å¹¶ç»˜åˆ¶æ€»å›æŠ¥æ›²çº¿</span></span><br><span class="line">        returns = []</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            reward = res_queue.get()</span><br><span class="line">            <span class="keyword">if</span> reward <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                returns.append(reward)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># ç»“æŸæ ‡å¿—</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        [w.join() <span class="keyword">for</span> w <span class="keyword">in</span> workers] <span class="comment"># ç­‰å¾…çº¿ç¨‹é€€å‡º </span></span><br><span class="line"></span><br><span class="line">        print(returns)</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(returns)), returns)</span><br><span class="line">        <span class="comment"># plt.plot(np.arange(len(moving_average_rewards)), np.array(moving_average_rewards), &#x27;s&#x27;)</span></span><br><span class="line">        plt.xlabel(<span class="string">&#x27;å›åˆæ•°&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;æ€»å›æŠ¥&#x27;</span>)</span><br><span class="line">        plt.savefig(<span class="string">&#x27;a3c-tf-cartpole.svg&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>å®Œæ•´ä»£ç </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span>  matplotlib</span><br><span class="line"><span class="keyword">from</span>    matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.titlesize&#x27;</span>] = <span class="number">18</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = [<span class="number">9</span>, <span class="number">7</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = [<span class="string">&#x27;KaiTi&#x27;</span>]</span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span>  threading</span><br><span class="line"><span class="keyword">import</span>  gym</span><br><span class="line"><span class="keyword">import</span>  multiprocessing</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span>    queue <span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">import</span>  matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span>    tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span>    tensorflow.keras <span class="keyword">import</span> layers,optimizers,losses</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.random.set_seed(<span class="number">1231</span>)</span><br><span class="line">np.random.seed(<span class="number">1231</span>)</span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;2&#x27;</span></span><br><span class="line"><span class="keyword">assert</span> tf.__version__.startswith(<span class="string">&#x27;2.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorCritic</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="comment"># Actor-Criticæ¨¡å‹</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, state_size, action_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ActorCritic, self).__init__()</span><br><span class="line">        self.state_size = state_size <span class="comment"># çŠ¶æ€å‘é‡é•¿åº¦</span></span><br><span class="line">        self.action_size = action_size <span class="comment"># åŠ¨ä½œæ•°é‡</span></span><br><span class="line">        <span class="comment"># ç­–ç•¥ç½‘ç»œActor</span></span><br><span class="line">        self.dense1 = layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.policy_logits = layers.Dense(action_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Vç½‘ç»œCritic</span></span><br><span class="line">        self.dense2 = layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        self.values = layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="comment"># è·å¾—ç­–ç•¥åˆ†å¸ƒPi(a|s)</span></span><br><span class="line">        x = self.dense1(inputs)</span><br><span class="line">        logits = self.policy_logits(x)</span><br><span class="line">        <span class="comment"># è·å¾—v(s)</span></span><br><span class="line">        v = self.dense2(inputs)</span><br><span class="line">        values = self.values(v)</span><br><span class="line">        <span class="keyword">return</span> logits, values</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">record</span>(<span class="params">episode,</span></span></span><br><span class="line"><span class="function"><span class="params">           episode_reward,</span></span></span><br><span class="line"><span class="function"><span class="params">           worker_idx,</span></span></span><br><span class="line"><span class="function"><span class="params">           global_ep_reward,</span></span></span><br><span class="line"><span class="function"><span class="params">           result_queue,</span></span></span><br><span class="line"><span class="function"><span class="params">           total_loss,</span></span></span><br><span class="line"><span class="function"><span class="params">           num_steps</span>):</span></span><br><span class="line">    <span class="comment"># ç»Ÿè®¡å·¥å…·å‡½æ•°</span></span><br><span class="line">    <span class="keyword">if</span> global_ep_reward == <span class="number">0</span>:</span><br><span class="line">        global_ep_reward = episode_reward</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        global_ep_reward = global_ep_reward * <span class="number">0.99</span> + episode_reward * <span class="number">0.01</span></span><br><span class="line">    print(</span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;episode&#125;</span> | &quot;</span></span><br><span class="line">        <span class="string">f&quot;Average Reward: <span class="subst">&#123;<span class="built_in">int</span>(global_ep_reward)&#125;</span> | &quot;</span></span><br><span class="line">        <span class="string">f&quot;Episode Reward: <span class="subst">&#123;<span class="built_in">int</span>(episode_reward)&#125;</span> | &quot;</span></span><br><span class="line">        <span class="string">f&quot;Loss: <span class="subst">&#123;<span class="built_in">int</span>(total_loss / <span class="built_in">float</span>(num_steps) * <span class="number">1000</span>) / <span class="number">1000</span>&#125;</span> | &quot;</span></span><br><span class="line">        <span class="string">f&quot;Steps: <span class="subst">&#123;num_steps&#125;</span> | &quot;</span></span><br><span class="line">        <span class="string">f&quot;Worker: <span class="subst">&#123;worker_idx&#125;</span>&quot;</span></span><br><span class="line">    )</span><br><span class="line">    result_queue.put(global_ep_reward) <span class="comment"># ä¿å­˜å›æŠ¥ï¼Œä¼ ç»™ä¸»çº¿ç¨‹</span></span><br><span class="line">    <span class="keyword">return</span> global_ep_reward</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Memory</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.states = []</span><br><span class="line">        self.actions = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, state, action, reward</span>):</span></span><br><span class="line">        self.states.append(state)</span><br><span class="line">        self.actions.append(action)</span><br><span class="line">        self.rewards.append(reward)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.states = []</span><br><span class="line">        self.actions = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>:</span></span><br><span class="line">    <span class="comment"># æ™ºèƒ½ä½“ï¼ŒåŒ…å«äº†ä¸­å¤®å‚æ•°ç½‘ç»œserver</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># serverä¼˜åŒ–å™¨ï¼Œclientä¸éœ€è¦ï¼Œç›´æ¥ä»serveræ‹‰å–å‚æ•°</span></span><br><span class="line">        self.opt = optimizers.Adam(<span class="number">1e-3</span>)</span><br><span class="line">        <span class="comment"># ä¸­å¤®æ¨¡å‹ï¼Œç±»ä¼¼äºå‚æ•°æœåŠ¡å™¨</span></span><br><span class="line">        self.server = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># çŠ¶æ€å‘é‡ï¼ŒåŠ¨ä½œæ•°é‡</span></span><br><span class="line">        self.server(tf.random.normal((<span class="number">2</span>, <span class="number">4</span>)))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        res_queue = Queue() <span class="comment"># å…±äº«é˜Ÿåˆ—</span></span><br><span class="line">        <span class="comment"># åˆ›å»ºå„ä¸ªäº¤äº’ç¯å¢ƒ</span></span><br><span class="line">        workers = [Worker(self.server, self.opt, res_queue, i)</span><br><span class="line">                   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(multiprocessing.cpu_count())]</span><br><span class="line">        <span class="keyword">for</span> i, worker <span class="keyword">in</span> <span class="built_in">enumerate</span>(workers):</span><br><span class="line">            print(<span class="string">&quot;Starting worker &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">            worker.start()</span><br><span class="line">        <span class="comment"># ç»Ÿè®¡å¹¶ç»˜åˆ¶æ€»å›æŠ¥æ›²çº¿</span></span><br><span class="line">        returns = []</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            reward = res_queue.get()</span><br><span class="line">            <span class="keyword">if</span> reward <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                returns.append(reward)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># ç»“æŸæ ‡å¿—</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        [w.join() <span class="keyword">for</span> w <span class="keyword">in</span> workers] <span class="comment"># ç­‰å¾…çº¿ç¨‹é€€å‡º </span></span><br><span class="line"></span><br><span class="line">        print(returns)</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(returns)), returns)</span><br><span class="line">        <span class="comment"># plt.plot(np.arange(len(moving_average_rewards)), np.array(moving_average_rewards), &#x27;s&#x27;)</span></span><br><span class="line">        plt.xlabel(<span class="string">&#x27;å›åˆæ•°&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;æ€»å›æŠ¥&#x27;</span>)</span><br><span class="line">        plt.savefig(<span class="string">&#x27;a3c-tf-cartpole.svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span>(<span class="params">threading.Thread</span>):</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,  server, opt, result_queue, idx</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Worker, self).__init__()</span><br><span class="line">        self.result_queue = result_queue <span class="comment"># å…±äº«é˜Ÿåˆ—</span></span><br><span class="line">        self.server = server <span class="comment"># ä¸­å¤®æ¨¡å‹</span></span><br><span class="line">        self.opt = opt <span class="comment"># ä¸­å¤®ä¼˜åŒ–å™¨</span></span><br><span class="line">        self.client = ActorCritic(<span class="number">4</span>, <span class="number">2</span>) <span class="comment"># çº¿ç¨‹ç§æœ‰ç½‘ç»œ</span></span><br><span class="line">        self.worker_idx = idx <span class="comment"># çº¿ç¨‹id</span></span><br><span class="line">        self.env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>).unwrapped</span><br><span class="line">        self.ep_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span> </span><br><span class="line">        mem = Memory() <span class="comment"># æ¯ä¸ªworkerè‡ªå·±ç»´æŠ¤ä¸€ä¸ªmemory</span></span><br><span class="line">        <span class="keyword">for</span> epi_counter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>): <span class="comment"># æœªè¾¾åˆ°æœ€å¤§å›åˆæ•°</span></span><br><span class="line">            current_state = self.env.reset() <span class="comment"># å¤ä½clientæ¸¸æˆçŠ¶æ€</span></span><br><span class="line">            mem.clear()</span><br><span class="line">            ep_reward = <span class="number">0.</span></span><br><span class="line">            ep_steps = <span class="number">0</span>  </span><br><span class="line">            done = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">                <span class="comment"># è·å¾—Pi(a|s),æœªç»softmax</span></span><br><span class="line">                logits, _ = self.client(tf.constant(current_state[<span class="literal">None</span>, :],</span><br><span class="line">                                         dtype=tf.float32))</span><br><span class="line">                probs = tf.nn.softmax(logits)</span><br><span class="line">                <span class="comment"># éšæœºé‡‡æ ·åŠ¨ä½œ</span></span><br><span class="line">                action = np.random.choice(<span class="number">2</span>, p=probs.numpy()[<span class="number">0</span>])</span><br><span class="line">                new_state, reward, done, _ = self.env.step(action) <span class="comment"># äº¤äº’ </span></span><br><span class="line">                ep_reward += reward <span class="comment"># ç´¯åŠ å¥–åŠ±</span></span><br><span class="line">                mem.store(current_state, action, reward) <span class="comment"># è®°å½•</span></span><br><span class="line">                ep_steps += <span class="number">1</span> <span class="comment"># è®¡ç®—å›åˆæ­¥æ•°</span></span><br><span class="line">                current_state = new_state <span class="comment"># åˆ·æ–°çŠ¶æ€ </span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> ep_steps &gt;= <span class="number">500</span> <span class="keyword">or</span> done: <span class="comment"># æœ€é•¿æ­¥æ•°500</span></span><br><span class="line">                    <span class="comment"># è®¡ç®—å½“å‰clientä¸Šçš„è¯¯å·®</span></span><br><span class="line">                    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">                        total_loss = self.compute_loss(done, new_state, mem) </span><br><span class="line">                    <span class="comment"># è®¡ç®—è¯¯å·®</span></span><br><span class="line">                    grads = tape.gradient(total_loss, self.client.trainable_weights)</span><br><span class="line">                    <span class="comment"># æ¢¯åº¦æäº¤åˆ°serverï¼Œåœ¨serverä¸Šæ›´æ–°æ¢¯åº¦</span></span><br><span class="line">                    self.opt.apply_gradients(<span class="built_in">zip</span>(grads,</span><br><span class="line">                                                 self.server.trainable_weights))</span><br><span class="line">                    <span class="comment"># ä»serveræ‹‰å–æœ€æ–°çš„æ¢¯åº¦</span></span><br><span class="line">                    self.client.set_weights(self.server.get_weights())</span><br><span class="line">                    mem.clear() <span class="comment"># æ¸…ç©ºMemory </span></span><br><span class="line">                    <span class="comment"># ç»Ÿè®¡æ­¤å›åˆå›æŠ¥</span></span><br><span class="line">                    self.result_queue.put(ep_reward)</span><br><span class="line">                    print(self.worker_idx, ep_reward)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        self.result_queue.put(<span class="literal">None</span>) <span class="comment"># ç»“æŸçº¿ç¨‹</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                     done,</span></span></span><br><span class="line"><span class="function"><span class="params">                     new_state,</span></span></span><br><span class="line"><span class="function"><span class="params">                     memory,</span></span></span><br><span class="line"><span class="function"><span class="params">                     gamma=<span class="number">0.99</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward_sum = <span class="number">0.</span> <span class="comment"># ç»ˆæ­¢çŠ¶æ€çš„v(ç»ˆæ­¢)=0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            reward_sum = self.client(tf.constant(new_state[<span class="literal">None</span>, :],</span><br><span class="line">                                     dtype=tf.float32))[-<span class="number">1</span>].numpy()[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># ç»Ÿè®¡æŠ˜æ‰£å›æŠ¥</span></span><br><span class="line">        discounted_rewards = []</span><br><span class="line">        <span class="keyword">for</span> reward <span class="keyword">in</span> memory.rewards[::-<span class="number">1</span>]:  <span class="comment"># reverse buffer r</span></span><br><span class="line">            reward_sum = reward + gamma * reward_sum</span><br><span class="line">            discounted_rewards.append(reward_sum)</span><br><span class="line">        discounted_rewards.reverse()</span><br><span class="line">        <span class="comment"># è·å–çŠ¶æ€çš„Pi(a|s)å’Œv(s)</span></span><br><span class="line">        logits, values = self.client(tf.constant(np.vstack(memory.states),</span><br><span class="line">                                 dtype=tf.float32))</span><br><span class="line">        <span class="comment"># è®¡ç®—advantage = R() - v(s)</span></span><br><span class="line">        advantage = tf.constant(np.array(discounted_rewards)[:, <span class="literal">None</span>],</span><br><span class="line">                                         dtype=tf.float32) - values</span><br><span class="line">        <span class="comment"># Criticç½‘ç»œæŸå¤±</span></span><br><span class="line">        value_loss = advantage ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># ç­–ç•¥æŸå¤±</span></span><br><span class="line">        policy = tf.nn.softmax(logits)</span><br><span class="line">        policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">                        labels=memory.actions, logits=logits)</span><br><span class="line">        <span class="comment"># è®¡ç®—ç­–ç•¥ç½‘ç»œæŸå¤±æ—¶ï¼Œå¹¶ä¸ä¼šè®¡ç®—Vç½‘ç»œ</span></span><br><span class="line">        policy_loss = policy_loss * tf.stop_gradient(advantage)</span><br><span class="line">        <span class="comment"># Entropy Bonus  labelsæ ‡ç­¾å€¼ï¼ˆçœŸå®å€¼ï¼‰logitsæ¨¡å‹çš„è¾“å‡º</span></span><br><span class="line">        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy,</span><br><span class="line">                                                          logits=logits)</span><br><span class="line">        policy_loss = policy_loss - <span class="number">0.01</span> * entropy</span><br><span class="line">        <span class="comment"># èšåˆå„ä¸ªè¯¯å·®</span></span><br><span class="line">        total_loss = tf.reduce_mean((<span class="number">0.5</span> * value_loss + policy_loss))</span><br><span class="line">        <span class="keyword">return</span> total_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    master = Agent()</span><br><span class="line">    master.train()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">æ·±åº¦å­¦ä¹ æ¡†æ¶</a><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/tensorflow/">tensorflow</a></div><div class="post_share"><div class="social-share" data-image="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/12/09/Pytorch/Pytorch-Introductory-knowledge/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">PyTorchå…¥é—¨çŸ¥è¯†</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/07/ReinforcementLearning/Reinforcement-Learning-Basic-Theory/"><img class="next-cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">å¼ºåŒ–å­¦ä¹ åŸºç¡€ç†è®º</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/2020/12/06/Tensorflow/Tensorflow-and-Encoder-Decoder/" title="Tensorflowä¸è‡ªç¼–ç å™¨"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-06</div><div class="title">Tensorflowä¸è‡ªç¼–ç å™¨</div></div></a></div><div><a href="/2020/12/06/Tensorflow/Tensorflow-and-Recurrent-neural-network/" title="Tensorflowä¸å¾ªç¯ç¥ç»ç½‘ç»œ"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-06</div><div class="title">Tensorflowä¸å¾ªç¯ç¥ç»ç½‘ç»œ</div></div></a></div><div><a href="/2020/12/04/Tensorflow/Tensorflow-and-Convolutional-Neural-Network/" title="Tensorflowä¸å·ç§¯ç¥ç»ç½‘ç»œ"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-04</div><div class="title">Tensorflowä¸å·ç§¯ç¥ç»ç½‘ç»œ</div></div></a></div><div><a href="/2020/12/04/Tensorflow/Tensorflow-Keras-high-level-interface/" title="Tensorflowä¸­Keras é«˜å±‚æ¥å£"><img class="cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-04</div><div class="title">Tensorflowä¸­Keras é«˜å±‚æ¥å£</div></div></a></div><div><a href="/2020/12/04/Tensorflow/Tensorflow-and-Neural-Networks/" title="Tensorflowæ„å»ºç®€å•ç¥ç»ç½‘ç»œ"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-04</div><div class="title">Tensorflowæ„å»ºç®€å•ç¥ç»ç½‘ç»œ</div></div></a></div><div><a href="/2020/12/03/Tensorflow/Tensorflow-advanced-knowledge/" title="Tensorflow2.0è¿›é˜¶çŸ¥è¯†"><img class="cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-03</div><div class="title">Tensorflow2.0è¿›é˜¶çŸ¥è¯†</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ccclll777</div><div class="author-info__description">èƒ¸æ€€çŒ›è™ ç»†å—…è”·è–‡</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">45</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">26</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">9</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ccclll777"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccclll777" target="_blank" title="fab fa-github"><i class="GitHub"></i></a><a class="social-icon" href="mailto:sdu945860882@gmail.com" target="_blank" title="fa fa-envelope"><i class="E-Mail"></i></a><a class="social-icon" href="https://www.weibo.com/6732062654" target="_blank" title="fab fa-weibo"><i class="Weibo"></i></a><a class="social-icon" href="https://blog.csdn.net/baidu_41871794" target="_blank" title="gratipay"><i class="CSDN"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E4%BE%8B"><span class="toc-number">1.</span> <span class="toc-text"> å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ä¾‹</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B3%E8%A1%A1%E6%9D%86%E6%B8%B8%E6%88%8F"><span class="toc-number">1.1.</span> <span class="toc-text"> å¹³è¡¡æ†æ¸¸æˆ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gym-%E5%B9%B3%E5%8F%B0"><span class="toc-number">1.2.</span> <span class="toc-text"> Gym å¹³å°</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.</span> <span class="toc-text"> ç­–ç•¥ç½‘ç»œ</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95policy-gradient"><span class="toc-number">2.</span> <span class="toc-text"> ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼ˆPolicy Gradient ï¼‰</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#ppo-%E7%AE%97%E6%B3%95"><span class="toc-number">2.1.</span> <span class="toc-text"> PPO ç®—æ³•</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text"> å€¼å‡½æ•°æ–¹æ³•</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text"> å€¼å‡½æ•°</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%B0%E8%AE%A1"><span class="toc-number">3.2.</span> <span class="toc-text"> å€¼å‡½æ•°çš„ä¼°è®¡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B"><span class="toc-number">3.3.</span> <span class="toc-text"> ç­–ç•¥æ”¹è¿›</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dqn-%E7%AE%97%E6%B3%95"><span class="toc-number">3.4.</span> <span class="toc-text"> DQN ç®—æ³•</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dqn-%E5%AE%9E%E6%88%98"><span class="toc-number">3.5.</span> <span class="toc-text"> DQN å®æˆ˜</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#actor-critic-%E6%96%B9%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text"> Actor-Critic æ–¹æ³•</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#advantage-ac-%E7%AE%97%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text"> Advantage AC ç®—æ³•</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#a3c-%E7%AE%97%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text"> A3C ç®—æ³•</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#a3c-%E5%AE%9E%E6%88%98"><span class="toc-number">4.3.</span> <span class="toc-text"> A3C å®æˆ˜</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>æœ€æ–°æ–‡ç« </span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. ç»™è¡¨è¾¾å¼æ·»åŠ è¿ç®—ç¬¦"><img src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Leetcode 282. ç»™è¡¨è¾¾å¼æ·»åŠ è¿ç®—ç¬¦"/></a><div class="content"><a class="title" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. ç»™è¡¨è¾¾å¼æ·»åŠ è¿ç®—ç¬¦">Leetcode 282. ç»™è¡¨è¾¾å¼æ·»åŠ è¿ç®—ç¬¦</a><time datetime="2021-10-16T15:35:16.000Z" title="å‘è¡¨äº 2021-10-16 23:35:16">2021-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°"><img src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°"/></a><div class="content"><a class="title" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°">Pytorchå¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°</a><time datetime="2020-12-12T02:54:37.000Z" title="å‘è¡¨äº 2020-12-12 10:54:37">2020-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—">PyTorchå¸¸ç”¨å·¥å…·æ¨¡å—</a><time datetime="2020-12-09T13:32:23.000Z" title="å‘è¡¨äº 2020-12-09 21:32:23">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorchä¸­ç¥ç»ç½‘ç»œå·¥å…·ç®±nnæ¨¡å—"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorchä¸­ç¥ç»ç½‘ç»œå·¥å…·ç®±nnæ¨¡å—"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorchä¸­ç¥ç»ç½‘ç»œå·¥å…·ç®±nnæ¨¡å—">Pytorchä¸­ç¥ç»ç½‘ç»œå·¥å…·ç®±nnæ¨¡å—</a><time datetime="2020-12-09T13:25:38.000Z" title="å‘è¡¨äº 2020-12-09 21:25:38">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorchä¸­çš„Autograd"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorchä¸­çš„Autograd"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorchä¸­çš„Autograd">Pytorchä¸­çš„Autograd</a><time datetime="2020-12-09T13:25:19.000Z" title="å‘è¡¨äº 2020-12-09 21:25:19">2020-12-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By ccclll777</div><div class="framework-info"><span>æ¡†æ¶ </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>ä¸»é¢˜ </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="ç®€ç¹è½¬æ¢">ç®€</button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>