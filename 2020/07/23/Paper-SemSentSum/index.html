<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="多文档摘要论文复现">
<meta property="og:type" content="article">
<meta property="og:title" content="多文档摘要">
<meta property="og:url" content="http://yoursite.com/2020/07/23/Paper-SemSentSum/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="多文档摘要论文复现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/202007231606529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020072316103727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200723161135460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200723162557845.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200723163050742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729175411378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200723163451721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729175639695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200723164311128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729202514147.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729203547914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729203558981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729203628762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729203718684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729203759813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729202514147.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729210107803.png">
<meta property="article:published_time" content="2020-07-23T09:20:08.000Z">
<meta property="article:modified_time" content="2020-09-27T07:47:18.000Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="多文档摘要">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/202007231606529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">

<link rel="canonical" href="http://yoursite.com/2020/07/23/Paper-SemSentSum/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>多文档摘要 | ccclll777's blogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
<a href="https://github.com/ccclll777" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ccclll777's blogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/23/Paper-SemSentSum/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ccclll777">
      <meta itemprop="description" content="胸怀猛虎 细嗅蔷薇">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ccclll777's blogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          多文档摘要
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-23 17:20:08" itemprop="dateCreated datePublished" datetime="2020-07-23T17:20:08+08:00">2020-07-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-27 15:47:18" itemprop="dateModified" datetime="2020-09-27T15:47:18+08:00">2020-09-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/" itemprop="url" rel="index"><span itemprop="name">论文复现</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>33k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>30 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>多文档摘要论文复现<br> <a id="more"></a></p>
<h2 id="一-数据加载"><a href="#一-数据加载" class="headerlink" title="一.数据加载"></a>一.数据加载</h2><p> <strong>- 1.数据集说明</strong><br>  文章采用了Document Understanding Conferences（DUC）上针对最常用的多文档摘要数据集，其中DUC 2001/2002用于训练，DUC 2003用于验证，最后DUC 2004用于测试。</p>
<blockquote>
<p>数据集需要在<a href="https://www-nlpir.nist.gov/projects/duc/guidelines.html进行申请" target="_blank" rel="noopener">https://www-nlpir.nist.gov/projects/duc/guidelines.html进行申请</a></p>
</blockquote>
<p> <strong>- 2.读取数据</strong><br> 首先，从文档中读取数据，然后切分数据集，由于机器的性能，我没有读取全部的数据集，只读取了部分的数据集进行实验。我是用正则表达式，从对应的文档中读取了文档的正文以及文档对应的参考摘要。<br> 格式为{doc_no : 文档内容}和{doc_no : 参考摘要}</p>
<p> <strong>- 3.切分句子</strong><br> 读取完数据之后，需要将文档切分成句子，存储格式为{doc_no :{sen_id:句子1 ，sen_id2 :句子2 }}</p>
<p> <strong>- 4.将切分完的句子建立索引</strong><br> 建立（index-&gt;句子) 的字典，格式为{index1 : doc_no#sen_Id1，index2 : doc_no#sen_Id2}<br>  建立（句子-&gt;index) 的字典，格式为{doc_no#sen_Id1 :index1，doc_no#sen_Id2 : index2}<br>  建立 （文档-&gt;index）的字典，格式为 {doc_no :  [index1,index2…] }<br>  将训练集，验证集，测试集对应的index存储到对应的列表中。</p>
<p> <strong>- 5.代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding = utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> util</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoadData</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.data_set = &#123;&#125;</span><br><span class="line">        self.data_set_summary = &#123;&#125;</span><br><span class="line">        self.current_path = os.path.abspath(os.path.dirname(__file__))  <span class="comment"># 当前文件路径</span></span><br><span class="line">        self.train_set = &#123;&#125;  <span class="comment"># 训练集</span></span><br><span class="line">        self.validation_set = &#123;&#125;  <span class="comment"># 验证集</span></span><br><span class="line">        self.test_set = &#123;&#125;  <span class="comment"># 测试集</span></span><br><span class="line"></span><br><span class="line">        self.train_set_len = <span class="number">0</span></span><br><span class="line">        self.validation_set_len = <span class="number">0</span></span><br><span class="line">        self.test_set_len = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.train_set_sentences = &#123;&#125;  <span class="comment"># 训练集  将文档切分成了句子 存储格式为 &#123;doc_no :&#123;sen_id:句子1 ,sen_id2 :句子2 &#125;&#125;</span></span><br><span class="line">        self.validation_set_sentences = &#123;&#125;  <span class="comment"># 验证集</span></span><br><span class="line">        self.test_set_sentences = &#123;&#125;  <span class="comment"># 测试集</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#数据集</span></span><br><span class="line">        self.index_to_sentence = &#123;&#125;  <span class="comment"># 下标与 文档句子的映射</span></span><br><span class="line"></span><br><span class="line">        self.sentence_to_index = &#123;&#125;  <span class="comment"># 文档中的句子与下标的映射</span></span><br><span class="line">        <span class="comment"># 训练集</span></span><br><span class="line">        self.train_set_sencence_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.train_set_sentences_list = &#123;&#125;  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line"></span><br><span class="line">        self.train_index = []  <span class="comment"># 训练集的所有下标</span></span><br><span class="line">        <span class="comment"># 验证集</span></span><br><span class="line">        self.validation_set_sencence_count = <span class="number">0</span></span><br><span class="line">        self.validation_set_sentences_list = &#123;&#125;  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line"></span><br><span class="line">        self.validation_index = []  <span class="comment"># 训练集的所有下标</span></span><br><span class="line">        <span class="comment"># 测试集</span></span><br><span class="line">        self.test_set_sencence_count = <span class="number">0</span></span><br><span class="line">        self.test_set_sentences_list = &#123;&#125;  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line"></span><br><span class="line">        self.test_index = []  <span class="comment"># 训练集的所有下标</span></span><br><span class="line"></span><br><span class="line">        self.train_set_summary = &#123;&#125;  <span class="comment"># 训练集对应的 参考摘要</span></span><br><span class="line">        self.validation_set_summary = &#123;&#125;  <span class="comment"># 验证集对应的 参考摘要</span></span><br><span class="line">        self.test_set_summary = &#123;&#125;  <span class="comment"># 测试集对应的 参考摘要</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(self)</span>:</span></span><br><span class="line">        DUC2001_path = self.current_path + <span class="string">'/data/DUC/DUC2001_Summarization_Documents/data/training'</span>  <span class="comment"># 待读取文件的文件夹地址</span></span><br><span class="line">        DUC2001_files = os.listdir(DUC2001_path)  <span class="comment"># 获得文件夹中所有文件的名称列表</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> DUC2001_files:</span><br><span class="line">            <span class="keyword">if</span> os.path.isdir(DUC2001_path + <span class="string">"/"</span> + file):  <span class="comment"># 判断是否是文件夹</span></span><br><span class="line">                doc_path_list = DUC2001_path + <span class="string">"/"</span> + file + <span class="string">"/docs"</span></span><br><span class="line">                summary_path = DUC2001_path + <span class="string">"/"</span> + file + <span class="string">"/"</span> + file + str(file)[<span class="number">-1</span>]</span><br><span class="line">                <span class="keyword">for</span> doc_path <span class="keyword">in</span> os.listdir(doc_path_list):</span><br><span class="line">                    <span class="keyword">if</span> os.path.isfile(doc_path_list + <span class="string">"/"</span> + doc_path):</span><br><span class="line">                        self.get_data(doc_path_list + <span class="string">"/"</span> + doc_path, summary_path + <span class="string">"/perdocs"</span>)</span><br><span class="line">        DUC2001_path_test = self.current_path + <span class="string">'/data/DUC/DUC2001_Summarization_Documents/data/test'</span>  <span class="comment"># 待读取文件的文件夹地址</span></span><br><span class="line">        DUC2001_files_test = os.listdir(DUC2001_path_test + <span class="string">"/docs"</span>)  <span class="comment"># 获得文件夹中所有文件的名称列表</span></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> DUC2001_files_test:</span><br><span class="line">            <span class="keyword">if</span> os.path.isdir(DUC2001_path_test + <span class="string">"/docs/"</span> + file):  <span class="comment"># 判断是否是文件夹</span></span><br><span class="line">                doc_path_list = DUC2001_path_test + <span class="string">"/docs/"</span> + file</span><br><span class="line">                summary_path = DUC2001_path_test + <span class="string">"/original.summaries/"</span> + file + str(file)[<span class="number">-1</span>]</span><br><span class="line">                <span class="keyword">for</span> doc_path <span class="keyword">in</span> os.listdir(doc_path_list):</span><br><span class="line">                    <span class="keyword">if</span> os.path.isfile(doc_path_list + <span class="string">"/"</span> + doc_path):</span><br><span class="line">                        self.get_data(doc_path_list + <span class="string">"/"</span> + doc_path, summary_path + <span class="string">"/perdocs"</span>)</span><br><span class="line"></span><br><span class="line">        DUC2001_path_testtraining = self.current_path + <span class="string">'/data/DUC/DUC2001_Summarization_Documents/data/testtraining/duc2002testtraining'</span>  <span class="comment"># 待读取文件的文件夹地址</span></span><br><span class="line">        DUC2001_files_testtraining = os.listdir(DUC2001_path_testtraining)  <span class="comment"># 获得文件夹中所有文件的名称列表</span></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> DUC2001_files_testtraining:</span><br><span class="line">            <span class="keyword">if</span> os.path.isdir(DUC2001_path_testtraining + <span class="string">"/"</span> + file):  <span class="comment"># 判断是否是文件夹</span></span><br><span class="line">                path_list = os.listdir(DUC2001_path_testtraining + <span class="string">"/"</span> + file)</span><br><span class="line">                <span class="keyword">for</span> path <span class="keyword">in</span> path_list:</span><br><span class="line">                    doc_no = path</span><br><span class="line">                    fr_doc = open(DUC2001_path_testtraining + <span class="string">"/"</span> + file + <span class="string">"/"</span> + path + <span class="string">"/"</span> + path + <span class="string">".body"</span>,</span><br><span class="line">                                  encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">                    content = fr_doc.read()</span><br><span class="line">                    content = content.replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">                    self.data_set[doc_no] = content</span><br><span class="line">                    fr_summary = open(DUC2001_path_testtraining + <span class="string">"/"</span> + file + <span class="string">"/"</span> + path + <span class="string">"/"</span> + path + <span class="string">".abs"</span>,</span><br><span class="line">                                      encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">                    summary = fr_summary.read()</span><br><span class="line">                    summary = summary.replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">                    self.data_set_summary[doc_no] = summary</span><br><span class="line">        length = len(self.data_set)</span><br><span class="line">        self.train_set_len = int(length * <span class="number">0.8</span>)</span><br><span class="line">        self.validation_set_len = int(length * <span class="number">0.1</span>)</span><br><span class="line">        self.test_set_len = int(length * <span class="number">0.1</span>)</span><br><span class="line">        <span class="comment"># 切分读取的数据为训练集 验证集 测试集</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> doc_no,content <span class="keyword">in</span> self.data_set.items():</span><br><span class="line">            <span class="keyword">if</span> index &lt; self.train_set_len:</span><br><span class="line">                self.train_set[doc_no] = content</span><br><span class="line">                self.train_set_summary[doc_no] = self.data_set_summary[doc_no]</span><br><span class="line">            <span class="keyword">elif</span> index &gt; self.train_set_len <span class="keyword">and</span> index&lt;self.train_set_len+self.validation_set_len:</span><br><span class="line">                self.validation_set[doc_no] = content</span><br><span class="line">                self.validation_set_summary[doc_no] = self.data_set_summary[doc_no]</span><br><span class="line">            <span class="keyword">elif</span> index &gt; self.train_set_len+self.validation_set_len <span class="keyword">and</span> index &lt; length:</span><br><span class="line">                self.test_set[doc_no] = content</span><br><span class="line">                self.test_set_summary[doc_no] = self.data_set_summary[doc_no]</span><br><span class="line">            index +=<span class="number">1</span></span><br><span class="line">      <span class="string">"""利用正则表达式，提取文档内容和文档的摘要"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(self, doc_path, summary_path)</span>:</span></span><br><span class="line">        fr = open(doc_path, <span class="string">"r"</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">        content = fr.read()</span><br><span class="line">        content = content.replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">        doc = re.findall(<span class="string">"&lt;TEXT&gt;(.*?)&lt;/TEXT&gt;"</span>, content)[<span class="number">0</span>]</span><br><span class="line">        doc_no = re.findall(<span class="string">"&lt;DOCNO&gt;(.*?)&lt;/DOCNO&gt;"</span>, content)[<span class="number">0</span>].replace(<span class="string">" "</span>, <span class="string">""</span>)</span><br><span class="line">        self.data_set[doc_no] = doc.replace(<span class="string">"&lt;p&gt;"</span>, <span class="string">""</span>).replace(<span class="string">"&lt;/p&gt;"</span>, <span class="string">""</span>).replace(<span class="string">"&lt;P&gt;"</span>, <span class="string">""</span>).replace(</span><br><span class="line">            <span class="string">"&lt;/P&gt;"</span>, <span class="string">""</span>)</span><br><span class="line">        fr = open(summary_path, <span class="string">"r"</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">        summary_list = fr.read()</span><br><span class="line">        summary_list = summary_list.replace(<span class="string">"\n"</span>, <span class="string">""</span>)</span><br><span class="line">        summary = re.findall(<span class="string">'&lt;SUM.*?DOCREF="&#123;&#125;.*?"&gt;(.*?)&lt;/SUM&gt;'</span>.format(doc_no), summary_list)[<span class="number">0</span>]</span><br><span class="line">        self.data_set_summary[doc_no] = summary</span><br><span class="line"></span><br><span class="line">    <span class="string">"""将文档切分成句子"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut_doc_to_sentences</span><span class="params">(self, set=<span class="string">"train_set"</span>)</span>:</span></span><br><span class="line">        sentences_count = <span class="number">0</span>  <span class="comment"># 句子数量</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> set == <span class="string">"train_set"</span>:</span><br><span class="line">            <span class="comment"># 读取数据</span></span><br><span class="line">            <span class="keyword">for</span> doc_no, doc_content <span class="keyword">in</span> self.train_set.items():</span><br><span class="line">                <span class="comment"># 将文档切分成句子</span></span><br><span class="line">                sentence_list = util.cut_doc_to_sentences(doc_content)</span><br><span class="line">                <span class="keyword">if</span> (len(sentence_list)) == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                document_dict = &#123;&#125;</span><br><span class="line">                sentences_count += len(sentence_list)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(sentence_list)):</span><br><span class="line">                    <span class="comment"># 将句子编号 然后存入字典中</span></span><br><span class="line">                    document_dict[<span class="string">"sen_id_"</span> + str(i)] = sentence_list[i]</span><br><span class="line">                self.train_set_sentences[doc_no] = document_dict</span><br><span class="line">            self.train_set_sencence_count = sentences_count</span><br><span class="line">        <span class="keyword">elif</span> set == <span class="string">"validation_set"</span>:</span><br><span class="line">            <span class="keyword">for</span> doc_no, doc_content <span class="keyword">in</span> self.validation_set.items():</span><br><span class="line">                sentence_list = util.cut_doc_to_sentences(doc_content)</span><br><span class="line">                <span class="keyword">if</span> (len(sentence_list)) == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                document_dict = &#123;&#125;</span><br><span class="line">                sentences_count += len(sentence_list)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(sentence_list)):</span><br><span class="line">                    document_dict[<span class="string">"sen_id_"</span> + str(i)] = sentence_list[i]</span><br><span class="line">                self.validation_set_sentences[doc_no] = document_dict</span><br><span class="line">            self.validation_set_sencence_count = sentences_count</span><br><span class="line">        <span class="keyword">elif</span> set == <span class="string">"test_set"</span>:</span><br><span class="line">            <span class="keyword">for</span> doc_no, doc_content <span class="keyword">in</span> self.test_set.items():</span><br><span class="line">                sentence_list = util.cut_doc_to_sentences(doc_content)</span><br><span class="line">                <span class="keyword">if</span> (len(sentence_list)) == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                document_dict = &#123;&#125;</span><br><span class="line">                sentences_count += len(sentence_list)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(sentence_list)):</span><br><span class="line">                    document_dict[<span class="string">"sen_id_"</span> + str(i)] = sentence_list[i]</span><br><span class="line">                self.test_set_sentences[doc_no] = document_dict</span><br><span class="line">            self.test_set_sencence_count = sentences_count</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_index</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 构造句子和矩阵下标的映射</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> doc_no, sentences <span class="keyword">in</span> self.train_set_sentences.items():</span><br><span class="line">            <span class="comment"># 遍历文档中的每个句子</span></span><br><span class="line">            sentence_index_list = []</span><br><span class="line">            <span class="keyword">for</span> sen_id, sentence <span class="keyword">in</span> sentences.items():</span><br><span class="line">                self.index_to_sentence[index] = doc_no + <span class="string">"#"</span> + sen_id</span><br><span class="line">                self.sentence_to_index[doc_no + <span class="string">"#"</span> + sen_id] = index</span><br><span class="line">                sentence_index_list.append(index)</span><br><span class="line">                self.train_index.append(index)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            self.train_set_sentences_list[doc_no] = sentence_index_list</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构造句子和矩阵下标的映射</span></span><br><span class="line">        <span class="keyword">for</span> doc_no, sentences <span class="keyword">in</span> self.validation_set_sentences.items():</span><br><span class="line">            <span class="comment"># 遍历文档中的每个句子</span></span><br><span class="line">            sentence_index_list = []</span><br><span class="line">            <span class="keyword">for</span> sen_id, sentence <span class="keyword">in</span> sentences.items():</span><br><span class="line">                self.index_to_sentence[index] = doc_no + <span class="string">"#"</span> + sen_id</span><br><span class="line">                self.sentence_to_index[doc_no + <span class="string">"#"</span> + sen_id] = index</span><br><span class="line">                sentence_index_list.append(index)</span><br><span class="line">                self.validation_index.append(index)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">            self.validation_set_sentences_list[doc_no] = sentence_index_list</span><br><span class="line">        <span class="comment"># 构造句子和矩阵下标的映射</span></span><br><span class="line">        <span class="keyword">for</span> doc_no, sentences <span class="keyword">in</span> self.test_set_sentences.items():</span><br><span class="line">            <span class="comment"># 遍历文档中的每个句子</span></span><br><span class="line">            sentence_index_list = []</span><br><span class="line">            <span class="keyword">for</span> sen_id, sentence <span class="keyword">in</span> sentences.items():</span><br><span class="line">                self.index_to_sentence[index] = doc_no + <span class="string">"#"</span> + sen_id</span><br><span class="line">                self.sentence_to_index[doc_no + <span class="string">"#"</span> + sen_id] = index</span><br><span class="line">                sentence_index_list.append(index)</span><br><span class="line">                self.test_index.append(index)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">            self.test_set_sentences_list[doc_no] = sentence_index_list</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.read_data()</span><br><span class="line">        self.cut_doc_to_sentences(set=<span class="string">"train_set"</span>)</span><br><span class="line">        self.cut_doc_to_sentences(set=<span class="string">"validation_set"</span>)</span><br><span class="line">        self.cut_doc_to_sentences(set=<span class="string">"test_set"</span>)</span><br><span class="line">        self.create_index()</span><br></pre></td></tr></table></figure>





<h2 id="二-构建句子的语义关系图（Sentence-Semantic-Relation-Graph）"><a href="#二-构建句子的语义关系图（Sentence-Semantic-Relation-Graph）" class="headerlink" title="二.构建句子的语义关系图（Sentence Semantic Relation Graph）"></a>二.构建句子的语义关系图（Sentence Semantic Relation Graph）</h2><ul>
<li><strong>1.解释：</strong></li>
</ul>
<p>（1）用图对句子建模，图的顶点为文档i中的句子j（Si，j），边为两个句子之间的相似程度。需要使用英语Wikipedia语料库上训练的的模型进行句子嵌入（sentence embeddings），产生句子的向量，然后根据向量计算句子之间的余弦相似度，然后构建矩阵。<br>（2）引入一个阈值t，去除相似度小于阈值t的边，以强调较高的句子相似度，避免模型无法显著地利用句子之间的语义结构</p>
<ul>
<li><strong>2.sentence embeddings环境安装工作</strong></li>
</ul>
<p>我找到了（Pagliardini et al. (2018)）的论文中描述的模型的开源实现，他可以在fasttext库的基础上，进行句子嵌入。</p>
<blockquote>
<p>sent2vec模型地址：<a href="https://github.com/epfml/sent2vec" target="_blank" rel="noopener">https://github.com/epfml/sent2vec</a></p>
</blockquote>
<p>（1）由于他基于fasttext，所以先下载并编译了fasttext，具体方法他在github中写的非常清楚，在他们目录文件中写好了Makefile，直接用本地的gcc环境进行编译</p>
<blockquote>
<p><a href="https://github.com/facebookresearch/fastText" target="_blank" rel="noopener">https://github.com/facebookresearch/fastText</a></p>
</blockquote>
<p>（2）下载论文中描述的的wiki百科600维的预训练unigram模型，在fasttext中进行使用</p>
<blockquote>
<p><a href="https://drive.google.com/uc?id=0B6VhzidiLvjSa19uYWlLUEkzX3c&amp;export=download" target="_blank" rel="noopener">https://drive.google.com/uc?id=0B6VhzidiLvjSa19uYWlLUEkzX3c&amp;export=download</a></p>
</blockquote>
<p>（3）下载了斯坦福解析器，配合nltk库进行Tokenizer</p>
<blockquote>
<p>解析器下载：<a href="https://nlp.stanford.edu/software/lex-parser.shtml#Download" target="_blank" rel="noopener">https://nlp.stanford.edu/software/lex-parser.shtml#Download</a><br>使用教程：<a href="https://blog.csdn.net/qq_36652619/article/details/75091327" target="_blank" rel="noopener">https://blog.csdn.net/qq_36652619/article/details/75091327</a></p>
</blockquote>
<ul>
<li><strong>3.使用nltk库和斯坦福解析器，进行Tokenizer，然后使用wiki百科600维的预训练unigram模型进行sentence embeddings</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> subprocess <span class="keyword">import</span> call</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize.stanford <span class="keyword">import</span> StanfordTokenizer</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentencesEmbeddings</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#fasttext执行</span></span><br><span class="line">        self.FASTTEXT_EXEC_PATH = os.path.abspath(<span class="string">"./sent2vec-master/fasttext"</span>)</span><br><span class="line">        <span class="comment">#斯坦福分词器的路径</span></span><br><span class="line">        self.BASE_SNLP_PATH = <span class="string">"sent2vec-master/stanford-postagger-full/"</span></span><br><span class="line">        self.SNLP_TAGGER_JAR = os.path.join(self.BASE_SNLP_PATH, <span class="string">"stanford-postagger.jar"</span>)</span><br><span class="line">        <span class="comment">#wiki百科预训练模型的路径</span></span><br><span class="line">        self.MODEL_WIKI_UNIGRAMS = os.path.abspath(<span class="string">"sent2vec-master/wiki_unigrams.bin"</span>)</span><br><span class="line">        self.tknzr = StanfordTokenizer(self.SNLP_TAGGER_JAR, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">        print(<span class="string">"SentencesEmbeddings初始化完成"</span>)</span><br><span class="line">    <span class="comment">#将句子去除符号，大小写转换后，进行分词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, sentence, to_lower=True)</span>:</span></span><br><span class="line">        <span class="string">"""Arguments:</span></span><br><span class="line"><span class="string">            - tknzr: a tokenizer implementing the NLTK tokenizer interface</span></span><br><span class="line"><span class="string">            - sentence: a string to be tokenized</span></span><br><span class="line"><span class="string">            - to_lower: lowercasing or not</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        sentence = sentence.strip()</span><br><span class="line">        sentence = <span class="string">' '</span>.join([self.format_token(x) <span class="keyword">for</span> x <span class="keyword">in</span>  self.tknzr.tokenize(sentence)])</span><br><span class="line">        <span class="keyword">if</span> to_lower:</span><br><span class="line">            sentence = sentence.lower()</span><br><span class="line">        sentence = re.sub(<span class="string">'((www\.[^\s]+)|(https?://[^\s]+)|(http?://[^\s]+))'</span>,<span class="string">'&lt;url&gt;'</span>,sentence) <span class="comment">#replace urls by &lt;url&gt;</span></span><br><span class="line">        sentence = re.sub(<span class="string">'(\@[^\s]+)'</span>,<span class="string">'&lt;user&gt;'</span>,sentence) <span class="comment">#replace @user268 by &lt;user&gt;</span></span><br><span class="line">        filter(<span class="keyword">lambda</span> word: <span class="string">' '</span> <span class="keyword">not</span> <span class="keyword">in</span> word, sentence)</span><br><span class="line">        <span class="keyword">return</span> sentence</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">format_token</span><span class="params">(self,token)</span>:</span></span><br><span class="line">        <span class="string">""""""</span></span><br><span class="line">        <span class="keyword">if</span> token == <span class="string">'-LRB-'</span>:</span><br><span class="line">            token = <span class="string">'('</span></span><br><span class="line">        <span class="keyword">elif</span> token == <span class="string">'-RRB-'</span>:</span><br><span class="line">            token = <span class="string">')'</span></span><br><span class="line">        <span class="keyword">elif</span> token == <span class="string">'-RSB-'</span>:</span><br><span class="line">            token = <span class="string">']'</span></span><br><span class="line">        <span class="keyword">elif</span> token == <span class="string">'-LSB-'</span>:</span><br><span class="line">            token = <span class="string">'['</span></span><br><span class="line">        <span class="keyword">elif</span> token == <span class="string">'-LCB-'</span>:</span><br><span class="line">            token = <span class="string">'&#123;'</span></span><br><span class="line">        <span class="keyword">elif</span> token == <span class="string">'-RCB-'</span>:</span><br><span class="line">            token = <span class="string">'&#125;'</span></span><br><span class="line">        <span class="keyword">return</span> token</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_sentences</span><span class="params">(self, sentences, to_lower=True)</span>:</span></span><br><span class="line">        <span class="string">"""Arguments:</span></span><br><span class="line"><span class="string">            - tknzr: 斯坦福解析器</span></span><br><span class="line"><span class="string">            - sentences:句子列表</span></span><br><span class="line"><span class="string">            - to_lower: 是否转化成消协</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#返回token化的结果</span></span><br><span class="line">        <span class="keyword">return</span> [self.tokenize( s, to_lower) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_embeddings_for_preprocessed_sentences</span><span class="params">(self,sentences, model_path, fasttext_exec_path)</span>:</span></span><br><span class="line">        <span class="string">"""Arguments:</span></span><br><span class="line"><span class="string">            - sentences:分词后的结果</span></span><br><span class="line"><span class="string">            - model_path: wiki百科模型文件的路径</span></span><br><span class="line"><span class="string">            - fasttext_exec_path: fasttext的路径</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        timestamp = str(time.time())</span><br><span class="line">        test_path = os.path.abspath(<span class="string">'./'</span>+timestamp+<span class="string">'_fasttext.test.txt'</span>)</span><br><span class="line">        embeddings_path = os.path.abspath(<span class="string">'./'</span>+timestamp+<span class="string">'_fasttext.embeddings.txt'</span>)</span><br><span class="line">        self.dump_text_to_disk(test_path, sentences)</span><br><span class="line">        call(fasttext_exec_path+</span><br><span class="line">              <span class="string">' print-sentence-vectors '</span>+</span><br><span class="line">              model_path + <span class="string">' &lt; '</span>+</span><br><span class="line">              test_path + <span class="string">' &gt; '</span> +</span><br><span class="line">              embeddings_path, shell=<span class="literal">True</span>)</span><br><span class="line">        embeddings = self.read_embeddings(embeddings_path)</span><br><span class="line">        os.remove(test_path)</span><br><span class="line">        os.remove(embeddings_path)</span><br><span class="line">        <span class="keyword">assert</span>(len(sentences) == len(embeddings))</span><br><span class="line">        <span class="keyword">return</span> np.array(embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_embeddings</span><span class="params">(self,embeddings_path)</span>:</span></span><br><span class="line">        <span class="string">"""Arguments:</span></span><br><span class="line"><span class="string">            - embeddings_path: path to the embeddings</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">with</span> open(embeddings_path, <span class="string">'r'</span>) <span class="keyword">as</span> in_stream:</span><br><span class="line">            embeddings = []</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> in_stream:</span><br><span class="line">                line = <span class="string">'['</span>+line.replace(<span class="string">' '</span>,<span class="string">','</span>)+<span class="string">']'</span></span><br><span class="line">                embeddings.append(eval(line))</span><br><span class="line">            <span class="keyword">return</span> embeddings</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dump_text_to_disk</span><span class="params">(self,file_path, X, Y=None)</span>:</span></span><br><span class="line">        <span class="string">"""Arguments:</span></span><br><span class="line"><span class="string">            - file_path: where to dump the data</span></span><br><span class="line"><span class="string">            - X: list of sentences to dump</span></span><br><span class="line"><span class="string">            - Y: labels, if any</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">with</span> open(file_path, <span class="string">'w'</span>) <span class="keyword">as</span> out_stream:</span><br><span class="line">            <span class="keyword">if</span> Y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(X, Y):</span><br><span class="line">                    out_stream.write(<span class="string">'__label__'</span>+str(y)+<span class="string">' '</span>+x+<span class="string">' \n'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">                    out_stream.write(x+<span class="string">' \n'</span>)</span><br><span class="line">    <span class="comment">#输入为一个句子的列表 返回embeeding之后的数据 600纬</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_sentence_embeddings</span><span class="params">(self,sentences)</span>:</span></span><br><span class="line">        wiki_embeddings = <span class="literal">None</span></span><br><span class="line">        <span class="comment">#加载斯坦福的分词器</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#进行分词操作</span></span><br><span class="line">        s = <span class="string">' &lt;delimiter&gt; '</span>.join(sentences)</span><br><span class="line">        tokenized_sentences_SNLP = self.tokenize_sentences([s])</span><br><span class="line">        tokenized_sentences_SNLP = tokenized_sentences_SNLP[<span class="number">0</span>].split(<span class="string">' &lt;delimiter&gt; '</span>)</span><br><span class="line">        <span class="keyword">assert</span>(len(tokenized_sentences_SNLP) == len(sentences))</span><br><span class="line">        <span class="comment">#使用wiki百科预训练的模型进行embeddings</span></span><br><span class="line">        wiki_embeddings = self.get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \</span><br><span class="line">                                         self.MODEL_WIKI_UNIGRAMS, self.FASTTEXT_EXEC_PATH)</span><br><span class="line">        <span class="keyword">return</span> wiki_embeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">embeddings</span><span class="params">(self,sentences)</span>:</span></span><br><span class="line"></span><br><span class="line">        my_embeddings = self.get_sentence_embeddings(sentences)</span><br><span class="line">        <span class="keyword">return</span> my_embeddings</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>4.句子相似度的计算</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env Python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">构造语义关系图  以及进行sentence encoder</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> SentencesEmbeddings</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> util</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentenceSemanticRelationGraph</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_set_sentences,</span></span></span><br><span class="line"><span class="function"><span class="params">                 validation_set_sentences,</span></span></span><br><span class="line"><span class="function"><span class="params">                 test_set_sentences,</span></span></span><br><span class="line"><span class="function"><span class="params">                 train_set_sencence_count,</span></span></span><br><span class="line"><span class="function"><span class="params">                 validation_set_sencence_count,</span></span></span><br><span class="line"><span class="function"><span class="params">                 test_set_sencence_count,</span></span></span><br><span class="line"><span class="function"><span class="params">                 sentence_to_index,</span></span></span><br><span class="line"><span class="function"><span class="params">                 index_to_sentence,</span></span></span><br><span class="line"><span class="function"><span class="params">                 train_set_sentences_list,</span></span></span><br><span class="line"><span class="function"><span class="params">                 validation_set_sentences_list,</span></span></span><br><span class="line"><span class="function"><span class="params">                 test_set_sentences_list,</span></span></span><br><span class="line"><span class="function"><span class="params">                 train_index,</span></span></span><br><span class="line"><span class="function"><span class="params">                 validation_index,</span></span></span><br><span class="line"><span class="function"><span class="params">                 test_index)</span>:</span></span><br><span class="line">        self.sentences_embeddings = SentencesEmbeddings.SentencesEmbeddings()</span><br><span class="line">        self.train_set_sentences = train_set_sentences  <span class="comment"># 训练集  将文档切分成了句子 存储格式为 &#123;doc_no :&#123;sen_id:句子1 ,sen_id2 :句子2 &#125;&#125;</span></span><br><span class="line">        self.validation_set_sentences = validation_set_sentences  <span class="comment"># 验证集</span></span><br><span class="line">        self.test_set_sentences = test_set_sentences  <span class="comment"># 测试集</span></span><br><span class="line">        self.train_set_sencence_count = train_set_sencence_count <span class="comment">#训练集句子数量</span></span><br><span class="line">        self.validation_set_sencence_count = validation_set_sencence_count  <span class="comment"># 验证集句子数量</span></span><br><span class="line">        self.test_set_sencence_count = test_set_sencence_count  <span class="comment"># 测试集句子数量</span></span><br><span class="line">        self.sentence_count = train_set_sencence_count+validation_set_sencence_count+test_set_sencence_count</span><br><span class="line">        self.data_set_sentences_embeddings = &#123;&#125; <span class="comment">#存储句子嵌入的集合</span></span><br><span class="line"></span><br><span class="line">        self.data_set_sentences = &#123;&#125;</span><br><span class="line">        self.tgsim = <span class="number">0.35</span>  <span class="comment">#句子之间余弦相似度的阈值  大于阈值的边才会被保留</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#数据集相关</span></span><br><span class="line">        self.index_to_sentence = index_to_sentence  <span class="comment"># 余弦相似度矩阵的下标与 文档句子的映射</span></span><br><span class="line"></span><br><span class="line">        self.sentence_to_index = sentence_to_index  <span class="comment"># 文档中的句子与余弦相似度矩阵下标的映射</span></span><br><span class="line">        self.data_set_cosine_similarity_matrix = <span class="literal">None</span>  <span class="comment"># 存储余弦相似度的矩阵</span></span><br><span class="line">        self.data_set_sentences_list = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#训练集相关</span></span><br><span class="line">        self.train_index = train_index <span class="comment">#训练集的下标</span></span><br><span class="line">        self.train_set_sentences_list = train_set_sentences_list  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line">        <span class="comment">#验证集相关</span></span><br><span class="line">        self.validation_index = validation_index  <span class="comment"># 验证集的下标</span></span><br><span class="line"></span><br><span class="line">        self.validation_set_sentences_list = validation_set_sentences_list  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line">        <span class="comment">#测试集相关</span></span><br><span class="line">        self.test_index = test_index  <span class="comment"># 测试集的下标</span></span><br><span class="line"></span><br><span class="line">        self.test_set_sentences_list = test_set_sentences_list  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    处理已经读入的训练集 测试集  验证集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deal_data</span><span class="params">(self)</span>:</span></span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.train_set_sentences_list.items():</span><br><span class="line">           self.data_set_sentences_list[doc_no] = sentences_list</span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.validation_set_sentences_list.items():</span><br><span class="line">           self.data_set_sentences_list[doc_no] = sentences_list</span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.test_set_sentences_list.items():</span><br><span class="line">           self.data_set_sentences_list[doc_no] = sentences_list</span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.train_set_sentences.items():</span><br><span class="line">           self.data_set_sentences[doc_no] = sentences_list</span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.validation_set_sentences.items():</span><br><span class="line">           self.data_set_sentences[doc_no] = sentences_list</span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.test_set_sentences.items():</span><br><span class="line">           self.data_set_sentences[doc_no] = sentences_list</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    调用方法计算句子嵌入 </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_sentences_embeddings</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 以文档为单位计算句子嵌入  遍历每个文档的每一句话的index</span></span><br><span class="line">        <span class="keyword">for</span> doc_no, sentence_index_list <span class="keyword">in</span> self.data_set_sentences_list.items():</span><br><span class="line">            index_list = []</span><br><span class="line">            sentence_list = []</span><br><span class="line">            <span class="comment"># 根据index找到对应的句子</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> sentence_index_list:</span><br><span class="line">                <span class="comment"># sen_id = str(self.train_set_index_to_sentence[index]).split("#")[1]</span></span><br><span class="line">                sen_id = str(self.index_to_sentence[index]).split(<span class="string">"#"</span>)[<span class="number">1</span>]</span><br><span class="line">                sentence = self.data_set_sentences[doc_no][sen_id]</span><br><span class="line">                sentence_list.append(sentence)</span><br><span class="line">                index_list.append(index)</span><br><span class="line">            <span class="comment"># 将文档中所有的句子加入列表，统一计算句子嵌入</span></span><br><span class="line">            sentence_embedding_list = self.sentences_embeddings.embeddings(sentence_list)</span><br><span class="line">            <span class="comment"># 然后加入句子嵌入的列表</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(index_list)):</span><br><span class="line">                self.data_set_sentences_embeddings[index_list[i]] = sentence_embedding_list[i]</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算句子间的余弦相似度 构造语义关系图</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_cosine_similarity_matrix</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 初始化余弦相似度的矩阵</span></span><br><span class="line">        self.data_set_cosine_similarity_matrix = np.zeros(</span><br><span class="line">            (self.sentence_count, self.sentence_count))</span><br><span class="line">        <span class="comment"># 遍历映射 进行余弦相似度的计算</span></span><br><span class="line">        <span class="keyword">for</span> index, id <span class="keyword">in</span> self.index_to_sentence.items():</span><br><span class="line">            <span class="keyword">for</span> index2, id2 <span class="keyword">in</span> self.index_to_sentence.items():</span><br><span class="line">                <span class="keyword">if</span> index == index2 :</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> self.data_set_cosine_similarity_matrix[index][index2] == <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># 句子1 的嵌入</span></span><br><span class="line">                    embeddings1 = self.data_set_sentences_embeddings[index]</span><br><span class="line">                    <span class="comment"># 句子2的嵌入</span></span><br><span class="line">                    embeddings2 = self.data_set_sentences_embeddings[index2]</span><br><span class="line">                    cosine_similarity = self.calculate_cosine_similarity(embeddings1, embeddings2)</span><br><span class="line">                    <span class="comment"># 如果大于阈值 在进行存储</span></span><br><span class="line">                    <span class="keyword">if</span> cosine_similarity &gt; self.tgsim:</span><br><span class="line">                        self.data_set_cosine_similarity_matrix[index][index2] = cosine_similarity</span><br><span class="line">                        self.data_set_cosine_similarity_matrix[index2][index] = cosine_similarity</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算两个numpy向量之间的余弦相似度</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_cosine_similarity</span><span class="params">(self,vector1,vector2)</span>:</span></span><br><span class="line">        cosine_similarity = np.dot(vector1,vector2)/(np.linalg.norm(vector1)*(np.linalg.norm(vector2)))</span><br><span class="line">        <span class="keyword">return</span> cosine_similarity</span><br></pre></td></tr></table></figure>

<p>解释：</p>
<ol>
<li><p>获得读取到的数据，将其构成几个集合：<br>（1）data_set_sentences_list——表示文档的句子的索引,存储方式为{doc_no : [index1,index2]}，表示一篇文档句子的存储位置<br>（2）index_to_sentence——表示文档句子的索引与文档句子编号的映射（可以根据index找到属于那篇文档的那个句子），存储方式为{index : doc_no#sen_id}<br>（3）sentence_to_index——表示文档句子编号与文档句子索引的的映射（可以根据某文档的某个句子找到他存储的index，存储方式为{doc_no#sen_id : index}<br>（4）data_set_sentences——表示具体文档句子id与文档内容的映射，存储方式为{doc_no#sen_id : 句子内容}</p>
</li>
<li><p>然后调用方法计算每一篇文档的sentence embeddings，存储格式为{reviewer_id1：{sen_id1：embeddings1，sen_id2：embeddings2} ，reviewer_id2：{sen_id1：embeddings1，sen_id2：embeddings2} }。</p>
</li>
<li><p>最后借助sentence embeddings，来计算句子语义关系图data_set_cosine_similarity_matrix</p>
</li>
</ol>
<p>至此，第一部分的工作完成。</p>
<h2 id="三-句子编码（Sentence-Encoder）"><a href="#三-句子编码（Sentence-Encoder）" class="headerlink" title="三.句子编码（Sentence Encoder）"></a>三.句子编码（Sentence Encoder）</h2><p> <strong>- 1.解释</strong><br> 在构建完语义关系图之后，还需要对训练集中的所有单词，使用300维的预先训练的GloVe嵌入（Pen- nington et al., 2014）进行单词嵌入（word embeddings），然后将单词嵌入输入句子编码器以计算句子嵌入，句子编码器使用了单层正向循环神经网络（ a single-layer forward recurrent neural network）的变体，长短时记忆神经网络LSTM，最后从隐藏层中提取句子嵌入，然后，将所有句子嵌入连接到一个矩阵X中，该矩阵X将构成图卷积网络将使用的输入节点特征。</p>
<p> <strong>- 2.循环神经网络RNN的学习</strong></p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/50915723" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/50915723</a><br><a href="https://zhuanlan.zhihu.com/p/30844905" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/30844905</a></p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/202007231606529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。对于文本数据，句子前后是有语义关系的，所以RNN正好可以处理句子前后关系的影响。但是RNN也有序列过长的情况下梯度消失或者梯度爆炸的问题。</p>
<p> <strong>- 3.LSTM：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/2020072316103727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200723161135460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>用pytorch实现lstm：</p>
<blockquote>
<p><a href="https://pytorch.org/docs/master/generated/torch.nn.LSTM.html" target="_blank" rel="noopener">https://pytorch.org/docs/master/generated/torch.nn.LSTM.html</a><br><a href="https://zhuanlan.zhihu.com/p/79064602" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79064602</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size,batch_size)</span>:</span></span><br><span class="line">        super(EncoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size <span class="comment">#</span></span><br><span class="line">        self.input_size = input_size <span class="comment">#每个单词向量的长度</span></span><br><span class="line">        self.batch_size = batch_size <span class="comment">#句子的个数</span></span><br><span class="line">        <span class="comment"># requires_grad指定是否在训练过程中对词向量的权重进行微调</span></span><br><span class="line">        <span class="comment"># self.embedding.weight.requires_grad = True</span></span><br><span class="line">        self.lstm = nn.LSTM(self.input_size, hidden_size)</span><br><span class="line">        <span class="comment"># self.gru = nn.GRU(hidden_size, hidden_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden,cell,seq_len,batch_size)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param input:</span></span><br><span class="line"><span class="string">        :param hidden:</span></span><br><span class="line"><span class="string">        :param cell:</span></span><br><span class="line"><span class="string">        :param seq_len:     句子的长度</span></span><br><span class="line"><span class="string">        :param batch_size:  句子的个数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># embedded = self.embedding[input].view(1, 1, self.hidden_size)</span></span><br><span class="line">        <span class="comment"># embedded = input.view(len(input), 1, 300)</span></span><br><span class="line">        embedded = input.view(seq_len, batch_size, <span class="number">300</span>)</span><br><span class="line">        <span class="comment"># output, hidden = self.gru(embedded, hidden)</span></span><br><span class="line">        output , (hidden, cell)= self.lstm(embedded.float(), (hidden.float(),cell.float()))</span><br><span class="line">        <span class="keyword">return</span> output,hidden,cell</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200723162557845.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200723163050742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>（1）torch.nn.LSTM(*args, kwargs)<br>参数列表：<br>– input_size ：输入特征维数，如我们输入的是300维的预先训练的GloVe嵌入产生的词向量<br>– hidden_size：隐藏层状态的维数，即隐藏层节点的个数，这里我选择输入的也是300维<br>– num_layers： LSTM 堆叠的层数，默认值是1层，我们的神经网络也是单层的<br>– bias： 隐层状态是否带bias，默认为true。bias是偏置值，或者偏移值。没有偏置值就是以0为中轴，或以0为起点。<br>– batch_first：输入输出的第一维是否为 batch_size，默认值 False。因为 Torch 中，人们习惯使用Torch中带有的dataset，dataloader向神经网络模型连续输入数据，这里面就有一个 batch_size 的参数，表示一次输入多少个数据。 在 LSTM 模型中，输入数据必须是一批数据，为了区分LSTM中的批量数据和dataloader中的批量数据是否相同意义，LSTM 模型就通过这个参数的设定来区分。 如果是相同意义的，就设置为True，如果不同意义的，设置为False。 torch.LSTM 中 batch_size 维度默认是放在第二维度，故此参数设置可以将 batch_size 放在第一维度。如：input 默认是(4,1,5)，中间的 1 是 batch_size，指定batch_first=True后就是(1,4,5)。所以，如果你的输入数据是二维数据的话，就应该将 batch_first 设置为True;</p>
<p>– dropout： 默认值0。是否在除最后一个 RNN 层外的其他 RNN 层后面加 dropout 层。输入值是 0-1 之间的小数，表示概率。0表示0概率dripout，即不dropout<br>– bidirectional： 是否是双向 RNN，默认为：false，若为 true，则：num_directions=2，否则为1。<br>（2）向前传播时的输入<br><img src="https://img-blog.csdnimg.cn/20200729175411378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20200723163451721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200729175639695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>输入的张量的每一个维度都有固定的含义，不能弄错，需要在理解之后，才能进行修改。</p>
<ul>
<li><p><strong>4.word embeddings（使用预训练的glove进行词嵌入）</strong></p>
<p>（1）下载预训练的模型</p>
</li>
</ul>
<blockquote>
<p><a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/glove/</a></p>
</blockquote>
<p>（2）glove模型的使用<br>使用python的gensim工具包。首先需要将这个训练好的模型转换成gensim方便加载的格式(gensim支持word2vec格式的预训练模型格式）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.scripts.glove2word2vec <span class="keyword">import</span> glove2word2vec</span><br><span class="line">glove_input_file = <span class="string">'data/glove.6B.300d.txt'</span></span><br><span class="line">word2vec_output_file = <span class="string">'data/glove.6B.300d.word2vec.txt'</span></span><br><span class="line">glove2word2vec(self.glove_input_file, self.word2vec_output_file)</span><br></pre></td></tr></table></figure>
<p>转换过模型格式后，就可以使用里面的词向量了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 获得单词cat的词向量</span></span><br><span class="line">cat_vec = glove_model[<span class="string">'cat'</span>]</span><br><span class="line">print(cat_vec)</span><br></pre></td></tr></table></figure>

<p><img src="https://img-blog.csdnimg.cn/20200723164311128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li><strong>5.具体实现</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env Python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> gensim.scripts.glove2word2vec <span class="keyword">import</span> glove2word2vec</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> util</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size,batch_size)</span>:</span></span><br><span class="line">        super(EncoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size <span class="comment">#</span></span><br><span class="line">        self.input_size = input_size <span class="comment">#每个单词向量的长度</span></span><br><span class="line">        self.batch_size = batch_size <span class="comment">#句子的个数</span></span><br><span class="line">        <span class="comment"># requires_grad指定是否在训练过程中对词向量的权重进行微调</span></span><br><span class="line">        <span class="comment"># self.embedding.weight.requires_grad = True</span></span><br><span class="line">        self.lstm = nn.LSTM(self.input_size, hidden_size)</span><br><span class="line">        <span class="comment"># self.gru = nn.GRU(hidden_size, hidden_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden,cell,seq_len,batch_size)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param input:</span></span><br><span class="line"><span class="string">        :param hidden:</span></span><br><span class="line"><span class="string">        :param cell:</span></span><br><span class="line"><span class="string">        :param seq_len:     句子的长度</span></span><br><span class="line"><span class="string">        :param batch_size:  句子的个数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># embedded = self.embedding[input].view(1, 1, self.hidden_size)</span></span><br><span class="line">        <span class="comment"># embedded = input.view(len(input), 1, 300)</span></span><br><span class="line">        embedded = input.view(seq_len, batch_size, <span class="number">300</span>)</span><br><span class="line">        <span class="comment"># output, hidden = self.gru(embedded, hidden)</span></span><br><span class="line">        output , (hidden, cell)= self.lstm(embedded.float(), (hidden.float(),cell.float()))</span><br><span class="line">        <span class="keyword">return</span> output,hidden,cell</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#各个维度的含义是 (Seguence, minibatch_size, hidden_dim)</span></span><br><span class="line">        <span class="comment"># 1个LSTM层，batch_size=句子的个数, 隐藏层的特征维度300</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, self.batch_size, self.hidden_size, device=device,dtype=torch.float)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentenceEncoder</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,data_set_sentences,</span></span></span><br><span class="line"><span class="function"><span class="params">                 sentence_to_index,</span></span></span><br><span class="line"><span class="function"><span class="params">                 index_to_sentence)</span></span></span><br><span class="line">        self.data_set_sentences = data_set_sentences  #数据集  将文档切分成了句子 存储格式为 &#123;doc_no :&#123;sen_id:句子1 ,sen_id2 :句子2 &#125;&#125;</span><br><span class="line">        <span class="comment">#有关数据集</span></span><br><span class="line">        self.data_set_word = &#123;&#125; <span class="comment"># 将句子切分成单词  &#123;doc_no:&#123;sen_id:[word1,word2]&#125;&#125;</span></span><br><span class="line">        self.sentence_to_index = sentence_to_index  <span class="comment"># 训练集 将句子切分成单词  然后与下标的映射  句子→索引的字典  &#123;reviewer_id+"#"+sen_id : index&#125;</span></span><br><span class="line">        self.index_to_sentence = index_to_sentence  <span class="comment"># 索引→单词的字典 &#123;index  :  reviewer_id+"#"+sen_id &#125;</span></span><br><span class="line">        self.data_set_sentence_word = &#123;&#125; <span class="comment">#&#123;index : [单词1  单词2  单词3 ]&#125;</span></span><br><span class="line">        self.data_set_sentence_count = <span class="number">0</span>  <span class="comment"># 句子的数量</span></span><br><span class="line">        self.max_sentence_length = <span class="number">0</span><span class="comment">#最长句子的长度</span></span><br><span class="line">        self.data_set_sentence_word_to_vec = &#123;&#125;</span><br><span class="line">        self.data_set_word_to_index = &#123;&#125;<span class="comment">#单词→索引</span></span><br><span class="line">        self.data_set_word_count = &#123;&#125;<span class="comment">#每个单词的计数</span></span><br><span class="line">        self.index_to_data_set_word = &#123;&#125;<span class="comment">#索引→单词</span></span><br><span class="line">        self.word_to_vec = &#123;&#125;  <span class="comment">#单词与向量的映射</span></span><br><span class="line">        <span class="comment">#定义了一个unknown的词，也就是说没有出现在训练集里的词，我们都叫做unknown，词向量就定义为0</span></span><br><span class="line">        self.number_word = <span class="number">0</span><span class="comment">#词的数量</span></span><br><span class="line">        <span class="comment">#词向量的维度</span></span><br><span class="line">        self.embeddings_size = <span class="number">300</span></span><br><span class="line">        self.data_set_sentence_word_encoder = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.glove_input_file = <span class="string">'data/glove.6B.300d.txt'</span></span><br><span class="line">        self.word2vec_output_file = <span class="string">'data/glove.6B.300d.word2vec.txt'</span></span><br><span class="line">        <span class="comment"># 加载预训练的glove模型</span></span><br><span class="line">        self.glove_model = KeyedVectors.load_word2vec_format(self.word2vec_output_file, binary=<span class="literal">False</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deal_data</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#索引标记</span></span><br><span class="line">        <span class="keyword">for</span> index, sentence <span class="keyword">in</span> self.index_to_sentence.items():</span><br><span class="line">            doc_no = str(sentence).split(<span class="string">"#"</span>)[<span class="number">0</span>]</span><br><span class="line">            sen_id = str(sentence).split(<span class="string">"#"</span>)[<span class="number">1</span>]</span><br><span class="line">            sentence = self.data_set_sentences[doc_no][sen_id]</span><br><span class="line">            <span class="comment"># 句子转化为 单词列表</span></span><br><span class="line">            word_list = self.sentence_to_word(sentence)</span><br><span class="line">            self.data_set_sentence_word[index] = word_list</span><br><span class="line">            self.data_set_sentence_count += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 将句子转化成词语</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sentence_to_word</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        sentence = util.normalize_string(sentence)</span><br><span class="line">        word_list= []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>):</span><br><span class="line">            word = util.normalize_string(word)</span><br><span class="line">            <span class="keyword">if</span> word != <span class="string">""</span>:</span><br><span class="line">                self.addWord(word)</span><br><span class="line">                word_list.append(word)</span><br><span class="line">        <span class="keyword">if</span> len(word_list) &gt;self.max_sentence_length:</span><br><span class="line">            self.max_sentence_length = len(word_list)</span><br><span class="line">        <span class="keyword">return</span> word_list</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addWord</span><span class="params">(self, word)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.data_set_word_to_index:</span><br><span class="line">            self.data_set_word_to_index[word] = self.number_word</span><br><span class="line">            self.data_set_word_count[word] = <span class="number">1</span></span><br><span class="line">            self.index_to_data_set_word[self.number_word] = word</span><br><span class="line">            self.number_word += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data_set_word_count[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#首先需要将这个训练好的模型转换成gensim方便加载的格式(gensim支持word2vec格式的预训练模型格式）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">glove_to_word2vec</span><span class="params">(self)</span>:</span></span><br><span class="line">        glove2word2vec(self.glove_input_file, self.word2vec_output_file)</span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    获得词向量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_vec</span><span class="params">(self,word)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.glove_model[word]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_to_vector</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> index , word <span class="keyword">in</span> self.index_to_data_set_word.items():</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                word_vec = self.get_word_vec(word)</span><br><span class="line">            <span class="keyword">except</span> KeyError:</span><br><span class="line">                word_vec = np.zeros(<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">            self.word_to_vec[word] = word_vec</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将句子使用rnn进行encoder</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sentence_encoder</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#遍历每句话中的每个单词</span></span><br><span class="line">        hedden_size = <span class="number">300</span></span><br><span class="line">        encoder = EncoderRNN(<span class="number">300</span>, hedden_size,self.data_set_sentence_count).to(device)</span><br><span class="line">        hidden = encoder.initHidden()</span><br><span class="line">        cell = encoder.initHidden()</span><br><span class="line">        output = <span class="literal">None</span></span><br><span class="line">        <span class="comment">#所有句子的 向量列表 三维的</span></span><br><span class="line">        sentences_vector_list = []</span><br><span class="line">        <span class="keyword">for</span> index ,words <span class="keyword">in</span> self.data_set_sentence_word.items():</span><br><span class="line">            <span class="comment">#每句话的单词进行word embeddings之后生成的矩阵</span></span><br><span class="line">            sentence_vector_list = []</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            <span class="comment">#将单词转化为词向量</span></span><br><span class="line">                sentence_vector_list.append(self.word_to_vec[word])</span><br><span class="line">                count +=<span class="number">1</span></span><br><span class="line">            <span class="comment">#然后将所有的句子都结合在一起</span></span><br><span class="line">            sentence_vector_list = np.array(sentence_vector_list)</span><br><span class="line">            <span class="comment"># 在数组A的边缘填充constant_values指定的数值</span></span><br><span class="line">            <span class="comment"># （3,2）表示在A的第[0]轴填充（二维数组中，0轴表示行），即在0轴前面填充3个宽度的0，比如数组A中的95,96两个元素前面各填充了3个0；在后面填充2个0，比如数组A中的97,98两个元素后面各填充了2个0stant_values表示填充值，且(b</span></span><br><span class="line">            <span class="comment"># （2,3）表示在A的第[1]轴填充（二维数组中，1轴表示列），即在1轴前面填充2个宽度的0，后面填充3个宽度的0</span></span><br><span class="line">            sentence_vector_list = np.pad(sentence_vector_list,((<span class="number">0</span>,self.max_sentence_length-count),(<span class="number">0</span>,<span class="number">0</span>)),<span class="string">'constant'</span>,constant_values = (<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">            sentences_vector_list.append(sentence_vector_list)</span><br><span class="line">        output,hidden, cell = encoder(self.sentence_to_tensor(sentences_vector_list), hidden, cell,self.max_sentence_length,self.data_set_sentence_count)</span><br><span class="line">        print(hidden.shape)</span><br><span class="line">        print(output.shape)</span><br><span class="line">            <span class="comment"># self.train_set_sentence_word_encoder[index] = hidden[0].detach().numpy()</span></span><br><span class="line">        self.data_set_sentence_word_encoder = output[output.shape[<span class="number">0</span>]<span class="number">-1</span>].tolist()</span><br><span class="line">        <span class="comment"># print(self.train_set_sentence_word_encoder)</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将句子向量转化成tensor</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sentence_to_tensor</span><span class="params">(self, vector_list)</span>:</span></span><br><span class="line">        vector_array = np.array(vector_list)</span><br><span class="line">        tensor = torch.tensor(vector_array, dtype=torch.float, device=device).view(self.data_set_sentence_count,self.max_sentence_length,<span class="number">300</span>)</span><br><span class="line">        <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure>

<p> 说明：<br> （1）读入数据<br>index_to_sentence——表示文档句子的索引与文档句子编号的映射（可以根据index找到属于那篇文档的那个句子），存储方式为{index : doc_no#sen_id}<br>sentence_to_index——表示文档句子编号与文档句子索引的的映射（可以根据某文档的某个句子找到他存储的index，存储方式为{doc_no#sen_id : index}<br>data_set_sentences——表示具体文档句子id与文档内容的映射，存储方式为{doc_no#sen_id : 句子内容}</p>
<p>（2）对数据进行处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.train_set_word_to_index = &#123;&#125;<span class="comment">#单词→索引</span></span><br><span class="line">self.train_set_word_count = &#123;&#125;<span class="comment">#每个单词的计数</span></span><br><span class="line">self.index_to_train_set_word = &#123;&#125;<span class="comment">#索引→单词</span></span><br></pre></td></tr></table></figure>
<p>将训练集中所有出现过的单词进行统计，建立单个词语的索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_data</span><span class="params">(self)</span>:</span></span><br><span class="line">       <span class="comment">#索引标记</span></span><br><span class="line">       <span class="keyword">for</span> index, sentence <span class="keyword">in</span> self.index_to_sentence.items():</span><br><span class="line">           doc_no = str(sentence).split(<span class="string">"#"</span>)[<span class="number">0</span>]</span><br><span class="line">           sen_id = str(sentence).split(<span class="string">"#"</span>)[<span class="number">1</span>]</span><br><span class="line">           sentence = self.data_set_sentences[doc_no][sen_id]</span><br><span class="line">           <span class="comment"># 句子转化为 单词列表</span></span><br><span class="line">           word_list = self.sentence_to_word(sentence)</span><br><span class="line">           self.data_set_sentence_word[index] = word_list</span><br><span class="line">           self.data_set_sentence_count += <span class="number">1</span></span><br><span class="line">   <span class="comment"># 将句子转化成词语</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">sentence_to_word</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">       sentence = util.normalize_string(sentence)</span><br><span class="line">       word_list= []</span><br><span class="line">       <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">' '</span>):</span><br><span class="line">           word = util.normalize_string(word)</span><br><span class="line">           <span class="keyword">if</span> word != <span class="string">""</span>:</span><br><span class="line">               self.addWord(word)</span><br><span class="line">               word_list.append(word)</span><br><span class="line">       <span class="keyword">if</span> len(word_list) &gt;self.max_sentence_length:</span><br><span class="line">           self.max_sentence_length = len(word_list)</span><br><span class="line">       <span class="keyword">return</span> word_list</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">addWord</span><span class="params">(self, word)</span>:</span></span><br><span class="line">       <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.data_set_word_to_index:</span><br><span class="line">           self.data_set_word_to_index[word] = self.number_word</span><br><span class="line">           self.data_set_word_count[word] = <span class="number">1</span></span><br><span class="line">           self.index_to_data_set_word[self.number_word] = word</span><br><span class="line">           self.number_word += <span class="number">1</span></span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           self.data_set_word_count[word] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>（3）  将句子使用LSTM进行encoder</p>
<p>使用之前处理过的句子的单词列表，求出每一句话中每一个单词的word embedding （1<em>300），然后将这句话中的所有单词的word embedding ，组成一个 （句子中单词个数</em>300）维的矩阵。然后将这个矩阵的行数，补全成了最长句子的单词个数，最终形成的矩阵是 （最长句子的单词个数*300）。</p>
<p>我将所有句子都这样操作，形成了一个三维的张量 （句子个数 * 最长句子的单词个数*300）作为LSTM的输入。</p>
<p>那么在初始化LSTM时 为nn.LSTM(每个单词向量的长度——300, hidden_size)</p>
<p> 然后我将hidden 和cell初始化成了 torch.zeros(1, 句子的个数, hidden_size)</p>
<ul>
<li>对于input(seq_len, batch, input_size)  的参数 ：</li>
</ul>
<p>seq_len是序列的个数，对于句子来说，应该是句子的长度，应该是每一句话中单词的个数，这个是需要固定的 ，取了最长的句子的单词数。</p>
<p>batch表示一次性喂给网络多少条句子，初始化成句子的个数</p>
<p>input_size应该是每个具体的输入是多少维的向量，这里应该是300（word embedding的维度数量）</p>
<p>最终的hidden_size =300  最终生成了（句子的个数*300）的矩阵，作为Encoder的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先需要将这个训练好的模型转换成gensim方便加载的格式(gensim支持word2vec格式的预训练模型格式）</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">glove_to_word2vec</span><span class="params">(self)</span>:</span></span><br><span class="line">      glove2word2vec(self.glove_input_file, self.word2vec_output_file)</span><br><span class="line"></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  获得词向量</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_word_vec</span><span class="params">(self,word)</span>:</span></span><br><span class="line">      <span class="keyword">return</span> self.glove_model[word]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">word_to_vector</span><span class="params">(self)</span>:</span></span><br><span class="line">      <span class="keyword">for</span> index , word <span class="keyword">in</span> self.index_to_data_set_word.items():</span><br><span class="line">          <span class="keyword">try</span>:</span><br><span class="line">              word_vec = self.get_word_vec(word)</span><br><span class="line">          <span class="keyword">except</span> KeyError:</span><br><span class="line">              word_vec = np.zeros(<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">          self.word_to_vec[word] = word_vec</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  将句子使用rnn进行encoder</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sentence_encoder</span><span class="params">(self)</span>:</span></span><br><span class="line">      <span class="comment">#遍历每句话中的每个单词</span></span><br><span class="line">      hedden_size = <span class="number">300</span></span><br><span class="line">      encoder = EncoderRNN(<span class="number">300</span>, hedden_size,self.data_set_sentence_count).to(device)</span><br><span class="line">      hidden = encoder.initHidden()</span><br><span class="line">      cell = encoder.initHidden()</span><br><span class="line">      output = <span class="literal">None</span></span><br><span class="line">      <span class="comment">#所有句子的 向量列表 三维的</span></span><br><span class="line">      sentences_vector_list = []</span><br><span class="line">      <span class="keyword">for</span> index ,words <span class="keyword">in</span> self.data_set_sentence_word.items():</span><br><span class="line">          <span class="comment">#每句话的单词进行word embeddings之后生成的矩阵</span></span><br><span class="line">          sentence_vector_list = []</span><br><span class="line">          count = <span class="number">0</span></span><br><span class="line">          <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">          <span class="comment">#将单词转化为词向量</span></span><br><span class="line">              sentence_vector_list.append(self.word_to_vec[word])</span><br><span class="line">              count +=<span class="number">1</span></span><br><span class="line">          <span class="comment">#然后将所有的句子都结合在一起</span></span><br><span class="line">          sentence_vector_list = np.array(sentence_vector_list)</span><br><span class="line">          <span class="comment"># 在数组A的边缘填充constant_values指定的数值</span></span><br><span class="line">          <span class="comment"># （3,2）表示在A的第[0]轴填充（二维数组中，0轴表示行），即在0轴前面填充3个宽度的0，比如数组A中的95,96两个元素前面各填充了3个0；在后面填充2个0，比如数组A中的97,98两个元素后面各填充了2个0stant_values表示填充值，且(b</span></span><br><span class="line">          <span class="comment"># （2,3）表示在A的第[1]轴填充（二维数组中，1轴表示列），即在1轴前面填充2个宽度的0，后面填充3个宽度的0</span></span><br><span class="line">          sentence_vector_list = np.pad(sentence_vector_list,((<span class="number">0</span>,self.max_sentence_length-count),(<span class="number">0</span>,<span class="number">0</span>)),<span class="string">'constant'</span>,constant_values = (<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">          sentences_vector_list.append(sentence_vector_list)</span><br><span class="line">      output,hidden, cell = encoder(self.sentence_to_tensor(sentences_vector_list), hidden, cell,self.max_sentence_length,self.data_set_sentence_count)</span><br><span class="line">      print(hidden.shape)</span><br><span class="line">      print(output.shape)</span><br><span class="line">          <span class="comment"># self.train_set_sentence_word_encoder[index] = hidden[0].detach().numpy()</span></span><br><span class="line">      self.data_set_sentence_word_encoder = output[output.shape[<span class="number">0</span>]<span class="number">-1</span>].tolist()</span><br><span class="line">      <span class="comment"># print(self.train_set_sentence_word_encoder)</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  将句子向量转化成tensor</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sentence_to_tensor</span><span class="params">(self, vector_list)</span>:</span></span><br><span class="line">      vector_array = np.array(vector_list)</span><br><span class="line">      tensor = torch.tensor(vector_array, dtype=torch.float, device=device).view(self.data_set_sentence_count,self.max_sentence_length,<span class="number">300</span>)</span><br><span class="line">      <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure>

<h2 id="四-图卷积网络（Graph-Convolutional-Network）及显著性估计（Saliency-Estimation）"><a href="#四-图卷积网络（Graph-Convolutional-Network）及显著性估计（Saliency-Estimation）" class="headerlink" title="四.图卷积网络（Graph Convolutional Network）及显著性估计（Saliency Estimation）"></a>四.图卷积网络（Graph Convolutional Network）及显著性估计（Saliency Estimation）</h2><p> <strong>- 1.前言</strong><br>（1）在计算完句子嵌入和句子语义关系图后，使用单层图卷积网络（GCN），以便捕获每个句子的高级隐藏特征，封装句子信息以及图结构。<br>图卷积网络的邻接矩阵为句子语义关系图加上单位矩阵的结果（A = A + I），特征矩阵为使用LSTM进行句子编码后的结果。<br>（2）然后使用以下等式，获取句子的隐藏特征<br><img src="https://img-blog.csdnimg.cn/20200729202514147.png" alt="在这里插入图片描述"><br>Wi是第i个图卷积层的权重矩阵，bi是偏差矢量。使用ELU作为激活函数。<br>（3）之后使用一个线性层来估计每个句子的显着性分数，然后通过softmax将分数归一化。</p>
<p> <strong>- 2.图卷积网络的学习与理解</strong></p>
<blockquote>
<p>参考文章：<a href="https://zhuanlan.zhihu.com/p/54505069" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/54505069</a><br><a href="https://zhuanlan.zhihu.com/p/89503068" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/89503068</a><br><a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html" target="_blank" rel="noopener">https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html</a></p>
</blockquote>
<p>（1）图神经网络GNN<br><img src="https://img-blog.csdnimg.cn/20200729203547914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200729203558981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200729203628762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>（2）图卷积网络</p>
<p><img src="https://img-blog.csdnimg.cn/20200729203718684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200729203759813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这个图也正好应证了论文中的公式<br><img src="https://img-blog.csdnimg.cn/20200729202514147.png" alt="在这里插入图片描述"><br>两层卷积层，每一层之后都有一个激活函数。</p>
<p> <strong>- 3.图卷积网络GCN的实现</strong><br> <strong>（1）卷积层的定义</strong><br> 卷积层输入维度为节点输入特征的维度，还可以设定偏差矢量。<br> 向前传播时，需要将节点的邻接矩阵和特征矩阵进行运算，然后进行相应的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraphConvolution</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, output_dim, use_bias=True)</span>:</span></span><br><span class="line">        <span class="string">"""图卷积：L*X*\theta</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">            input_dim: int</span></span><br><span class="line"><span class="string">                节点输入特征的维度 D</span></span><br><span class="line"><span class="string">            output_dim: int</span></span><br><span class="line"><span class="string">                输出特征维度 D‘</span></span><br><span class="line"><span class="string">            use_bias : bool, optional</span></span><br><span class="line"><span class="string">                是否使用偏置</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(GraphConvolution, self).__init__()</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.use_bias = use_bias</span><br><span class="line">        <span class="comment"># 定义GCN层的权重矩阵    input_dim=300  output_dim=300</span></span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))</span><br><span class="line">        <span class="keyword">if</span> self.use_bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(output_dim))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">'bias'</span>, <span class="literal">None</span>)</span><br><span class="line">        self.reset_parameters()  <span class="comment"># 使用自定义的参数初始化方式</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 自定义参数初始化方式</span></span><br><span class="line">        <span class="comment"># 权重参数初始化方式</span></span><br><span class="line">        init.kaiming_uniform_(self.weight)</span><br><span class="line">        <span class="keyword">if</span> self.use_bias:  <span class="comment"># 偏置参数初始化为0</span></span><br><span class="line">            init.zeros_(self.bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, adjacency, input_feature)</span>:</span></span><br><span class="line">        <span class="string">"""邻接矩阵是稀疏矩阵，因此在计算时使用稀疏矩阵乘法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">            adjacency:</span></span><br><span class="line"><span class="string">                邻接矩阵</span></span><br><span class="line"><span class="string">            input_feature: torch.Tensor</span></span><br><span class="line"><span class="string">                输入特征</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#矩阵相乘</span></span><br><span class="line">        <span class="comment"># h =  ̃ELU(A X W0 +b0 )   A为邻接矩阵  X为特征矩阵  W0为权重矩阵  B0为偏差矢量</span></span><br><span class="line">        <span class="comment"># S = ̃ELU(A h W1 +b1 )</span></span><br><span class="line">        support = torch.mm(input_feature, self.weight)  <span class="comment"># X W (N,D');   X (N,D);W (D,D')   input_feature 维度为 句子个数*300  weight的维度为 300 * 300  输出为  句子个数 *300</span></span><br><span class="line">        output = torch.mm(adjacency, support)  <span class="comment"># (N,D')  #adjacency 为句子个数*句子个数     support 为 句子个数 *300   output为句子个数 *300</span></span><br><span class="line">        <span class="comment">#也可以使用稀疏矩阵的乘法</span></span><br><span class="line">        <span class="comment"># support = torch.mm(input_feature, self.weight) #XW (N,D');X (N,D);W (D,D')  </span></span><br><span class="line">        <span class="comment">#output = torch.sparse.mm(adjacency, support) #(N,D')</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_bias:</span><br><span class="line">            output += self.bias</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>（2）GCN模型的定义<br>这个模型包含两个卷积层，并且在最终的输出前还使用了线性层以及softmax计算每一个句子的显著性分数。并且模型还使用了0.2的丢弃率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GcnNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定义一个包含两层GraphConvolution的模型</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim = <span class="number">300</span> ,output_dim =<span class="number">300</span>,dropout = <span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(GcnNet, self).__init__()</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.gcn1 = GraphConvolution(input_dim, output_dim)</span><br><span class="line">        self.gcn2 = GraphConvolution(input_dim, output_dim)</span><br><span class="line">        <span class="comment">#使用一个简单的线性层来估计每个句子的显着性分数,通过softmax将分数归一化并获得我们的显着性分数</span></span><br><span class="line">        self.linear = nn.Linear(input_dim,output_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    S =ELU(A ̃ELU(A ̃XW +b )W +b )</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, adjacency, feature)</span>:</span></span><br><span class="line">        <span class="comment">#采用elu作为激活函数</span></span><br><span class="line">        <span class="comment"># h =  ̃ELU(A X W0 +b0 )   A为邻接矩阵  X为特征矩阵  W0为权重矩阵  B0为偏差矢量</span></span><br><span class="line">        h = F.elu(self.gcn1(adjacency, feature))</span><br><span class="line"></span><br><span class="line">        s = F.dropout(h,p = self.dropout)</span><br><span class="line">        <span class="comment">#S = ̃ELU(A h W1 +b1 )</span></span><br><span class="line">        s = self.gcn2(adjacency, h)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#使用一个简单的线性层来估计每个句子的显着性分数,通过softmax将分数归一化并获得我们的显着性分数</span></span><br><span class="line">        output = self.linear(s)</span><br><span class="line">        <span class="comment">#输出为每个维度的得分 0-1之间</span></span><br><span class="line">        output = F.softmax(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<h2 id="五-模型的训练及结果的评估"><a href="#五-模型的训练及结果的评估" class="headerlink" title="五.模型的训练及结果的评估"></a>五.模型的训练及结果的评估</h2><p> <strong>- 1.前言</strong><br> 模型SemSentSum以端到端的方式训练(end-to-end端到端指的是输入是原始数据，输出是最后的结果)，并且使每个句子的显着性得分预测和ROUGE-1 F1得分之间的等式2的交叉熵损失最小。<br> <img src="https://img-blog.csdnimg.cn/20200729210107803.png" alt="在这里插入图片描述"></p>

    </div>

    
    
    
<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/" rel="tag"><i class="fa fa-tag"></i></a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/28/CurriculumDesign-jdsnCompare/" rel="prev" title="京东苏宁爬虫，商品价格比较">
      <i class="fa fa-chevron-left"></i> 京东苏宁爬虫，商品价格比较
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/09/27/AndrewNg-deepinglearning-ai-note-catalog/" rel="next" title="吴恩达老师深度学习公开课学习笔记目录">
      吴恩达老师深度学习公开课学习笔记目录 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一-数据加载"><span class="nav-number">1.</span> <span class="nav-text">一.数据加载</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二-构建句子的语义关系图（Sentence-Semantic-Relation-Graph）"><span class="nav-number">2.</span> <span class="nav-text">二.构建句子的语义关系图（Sentence Semantic Relation Graph）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三-句子编码（Sentence-Encoder）"><span class="nav-number">3.</span> <span class="nav-text">三.句子编码（Sentence Encoder）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四-图卷积网络（Graph-Convolutional-Network）及显著性估计（Saliency-Estimation）"><span class="nav-number">4.</span> <span class="nav-text">四.图卷积网络（Graph Convolutional Network）及显著性估计（Saliency Estimation）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#五-模型的训练及结果的评估"><span class="nav-number">5.</span> <span class="nav-text">五.模型的训练及结果的评估</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">ccclll777</p>
  <div class="site-description" itemprop="description">胸怀猛虎 细嗅蔷薇</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">37</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ccclll777" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ccclll777" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:sdu945860882@gmail.com" title="E-Mail → mailto:sdu945860882@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.weibo.com/6732062654" title="Weibo → https:&#x2F;&#x2F;www.weibo.com&#x2F;6732062654" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/baidu_41871794" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;baidu_41871794" rel="noopener" target="_blank"><i class="gratipay fa-fw"></i>CSDN</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ccclll777</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">344k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">5:13</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>



        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  















  

  

  

</body>
</html>
