<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>多文档摘要 | ccclll777's blogs</title><meta name="keywords" content="多文档摘要"><meta name="author" content="ccclll777"><meta name="copyright" content="ccclll777"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="多文档摘要论文复现">
<meta property="og:type" content="article">
<meta property="og:title" content="多文档摘要">
<meta property="og:url" content="http://yoursite.com/2020/07/23/Paper-SemSentSum/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="多文档摘要论文复现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png">
<meta property="article:published_time" content="2020-07-23T09:20:08.000Z">
<meta property="article:modified_time" content="2021-10-17T01:36:25.738Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="多文档摘要">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png"><link rel="shortcut icon" href="/images/avatar.png"><link rel="canonical" href="http://yoursite.com/2020/07/23/Paper-SemSentSum/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '多文档摘要',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-10-17 09:36:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="ccclll777's blogs" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ccclll777's blogs</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">多文档摘要</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-07-23T09:20:08.000Z" title="发表于 2020-07-23 17:20:08">2020-07-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-17T01:36:25.738Z" title="更新于 2021-10-17 09:36:25">2021-10-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/">论文复现</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="多文档摘要"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>多文档摘要论文复现</p>
 <span id="more"></span>
<h2 id="一数据加载"><a class="markdownIt-Anchor" href="#一数据加载"></a> 一.数据加载</h2>
<p><strong>- 1.数据集说明</strong><br />
文章采用了Document Understanding Conferences（DUC）上针对最常用的多文档摘要数据集，其中DUC 2001/2002用于训练，DUC 2003用于验证，最后DUC 2004用于测试。</p>
<blockquote>
<p>数据集需要在https://www-nlpir.nist.gov/projects/duc/guidelines.html进行申请</p>
</blockquote>
<p><strong>- 2.读取数据</strong><br />
首先，从文档中读取数据，然后切分数据集，由于机器的性能，我没有读取全部的数据集，只读取了部分的数据集进行实验。我是用正则表达式，从对应的文档中读取了文档的正文以及文档对应的参考摘要。<br />
格式为{doc_no : 文档内容}和{doc_no : 参考摘要}</p>
<p><strong>- 3.切分句子</strong><br />
读取完数据之后，需要将文档切分成句子，存储格式为{doc_no :{sen_id:句子1 ，sen_id2 :句子2 }}</p>
<p><strong>- 4.将切分完的句子建立索引</strong><br />
建立（index-&gt;句子) 的字典，格式为{index1 : doc_no#sen_Id1，index2 : doc_no#sen_Id2}<br />
建立（句子-&gt;index) 的字典，格式为{doc_no#sen_Id1 :index1，doc_no#sen_Id2 : index2}<br />
建立 （文档-&gt;index）的字典，格式为 {doc_no :  [index1,index2…] }<br />
将训练集，验证集，测试集对应的index存储到对应的列表中。</p>
<p><strong>- 5.代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding = utf-8</span><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> util<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LoadData</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        self.data_set = &#123;&#125;<br>        self.data_set_summary = &#123;&#125;<br>        self.current_path = os.path.abspath(os.path.dirname(__file__))  <span class="hljs-comment"># 当前文件路径</span><br>        self.train_set = &#123;&#125;  <span class="hljs-comment"># 训练集</span><br>        self.validation_set = &#123;&#125;  <span class="hljs-comment"># 验证集</span><br>        self.test_set = &#123;&#125;  <span class="hljs-comment"># 测试集</span><br><br>        self.train_set_len = <span class="hljs-number">0</span><br>        self.validation_set_len = <span class="hljs-number">0</span><br>        self.test_set_len = <span class="hljs-number">0</span><br><br>        self.train_set_sentences = &#123;&#125;  <span class="hljs-comment"># 训练集  将文档切分成了句子 存储格式为 &#123;doc_no :&#123;sen_id:句子1 ,sen_id2 :句子2 &#125;&#125;</span><br>        self.validation_set_sentences = &#123;&#125;  <span class="hljs-comment"># 验证集</span><br>        self.test_set_sentences = &#123;&#125;  <span class="hljs-comment"># 测试集</span><br><br><br>        <span class="hljs-comment">#数据集</span><br>        self.index_to_sentence = &#123;&#125;  <span class="hljs-comment"># 下标与 文档句子的映射</span><br><br>        self.sentence_to_index = &#123;&#125;  <span class="hljs-comment"># 文档中的句子与下标的映射</span><br>        <span class="hljs-comment"># 训练集</span><br>        self.train_set_sencence_count = <span class="hljs-number">0</span><br><br>        self.train_set_sentences_list = &#123;&#125;  <span class="hljs-comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span><br><br>        self.train_index = []  <span class="hljs-comment"># 训练集的所有下标</span><br>        <span class="hljs-comment"># 验证集</span><br>        self.validation_set_sencence_count = <span class="hljs-number">0</span><br>        self.validation_set_sentences_list = &#123;&#125;  <span class="hljs-comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span><br><br>        self.validation_index = []  <span class="hljs-comment"># 训练集的所有下标</span><br>        <span class="hljs-comment"># 测试集</span><br>        self.test_set_sencence_count = <span class="hljs-number">0</span><br>        self.test_set_sentences_list = &#123;&#125;  <span class="hljs-comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span><br><br>        self.test_index = []  <span class="hljs-comment"># 训练集的所有下标</span><br><br>        self.train_set_summary = &#123;&#125;  <span class="hljs-comment"># 训练集对应的 参考摘要</span><br>        self.validation_set_summary = &#123;&#125;  <span class="hljs-comment"># 验证集对应的 参考摘要</span><br>        self.test_set_summary = &#123;&#125;  <span class="hljs-comment"># 测试集对应的 参考摘要</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_data</span>(<span class="hljs-params">self</span>):</span><br>        DUC2001_path = self.current_path + <span class="hljs-string">&#x27;/data/DUC/DUC2001_Summarization_Documents/data/training&#x27;</span>  <span class="hljs-comment"># 待读取文件的文件夹地址</span><br>        DUC2001_files = os.listdir(DUC2001_path)  <span class="hljs-comment"># 获得文件夹中所有文件的名称列表</span><br><br>        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> DUC2001_files:<br>            <span class="hljs-keyword">if</span> os.path.isdir(DUC2001_path + <span class="hljs-string">&quot;/&quot;</span> + file):  <span class="hljs-comment"># 判断是否是文件夹</span><br>                doc_path_list = DUC2001_path + <span class="hljs-string">&quot;/&quot;</span> + file + <span class="hljs-string">&quot;/docs&quot;</span><br>                summary_path = DUC2001_path + <span class="hljs-string">&quot;/&quot;</span> + file + <span class="hljs-string">&quot;/&quot;</span> + file + <span class="hljs-built_in">str</span>(file)[-<span class="hljs-number">1</span>]<br>                <span class="hljs-keyword">for</span> doc_path <span class="hljs-keyword">in</span> os.listdir(doc_path_list):<br>                    <span class="hljs-keyword">if</span> os.path.isfile(doc_path_list + <span class="hljs-string">&quot;/&quot;</span> + doc_path):<br>                        self.get_data(doc_path_list + <span class="hljs-string">&quot;/&quot;</span> + doc_path, summary_path + <span class="hljs-string">&quot;/perdocs&quot;</span>)<br>        DUC2001_path_test = self.current_path + <span class="hljs-string">&#x27;/data/DUC/DUC2001_Summarization_Documents/data/test&#x27;</span>  <span class="hljs-comment"># 待读取文件的文件夹地址</span><br>        DUC2001_files_test = os.listdir(DUC2001_path_test + <span class="hljs-string">&quot;/docs&quot;</span>)  <span class="hljs-comment"># 获得文件夹中所有文件的名称列表</span><br>        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> DUC2001_files_test:<br>            <span class="hljs-keyword">if</span> os.path.isdir(DUC2001_path_test + <span class="hljs-string">&quot;/docs/&quot;</span> + file):  <span class="hljs-comment"># 判断是否是文件夹</span><br>                doc_path_list = DUC2001_path_test + <span class="hljs-string">&quot;/docs/&quot;</span> + file<br>                summary_path = DUC2001_path_test + <span class="hljs-string">&quot;/original.summaries/&quot;</span> + file + <span class="hljs-built_in">str</span>(file)[-<span class="hljs-number">1</span>]<br>                <span class="hljs-keyword">for</span> doc_path <span class="hljs-keyword">in</span> os.listdir(doc_path_list):<br>                    <span class="hljs-keyword">if</span> os.path.isfile(doc_path_list + <span class="hljs-string">&quot;/&quot;</span> + doc_path):<br>                        self.get_data(doc_path_list + <span class="hljs-string">&quot;/&quot;</span> + doc_path, summary_path + <span class="hljs-string">&quot;/perdocs&quot;</span>)<br><br>        DUC2001_path_testtraining = self.current_path + <span class="hljs-string">&#x27;/data/DUC/DUC2001_Summarization_Documents/data/testtraining/duc2002testtraining&#x27;</span>  <span class="hljs-comment"># 待读取文件的文件夹地址</span><br>        DUC2001_files_testtraining = os.listdir(DUC2001_path_testtraining)  <span class="hljs-comment"># 获得文件夹中所有文件的名称列表</span><br>        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> DUC2001_files_testtraining:<br>            <span class="hljs-keyword">if</span> os.path.isdir(DUC2001_path_testtraining + <span class="hljs-string">&quot;/&quot;</span> + file):  <span class="hljs-comment"># 判断是否是文件夹</span><br>                path_list = os.listdir(DUC2001_path_testtraining + <span class="hljs-string">&quot;/&quot;</span> + file)<br>                <span class="hljs-keyword">for</span> path <span class="hljs-keyword">in</span> path_list:<br>                    doc_no = path<br>                    fr_doc = <span class="hljs-built_in">open</span>(DUC2001_path_testtraining + <span class="hljs-string">&quot;/&quot;</span> + file + <span class="hljs-string">&quot;/&quot;</span> + path + <span class="hljs-string">&quot;/&quot;</span> + path + <span class="hljs-string">&quot;.body&quot;</span>,<br>                                  encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>                    content = fr_doc.read()<br>                    content = content.replace(<span class="hljs-string">&quot;\n&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)<br>                    self.data_set[doc_no] = content<br>                    fr_summary = <span class="hljs-built_in">open</span>(DUC2001_path_testtraining + <span class="hljs-string">&quot;/&quot;</span> + file + <span class="hljs-string">&quot;/&quot;</span> + path + <span class="hljs-string">&quot;/&quot;</span> + path + <span class="hljs-string">&quot;.abs&quot;</span>,<br>                                      encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>                    summary = fr_summary.read()<br>                    summary = summary.replace(<span class="hljs-string">&quot;\n&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)<br>                    self.data_set_summary[doc_no] = summary<br>        length = <span class="hljs-built_in">len</span>(self.data_set)<br>        self.train_set_len = <span class="hljs-built_in">int</span>(length * <span class="hljs-number">0.8</span>)<br>        self.validation_set_len = <span class="hljs-built_in">int</span>(length * <span class="hljs-number">0.1</span>)<br>        self.test_set_len = <span class="hljs-built_in">int</span>(length * <span class="hljs-number">0.1</span>)<br>        <span class="hljs-comment"># 切分读取的数据为训练集 验证集 测试集</span><br>        index = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> doc_no,content <span class="hljs-keyword">in</span> self.data_set.items():<br>            <span class="hljs-keyword">if</span> index &lt; self.train_set_len:<br>                self.train_set[doc_no] = content<br>                self.train_set_summary[doc_no] = self.data_set_summary[doc_no]<br>            <span class="hljs-keyword">elif</span> index &gt; self.train_set_len <span class="hljs-keyword">and</span> index&lt;self.train_set_len+self.validation_set_len:<br>                self.validation_set[doc_no] = content<br>                self.validation_set_summary[doc_no] = self.data_set_summary[doc_no]<br>            <span class="hljs-keyword">elif</span> index &gt; self.train_set_len+self.validation_set_len <span class="hljs-keyword">and</span> index &lt; length:<br>                self.test_set[doc_no] = content<br>                self.test_set_summary[doc_no] = self.data_set_summary[doc_no]<br>            index +=<span class="hljs-number">1</span><br>      <span class="hljs-string">&quot;&quot;&quot;利用正则表达式，提取文档内容和文档的摘要&quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_data</span>(<span class="hljs-params">self, doc_path, summary_path</span>):</span><br>        fr = <span class="hljs-built_in">open</span>(doc_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>        content = fr.read()<br>        content = content.replace(<span class="hljs-string">&quot;\n&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)<br>        doc = re.findall(<span class="hljs-string">&quot;&lt;TEXT&gt;(.*?)&lt;/TEXT&gt;&quot;</span>, content)[<span class="hljs-number">0</span>]<br>        doc_no = re.findall(<span class="hljs-string">&quot;&lt;DOCNO&gt;(.*?)&lt;/DOCNO&gt;&quot;</span>, content)[<span class="hljs-number">0</span>].replace(<span class="hljs-string">&quot; &quot;</span>, <span class="hljs-string">&quot;&quot;</span>)<br>        self.data_set[doc_no] = doc.replace(<span class="hljs-string">&quot;&lt;p&gt;&quot;</span>, <span class="hljs-string">&quot;&quot;</span>).replace(<span class="hljs-string">&quot;&lt;/p&gt;&quot;</span>, <span class="hljs-string">&quot;&quot;</span>).replace(<span class="hljs-string">&quot;&lt;P&gt;&quot;</span>, <span class="hljs-string">&quot;&quot;</span>).replace(<br>            <span class="hljs-string">&quot;&lt;/P&gt;&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)<br>        fr = <span class="hljs-built_in">open</span>(summary_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>        summary_list = fr.read()<br>        summary_list = summary_list.replace(<span class="hljs-string">&quot;\n&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)<br>        summary = re.findall(<span class="hljs-string">&#x27;&lt;SUM.*?DOCREF=&quot;&#123;&#125;.*?&quot;&gt;(.*?)&lt;/SUM&gt;&#x27;</span>.<span class="hljs-built_in">format</span>(doc_no), summary_list)[<span class="hljs-number">0</span>]<br>        self.data_set_summary[doc_no] = summary<br><br>    <span class="hljs-string">&quot;&quot;&quot;将文档切分成句子&quot;&quot;&quot;</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cut_doc_to_sentences</span>(<span class="hljs-params">self, <span class="hljs-built_in">set</span>=<span class="hljs-string">&quot;train_set&quot;</span></span>):</span><br>        sentences_count = <span class="hljs-number">0</span>  <span class="hljs-comment"># 句子数量</span><br>        j = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">set</span> == <span class="hljs-string">&quot;train_set&quot;</span>:<br>            <span class="hljs-comment"># 读取数据</span><br>            <span class="hljs-keyword">for</span> doc_no, doc_content <span class="hljs-keyword">in</span> self.train_set.items():<br>                <span class="hljs-comment"># 将文档切分成句子</span><br>                sentence_list = util.cut_doc_to_sentences(doc_content)<br>                <span class="hljs-keyword">if</span> (<span class="hljs-built_in">len</span>(sentence_list)) == <span class="hljs-number">0</span>:<br>                    <span class="hljs-keyword">break</span><br><br>                document_dict = &#123;&#125;<br>                sentences_count += <span class="hljs-built_in">len</span>(sentence_list)<br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sentence_list)):<br>                    <span class="hljs-comment"># 将句子编号 然后存入字典中</span><br>                    document_dict[<span class="hljs-string">&quot;sen_id_&quot;</span> + <span class="hljs-built_in">str</span>(i)] = sentence_list[i]<br>                self.train_set_sentences[doc_no] = document_dict<br>            self.train_set_sencence_count = sentences_count<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">set</span> == <span class="hljs-string">&quot;validation_set&quot;</span>:<br>            <span class="hljs-keyword">for</span> doc_no, doc_content <span class="hljs-keyword">in</span> self.validation_set.items():<br>                sentence_list = util.cut_doc_to_sentences(doc_content)<br>                <span class="hljs-keyword">if</span> (<span class="hljs-built_in">len</span>(sentence_list)) == <span class="hljs-number">0</span>:<br>                    <span class="hljs-keyword">break</span><br>                document_dict = &#123;&#125;<br>                sentences_count += <span class="hljs-built_in">len</span>(sentence_list)<br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sentence_list)):<br>                    document_dict[<span class="hljs-string">&quot;sen_id_&quot;</span> + <span class="hljs-built_in">str</span>(i)] = sentence_list[i]<br>                self.validation_set_sentences[doc_no] = document_dict<br>            self.validation_set_sencence_count = sentences_count<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">set</span> == <span class="hljs-string">&quot;test_set&quot;</span>:<br>            <span class="hljs-keyword">for</span> doc_no, doc_content <span class="hljs-keyword">in</span> self.test_set.items():<br>                sentence_list = util.cut_doc_to_sentences(doc_content)<br>                <span class="hljs-keyword">if</span> (<span class="hljs-built_in">len</span>(sentence_list)) == <span class="hljs-number">0</span>:<br>                    <span class="hljs-keyword">break</span><br>                document_dict = &#123;&#125;<br>                sentences_count += <span class="hljs-built_in">len</span>(sentence_list)<br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(sentence_list)):<br>                    document_dict[<span class="hljs-string">&quot;sen_id_&quot;</span> + <span class="hljs-built_in">str</span>(i)] = sentence_list[i]<br>                self.test_set_sentences[doc_no] = document_dict<br>            self.test_set_sencence_count = sentences_count<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_index</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 构造句子和矩阵下标的映射</span><br>        index = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> doc_no, sentences <span class="hljs-keyword">in</span> self.train_set_sentences.items():<br>            <span class="hljs-comment"># 遍历文档中的每个句子</span><br>            sentence_index_list = []<br>            <span class="hljs-keyword">for</span> sen_id, sentence <span class="hljs-keyword">in</span> sentences.items():<br>                self.index_to_sentence[index] = doc_no + <span class="hljs-string">&quot;#&quot;</span> + sen_id<br>                self.sentence_to_index[doc_no + <span class="hljs-string">&quot;#&quot;</span> + sen_id] = index<br>                sentence_index_list.append(index)<br>                self.train_index.append(index)<br>                index += <span class="hljs-number">1</span><br><br>            self.train_set_sentences_list[doc_no] = sentence_index_list<br><br>        <span class="hljs-comment"># 构造句子和矩阵下标的映射</span><br>        <span class="hljs-keyword">for</span> doc_no, sentences <span class="hljs-keyword">in</span> self.validation_set_sentences.items():<br>            <span class="hljs-comment"># 遍历文档中的每个句子</span><br>            sentence_index_list = []<br>            <span class="hljs-keyword">for</span> sen_id, sentence <span class="hljs-keyword">in</span> sentences.items():<br>                self.index_to_sentence[index] = doc_no + <span class="hljs-string">&quot;#&quot;</span> + sen_id<br>                self.sentence_to_index[doc_no + <span class="hljs-string">&quot;#&quot;</span> + sen_id] = index<br>                sentence_index_list.append(index)<br>                self.validation_index.append(index)<br>                index += <span class="hljs-number">1</span><br>            self.validation_set_sentences_list[doc_no] = sentence_index_list<br>        <span class="hljs-comment"># 构造句子和矩阵下标的映射</span><br>        <span class="hljs-keyword">for</span> doc_no, sentences <span class="hljs-keyword">in</span> self.test_set_sentences.items():<br>            <span class="hljs-comment"># 遍历文档中的每个句子</span><br>            sentence_index_list = []<br>            <span class="hljs-keyword">for</span> sen_id, sentence <span class="hljs-keyword">in</span> sentences.items():<br>                self.index_to_sentence[index] = doc_no + <span class="hljs-string">&quot;#&quot;</span> + sen_id<br>                self.sentence_to_index[doc_no + <span class="hljs-string">&quot;#&quot;</span> + sen_id] = index<br>                sentence_index_list.append(index)<br>                self.test_index.append(index)<br>                index += <span class="hljs-number">1</span><br>            self.test_set_sentences_list[doc_no] = sentence_index_list<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read</span>(<span class="hljs-params">self</span>):</span><br>        self.read_data()<br>        self.cut_doc_to_sentences(<span class="hljs-built_in">set</span>=<span class="hljs-string">&quot;train_set&quot;</span>)<br>        self.cut_doc_to_sentences(<span class="hljs-built_in">set</span>=<span class="hljs-string">&quot;validation_set&quot;</span>)<br>        self.cut_doc_to_sentences(<span class="hljs-built_in">set</span>=<span class="hljs-string">&quot;test_set&quot;</span>)<br>        self.create_index()<br><br></code></pre></td></tr></table></figure>
<h2 id="二构建句子的语义关系图sentence-semantic-relation-graph"><a class="markdownIt-Anchor" href="#二构建句子的语义关系图sentence-semantic-relation-graph"></a> 二.构建句子的语义关系图（Sentence Semantic Relation Graph）</h2>
<ul>
<li><strong>1.解释：</strong></li>
</ul>
<p>（1）用图对句子建模，图的顶点为文档i中的句子j（Si，j），边为两个句子之间的相似程度。需要使用英语Wikipedia语料库上训练的的模型进行句子嵌入（sentence embeddings），产生句子的向量，然后根据向量计算句子之间的余弦相似度，然后构建矩阵。<br />
（2）引入一个阈值t，去除相似度小于阈值t的边，以强调较高的句子相似度，避免模型无法显著地利用句子之间的语义结构</p>
<ul>
<li><strong>2.sentence embeddings环境安装工作</strong></li>
</ul>
<p>我找到了（Pagliardini et al. (2018)）的论文中描述的模型的开源实现，他可以在fasttext库的基础上，进行句子嵌入。</p>
<blockquote>
<p>sent2vec模型地址：<a target="_blank" rel="noopener" href="https://github.com/epfml/sent2vec">https://github.com/epfml/sent2vec</a></p>
</blockquote>
<p>（1）由于他基于fasttext，所以先下载并编译了fasttext，具体方法他在github中写的非常清楚，在他们目录文件中写好了Makefile，直接用本地的gcc环境进行编译</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/fastText">https://github.com/facebookresearch/fastText</a></p>
</blockquote>
<p>（2）下载论文中描述的的wiki百科600维的预训练unigram模型，在fasttext中进行使用</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://drive.google.com/uc?id=0B6VhzidiLvjSa19uYWlLUEkzX3c&amp;export=download">https://drive.google.com/uc?id=0B6VhzidiLvjSa19uYWlLUEkzX3c&amp;export=download</a></p>
</blockquote>
<p>（3）下载了斯坦福解析器，配合nltk库进行Tokenizer</p>
<blockquote>
<p>解析器下载：<a target="_blank" rel="noopener" href="https://nlp.stanford.edu/software/lex-parser.shtml#Download">https://nlp.stanford.edu/software/lex-parser.shtml#Download</a><br />
使用教程：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36652619/article/details/75091327">https://blog.csdn.net/qq_36652619/article/details/75091327</a></p>
</blockquote>
<ul>
<li><strong>3.使用nltk库和斯坦福解析器，进行Tokenizer，然后使用wiki百科600维的预训练unigram模型进行sentence embeddings</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> subprocess <span class="hljs-keyword">import</span> call<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> nltk.tokenize.stanford <span class="hljs-keyword">import</span> StanfordTokenizer<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SentencesEmbeddings</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br><br>        <span class="hljs-comment">#fasttext执行</span><br>        self.FASTTEXT_EXEC_PATH = os.path.abspath(<span class="hljs-string">&quot;./sent2vec-master/fasttext&quot;</span>)<br>        <span class="hljs-comment">#斯坦福分词器的路径</span><br>        self.BASE_SNLP_PATH = <span class="hljs-string">&quot;sent2vec-master/stanford-postagger-full/&quot;</span><br>        self.SNLP_TAGGER_JAR = os.path.join(self.BASE_SNLP_PATH, <span class="hljs-string">&quot;stanford-postagger.jar&quot;</span>)<br>        <span class="hljs-comment">#wiki百科预训练模型的路径</span><br>        self.MODEL_WIKI_UNIGRAMS = os.path.abspath(<span class="hljs-string">&quot;sent2vec-master/wiki_unigrams.bin&quot;</span>)<br>        self.tknzr = StanfordTokenizer(self.SNLP_TAGGER_JAR, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>        print(<span class="hljs-string">&quot;SentencesEmbeddings初始化完成&quot;</span>)<br>    <span class="hljs-comment">#将句子去除符号，大小写转换后，进行分词</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize</span>(<span class="hljs-params">self, sentence, to_lower=<span class="hljs-literal">True</span></span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;Arguments:</span><br><span class="hljs-string">            - tknzr: a tokenizer implementing the NLTK tokenizer interface</span><br><span class="hljs-string">            - sentence: a string to be tokenized</span><br><span class="hljs-string">            - to_lower: lowercasing or not</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        sentence = sentence.strip()<br>        sentence = <span class="hljs-string">&#x27; &#x27;</span>.join([self.format_token(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span>  self.tknzr.tokenize(sentence)])<br>        <span class="hljs-keyword">if</span> to_lower:<br>            sentence = sentence.lower()<br>        sentence = re.sub(<span class="hljs-string">&#x27;((www\.[^\s]+)|(https?://[^\s]+)|(http?://[^\s]+))&#x27;</span>,<span class="hljs-string">&#x27;&lt;url&gt;&#x27;</span>,sentence) <span class="hljs-comment">#replace urls by &lt;url&gt;</span><br>        sentence = re.sub(<span class="hljs-string">&#x27;(\@[^\s]+)&#x27;</span>,<span class="hljs-string">&#x27;&lt;user&gt;&#x27;</span>,sentence) <span class="hljs-comment">#replace @user268 by &lt;user&gt;</span><br>        <span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> word: <span class="hljs-string">&#x27; &#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> word, sentence)<br>        <span class="hljs-keyword">return</span> sentence<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">format_token</span>(<span class="hljs-params">self,token</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> token == <span class="hljs-string">&#x27;-LRB-&#x27;</span>:<br>            token = <span class="hljs-string">&#x27;(&#x27;</span><br>        <span class="hljs-keyword">elif</span> token == <span class="hljs-string">&#x27;-RRB-&#x27;</span>:<br>            token = <span class="hljs-string">&#x27;)&#x27;</span><br>        <span class="hljs-keyword">elif</span> token == <span class="hljs-string">&#x27;-RSB-&#x27;</span>:<br>            token = <span class="hljs-string">&#x27;]&#x27;</span><br>        <span class="hljs-keyword">elif</span> token == <span class="hljs-string">&#x27;-LSB-&#x27;</span>:<br>            token = <span class="hljs-string">&#x27;[&#x27;</span><br>        <span class="hljs-keyword">elif</span> token == <span class="hljs-string">&#x27;-LCB-&#x27;</span>:<br>            token = <span class="hljs-string">&#x27;&#123;&#x27;</span><br>        <span class="hljs-keyword">elif</span> token == <span class="hljs-string">&#x27;-RCB-&#x27;</span>:<br>            token = <span class="hljs-string">&#x27;&#125;&#x27;</span><br>        <span class="hljs-keyword">return</span> token<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_sentences</span>(<span class="hljs-params">self, sentences, to_lower=<span class="hljs-literal">True</span></span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;Arguments:</span><br><span class="hljs-string">            - tknzr: 斯坦福解析器</span><br><span class="hljs-string">            - sentences:句子列表</span><br><span class="hljs-string">            - to_lower: 是否转化成消协</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment">#返回token化的结果</span><br>        <span class="hljs-keyword">return</span> [self.tokenize( s, to_lower) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> sentences]<br><br><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_embeddings_for_preprocessed_sentences</span>(<span class="hljs-params">self,sentences, model_path, fasttext_exec_path</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;Arguments:</span><br><span class="hljs-string">            - sentences:分词后的结果</span><br><span class="hljs-string">            - model_path: wiki百科模型文件的路径</span><br><span class="hljs-string">            - fasttext_exec_path: fasttext的路径</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        timestamp = <span class="hljs-built_in">str</span>(time.time())<br>        test_path = os.path.abspath(<span class="hljs-string">&#x27;./&#x27;</span>+timestamp+<span class="hljs-string">&#x27;_fasttext.test.txt&#x27;</span>)<br>        embeddings_path = os.path.abspath(<span class="hljs-string">&#x27;./&#x27;</span>+timestamp+<span class="hljs-string">&#x27;_fasttext.embeddings.txt&#x27;</span>)<br>        self.dump_text_to_disk(test_path, sentences)<br>        call(fasttext_exec_path+<br>              <span class="hljs-string">&#x27; print-sentence-vectors &#x27;</span>+<br>              model_path + <span class="hljs-string">&#x27; &lt; &#x27;</span>+<br>              test_path + <span class="hljs-string">&#x27; &gt; &#x27;</span> +<br>              embeddings_path, shell=<span class="hljs-literal">True</span>)<br>        embeddings = self.read_embeddings(embeddings_path)<br>        os.remove(test_path)<br>        os.remove(embeddings_path)<br>        <span class="hljs-keyword">assert</span>(<span class="hljs-built_in">len</span>(sentences) == <span class="hljs-built_in">len</span>(embeddings))<br>        <span class="hljs-keyword">return</span> np.array(embeddings)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_embeddings</span>(<span class="hljs-params">self,embeddings_path</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;Arguments:</span><br><span class="hljs-string">            - embeddings_path: path to the embeddings</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(embeddings_path, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> in_stream:<br>            embeddings = []<br>            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> in_stream:<br>                line = <span class="hljs-string">&#x27;[&#x27;</span>+line.replace(<span class="hljs-string">&#x27; &#x27;</span>,<span class="hljs-string">&#x27;,&#x27;</span>)+<span class="hljs-string">&#x27;]&#x27;</span><br>                embeddings.append(<span class="hljs-built_in">eval</span>(line))<br>            <span class="hljs-keyword">return</span> embeddings<br>        <span class="hljs-keyword">return</span> []<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dump_text_to_disk</span>(<span class="hljs-params">self,file_path, X, Y=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;Arguments:</span><br><span class="hljs-string">            - file_path: where to dump the data</span><br><span class="hljs-string">            - X: list of sentences to dump</span><br><span class="hljs-string">            - Y: labels, if any</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> out_stream:<br>            <span class="hljs-keyword">if</span> Y <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(X, Y):<br>                    out_stream.write(<span class="hljs-string">&#x27;__label__&#x27;</span>+<span class="hljs-built_in">str</span>(y)+<span class="hljs-string">&#x27; &#x27;</span>+x+<span class="hljs-string">&#x27; \n&#x27;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X:<br>                    out_stream.write(x+<span class="hljs-string">&#x27; \n&#x27;</span>)<br>    <span class="hljs-comment">#输入为一个句子的列表 返回embeeding之后的数据 600纬</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_sentence_embeddings</span>(<span class="hljs-params">self,sentences</span>):</span><br>        wiki_embeddings = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">#加载斯坦福的分词器</span><br><br>        <span class="hljs-comment">#进行分词操作</span><br>        s = <span class="hljs-string">&#x27; &lt;delimiter&gt; &#x27;</span>.join(sentences)<br>        tokenized_sentences_SNLP = self.tokenize_sentences([s])<br>        tokenized_sentences_SNLP = tokenized_sentences_SNLP[<span class="hljs-number">0</span>].split(<span class="hljs-string">&#x27; &lt;delimiter&gt; &#x27;</span>)<br>        <span class="hljs-keyword">assert</span>(<span class="hljs-built_in">len</span>(tokenized_sentences_SNLP) == <span class="hljs-built_in">len</span>(sentences))<br>        <span class="hljs-comment">#使用wiki百科预训练的模型进行embeddings</span><br>        wiki_embeddings = self.get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \<br>                                         self.MODEL_WIKI_UNIGRAMS, self.FASTTEXT_EXEC_PATH)<br>        <span class="hljs-keyword">return</span> wiki_embeddings<br><br><br><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">embeddings</span>(<span class="hljs-params">self,sentences</span>):</span><br><br>        my_embeddings = self.get_sentence_embeddings(sentences)<br>        <span class="hljs-keyword">return</span> my_embeddings<br><br></code></pre></td></tr></table></figure>
<ul>
<li><strong>4.句子相似度的计算</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env Python</span><br><span class="hljs-comment"># coding=utf-8</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">构造语义关系图  以及进行sentence encoder</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">import</span> SentencesEmbeddings<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> util<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SentenceSemanticRelationGraph</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, train_set_sentences,</span></span><br><span class="hljs-function"><span class="hljs-params">                 validation_set_sentences,</span></span><br><span class="hljs-function"><span class="hljs-params">                 test_set_sentences,</span></span><br><span class="hljs-function"><span class="hljs-params">                 train_set_sencence_count,</span></span><br><span class="hljs-function"><span class="hljs-params">                 validation_set_sencence_count,</span></span><br><span class="hljs-function"><span class="hljs-params">                 test_set_sencence_count,</span></span><br><span class="hljs-function"><span class="hljs-params">                 sentence_to_index,</span></span><br><span class="hljs-function"><span class="hljs-params">                 index_to_sentence,</span></span><br><span class="hljs-function"><span class="hljs-params">                 train_set_sentences_list,</span></span><br><span class="hljs-function"><span class="hljs-params">                 validation_set_sentences_list,</span></span><br><span class="hljs-function"><span class="hljs-params">                 test_set_sentences_list,</span></span><br><span class="hljs-function"><span class="hljs-params">                 train_index,</span></span><br><span class="hljs-function"><span class="hljs-params">                 validation_index,</span></span><br><span class="hljs-function"><span class="hljs-params">                 test_index</span>):</span><br>        self.sentences_embeddings = SentencesEmbeddings.SentencesEmbeddings()<br>        self.train_set_sentences = train_set_sentences  <span class="hljs-comment"># 训练集  将文档切分成了句子 存储格式为 &#123;doc_no :&#123;sen_id:句子1 ,sen_id2 :句子2 &#125;&#125;</span><br>        self.validation_set_sentences = validation_set_sentences  <span class="hljs-comment"># 验证集</span><br>        self.test_set_sentences = test_set_sentences  <span class="hljs-comment"># 测试集</span><br>        self.train_set_sencence_count = train_set_sencence_count <span class="hljs-comment">#训练集句子数量</span><br>        self.validation_set_sencence_count = validation_set_sencence_count  <span class="hljs-comment"># 验证集句子数量</span><br>        self.test_set_sencence_count = test_set_sencence_count  <span class="hljs-comment"># 测试集句子数量</span><br>        self.sentence_count = train_set_sencence_count+validation_set_sencence_count+test_set_sencence_count<br>        self.data_set_sentences_embeddings = &#123;&#125; <span class="hljs-comment">#存储句子嵌入的集合</span><br><br>        self.data_set_sentences = &#123;&#125;<br>        self.tgsim = <span class="hljs-number">0.35</span>  <span class="hljs-comment">#句子之间余弦相似度的阈值  大于阈值的边才会被保留</span><br><br>        <span class="hljs-comment">#数据集相关</span><br>        self.index_to_sentence = index_to_sentence  <span class="hljs-comment"># 余弦相似度矩阵的下标与 文档句子的映射</span><br><br>        self.sentence_to_index = sentence_to_index  <span class="hljs-comment"># 文档中的句子与余弦相似度矩阵下标的映射</span><br>        self.data_set_cosine_similarity_matrix = <span class="hljs-literal">None</span>  <span class="hljs-comment"># 存储余弦相似度的矩阵</span><br>        self.data_set_sentences_list = &#123;&#125;<br><br>        <span class="hljs-comment">#训练集相关</span><br>        self.train_index = train_index <span class="hljs-comment">#训练集的下标</span><br>        self.train_set_sentences_list = train_set_sentences_list  <span class="hljs-comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span><br>        <span class="hljs-comment">#验证集相关</span><br>        self.validation_index = validation_index  <span class="hljs-comment"># 验证集的下标</span><br><br>        self.validation_set_sentences_list = validation_set_sentences_list  <span class="hljs-comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span><br>        <span class="hljs-comment">#测试集相关</span><br>        self.test_index = test_index  <span class="hljs-comment"># 测试集的下标</span><br><br>        self.test_set_sentences_list = test_set_sentences_list  <span class="hljs-comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    处理已经读入的训练集 测试集  验证集</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deal_data</span>(<span class="hljs-params">self</span>):</span><br>       <span class="hljs-keyword">for</span> doc_no, sentences_list <span class="hljs-keyword">in</span> self.train_set_sentences_list.items():<br>           self.data_set_sentences_list[doc_no] = sentences_list<br>       <span class="hljs-keyword">for</span> doc_no, sentences_list <span class="hljs-keyword">in</span> self.validation_set_sentences_list.items():<br>           self.data_set_sentences_list[doc_no] = sentences_list<br>       <span class="hljs-keyword">for</span> doc_no, sentences_list <span class="hljs-keyword">in</span> self.test_set_sentences_list.items():<br>           self.data_set_sentences_list[doc_no] = sentences_list<br>       <span class="hljs-keyword">for</span> doc_no, sentences_list <span class="hljs-keyword">in</span> self.train_set_sentences.items():<br>           self.data_set_sentences[doc_no] = sentences_list<br>       <span class="hljs-keyword">for</span> doc_no, sentences_list <span class="hljs-keyword">in</span> self.validation_set_sentences.items():<br>           self.data_set_sentences[doc_no] = sentences_list<br>       <span class="hljs-keyword">for</span> doc_no, sentences_list <span class="hljs-keyword">in</span> self.test_set_sentences.items():<br>           self.data_set_sentences[doc_no] = sentences_list<br><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    调用方法计算句子嵌入 </span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_sentences_embeddings</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 以文档为单位计算句子嵌入  遍历每个文档的每一句话的index</span><br>        <span class="hljs-keyword">for</span> doc_no, sentence_index_list <span class="hljs-keyword">in</span> self.data_set_sentences_list.items():<br>            index_list = []<br>            sentence_list = []<br>            <span class="hljs-comment"># 根据index找到对应的句子</span><br>            <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> sentence_index_list:<br>                <span class="hljs-comment"># sen_id = str(self.train_set_index_to_sentence[index]).split(&quot;#&quot;)[1]</span><br>                sen_id = <span class="hljs-built_in">str</span>(self.index_to_sentence[index]).split(<span class="hljs-string">&quot;#&quot;</span>)[<span class="hljs-number">1</span>]<br>                sentence = self.data_set_sentences[doc_no][sen_id]<br>                sentence_list.append(sentence)<br>                index_list.append(index)<br>            <span class="hljs-comment"># 将文档中所有的句子加入列表，统一计算句子嵌入</span><br>            sentence_embedding_list = self.sentences_embeddings.embeddings(sentence_list)<br>            <span class="hljs-comment"># 然后加入句子嵌入的列表</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(index_list)):<br>                self.data_set_sentences_embeddings[index_list[i]] = sentence_embedding_list[i]<br><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算句子间的余弦相似度 构造语义关系图</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_cosine_similarity_matrix</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 初始化余弦相似度的矩阵</span><br>        self.data_set_cosine_similarity_matrix = np.zeros(<br>            (self.sentence_count, self.sentence_count))<br>        <span class="hljs-comment"># 遍历映射 进行余弦相似度的计算</span><br>        <span class="hljs-keyword">for</span> index, <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> self.index_to_sentence.items():<br>            <span class="hljs-keyword">for</span> index2, id2 <span class="hljs-keyword">in</span> self.index_to_sentence.items():<br>                <span class="hljs-keyword">if</span> index == index2 :<br>                    <span class="hljs-keyword">break</span><br>                <span class="hljs-keyword">if</span> self.data_set_cosine_similarity_matrix[index][index2] == <span class="hljs-number">0</span>:<br>                    <span class="hljs-comment"># 句子1 的嵌入</span><br>                    embeddings1 = self.data_set_sentences_embeddings[index]<br>                    <span class="hljs-comment"># 句子2的嵌入</span><br>                    embeddings2 = self.data_set_sentences_embeddings[index2]<br>                    cosine_similarity = self.calculate_cosine_similarity(embeddings1, embeddings2)<br>                    <span class="hljs-comment"># 如果大于阈值 在进行存储</span><br>                    <span class="hljs-keyword">if</span> cosine_similarity &gt; self.tgsim:<br>                        self.data_set_cosine_similarity_matrix[index][index2] = cosine_similarity<br>                        self.data_set_cosine_similarity_matrix[index2][index] = cosine_similarity<br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">continue</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算两个numpy向量之间的余弦相似度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_cosine_similarity</span>(<span class="hljs-params">self,vector1,vector2</span>):</span><br>        cosine_similarity = np.dot(vector1,vector2)/(np.linalg.norm(vector1)*(np.linalg.norm(vector2)))<br>        <span class="hljs-keyword">return</span> cosine_similarity<br></code></pre></td></tr></table></figure>
<p>解释：</p>
<ol>
<li>
<p>获得读取到的数据，将其构成几个集合：<br />
（1）data_set_sentences_list——表示文档的句子的索引,存储方式为{doc_no : [index1,index2]}，表示一篇文档句子的存储位置<br />
（2）index_to_sentence——表示文档句子的索引与文档句子编号的映射（可以根据index找到属于那篇文档的那个句子），存储方式为{index : doc_no#sen_id}<br />
（3）sentence_to_index——表示文档句子编号与文档句子索引的的映射（可以根据某文档的某个句子找到他存储的index，存储方式为{doc_no#sen_id : index}<br />
（4）data_set_sentences——表示具体文档句子id与文档内容的映射，存储方式为{doc_no#sen_id : 句子内容}</p>
</li>
<li>
<p>然后调用方法计算每一篇文档的sentence embeddings，存储格式为{reviewer_id1：{sen_id1：embeddings1，sen_id2：embeddings2} ，reviewer_id2：{sen_id1：embeddings1，sen_id2：embeddings2} }。</p>
</li>
<li>
<p>最后借助sentence embeddings，来计算句子语义关系图data_set_cosine_similarity_matrix</p>
</li>
</ol>
<p>至此，第一部分的工作完成。</p>
<h2 id="三句子编码sentence-encoder"><a class="markdownIt-Anchor" href="#三句子编码sentence-encoder"></a> 三.句子编码（Sentence Encoder）</h2>
<p><strong>- 1.解释</strong><br />
在构建完语义关系图之后，还需要对训练集中的所有单词，使用300维的预先训练的GloVe嵌入（Pen- nington et al., 2014）进行单词嵌入（word embeddings），然后将单词嵌入输入句子编码器以计算句子嵌入，句子编码器使用了单层正向循环神经网络（ a single-layer forward recurrent neural network）的变体，长短时记忆神经网络LSTM，最后从隐藏层中提取句子嵌入，然后，将所有句子嵌入连接到一个矩阵X中，该矩阵X将构成图卷积网络将使用的输入节点特征。</p>
<p><strong>- 2.循环神经网络RNN的学习</strong></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/50915723">https://zhuanlan.zhihu.com/p/50915723</a><br />
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/30844905">https://zhuanlan.zhihu.com/p/30844905</a></p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/202007231606529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。对于文本数据，句子前后是有语义关系的，所以RNN正好可以处理句子前后关系的影响。但是RNN也有序列过长的情况下梯度消失或者梯度爆炸的问题。</p>
<p><strong>- 3.LSTM：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/2020072316103727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
<img src="https://img-blog.csdnimg.cn/20200723161135460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
用pytorch实现lstm：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/master/generated/torch.nn.LSTM.html">https://pytorch.org/docs/master/generated/torch.nn.LSTM.html</a><br />
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/79064602">https://zhuanlan.zhihu.com/p/79064602</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python">device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EncoderRNN</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_size, hidden_size,batch_size</span>):</span><br>        <span class="hljs-built_in">super</span>(EncoderRNN, self).__init__()<br>        self.hidden_size = hidden_size <span class="hljs-comment">#</span><br>        self.input_size = input_size <span class="hljs-comment">#每个单词向量的长度</span><br>        self.batch_size = batch_size <span class="hljs-comment">#句子的个数</span><br>        <span class="hljs-comment"># requires_grad指定是否在训练过程中对词向量的权重进行微调</span><br>        <span class="hljs-comment"># self.embedding.weight.requires_grad = True</span><br>        self.lstm = nn.LSTM(self.input_size, hidden_size)<br>        <span class="hljs-comment"># self.gru = nn.GRU(hidden_size, hidden_size)</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>, hidden,cell,seq_len,batch_size</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">        :param input:</span><br><span class="hljs-string">        :param hidden:</span><br><span class="hljs-string">        :param cell:</span><br><span class="hljs-string">        :param seq_len:     句子的长度</span><br><span class="hljs-string">        :param batch_size:  句子的个数</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># embedded = self.embedding[input].view(1, 1, self.hidden_size)</span><br>        <span class="hljs-comment"># embedded = input.view(len(input), 1, 300)</span><br>        embedded = <span class="hljs-built_in">input</span>.view(seq_len, batch_size, <span class="hljs-number">300</span>)<br>        <span class="hljs-comment"># output, hidden = self.gru(embedded, hidden)</span><br>        output , (hidden, cell)= self.lstm(embedded.<span class="hljs-built_in">float</span>(), (hidden.<span class="hljs-built_in">float</span>(),cell.<span class="hljs-built_in">float</span>()))<br>        <span class="hljs-keyword">return</span> output,hidden,cell<br></code></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200723162557845.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
<img src="https://img-blog.csdnimg.cn/20200723163050742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<p>（1）torch.nn.LSTM(*args, kwargs)<br />
参数列表：<br />
– input_size ：输入特征维数，如我们输入的是300维的预先训练的GloVe嵌入产生的词向量<br />
– hidden_size：隐藏层状态的维数，即隐藏层节点的个数，这里我选择输入的也是300维<br />
– num_layers： LSTM 堆叠的层数，默认值是1层，我们的神经网络也是单层的<br />
– bias： 隐层状态是否带bias，默认为true。bias是偏置值，或者偏移值。没有偏置值就是以0为中轴，或以0为起点。<br />
– batch_first：输入输出的第一维是否为 batch_size，默认值 False。因为 Torch 中，人们习惯使用Torch中带有的dataset，dataloader向神经网络模型连续输入数据，这里面就有一个 batch_size 的参数，表示一次输入多少个数据。 在 LSTM 模型中，输入数据必须是一批数据，为了区分LSTM中的批量数据和dataloader中的批量数据是否相同意义，LSTM 模型就通过这个参数的设定来区分。 如果是相同意义的，就设置为True，如果不同意义的，设置为False。 torch.LSTM 中 batch_size 维度默认是放在第二维度，故此参数设置可以将 batch_size 放在第一维度。如：input 默认是(4,1,5)，中间的 1 是 batch_size，指定batch_first=True后就是(1,4,5)。所以，如果你的输入数据是二维数据的话，就应该将 batch_first 设置为True;</p>
<p>– dropout： 默认值0。是否在除最后一个 RNN 层外的其他 RNN 层后面加 dropout 层。输入值是 0-1 之间的小数，表示概率。0表示0概率dripout，即不dropout<br />
– bidirectional： 是否是双向 RNN，默认为：false，若为 true，则：num_directions=2，否则为1。<br />
（2）向前传播时的输入<br />
<img src="https://img-blog.csdnimg.cn/20200729175411378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<p><img src="https://img-blog.csdnimg.cn/20200723163451721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
<img src="https://img-blog.csdnimg.cn/20200729175639695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
输入的张量的每一个维度都有固定的含义，不能弄错，需要在理解之后，才能进行修改。</p>
<ul>
<li><strong>4.word embeddings（使用预训练的glove进行词嵌入）</strong></li>
</ul>
<p>（1）下载预训练的模型</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></p>
</blockquote>
<p>（2）glove模型的使用<br />
使用python的gensim工具包。首先需要将这个训练好的模型转换成gensim方便加载的格式(gensim支持word2vec格式的预训练模型格式）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> gensim.scripts.glove2word2vec <span class="hljs-keyword">import</span> glove2word2vec<br>glove_input_file = <span class="hljs-string">&#x27;data/glove.6B.300d.txt&#x27;</span><br>word2vec_output_file = <span class="hljs-string">&#x27;data/glove.6B.300d.word2vec.txt&#x27;</span><br>glove2word2vec(self.glove_input_file, self.word2vec_output_file)<br></code></pre></td></tr></table></figure>
<p>转换过模型格式后，就可以使用里面的词向量了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> gensim.models <span class="hljs-keyword">import</span> KeyedVectors<br><br><br><span class="hljs-comment"># 加载模型</span><br>glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 获得单词cat的词向量</span><br>cat_vec = glove_model[<span class="hljs-string">&#x27;cat&#x27;</span>]<br>print(cat_vec)<br></code></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200723164311128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<ul>
<li><strong>5.具体实现</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env Python</span><br><span class="hljs-comment"># coding=utf-8</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> gensim.scripts.glove2word2vec <span class="hljs-keyword">import</span> glove2word2vec<br><span class="hljs-keyword">from</span> gensim.models <span class="hljs-keyword">import</span> KeyedVectors<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> util<br><br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EncoderRNN</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_size, hidden_size,batch_size</span>):</span><br>        <span class="hljs-built_in">super</span>(EncoderRNN, self).__init__()<br>        self.hidden_size = hidden_size <span class="hljs-comment">#</span><br>        self.input_size = input_size <span class="hljs-comment">#每个单词向量的长度</span><br>        self.batch_size = batch_size <span class="hljs-comment">#句子的个数</span><br>        <span class="hljs-comment"># requires_grad指定是否在训练过程中对词向量的权重进行微调</span><br>        <span class="hljs-comment"># self.embedding.weight.requires_grad = True</span><br>        self.lstm = nn.LSTM(self.input_size, hidden_size)<br>        <span class="hljs-comment"># self.gru = nn.GRU(hidden_size, hidden_size)</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>, hidden,cell,seq_len,batch_size</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">        :param input:</span><br><span class="hljs-string">        :param hidden:</span><br><span class="hljs-string">        :param cell:</span><br><span class="hljs-string">        :param seq_len:     句子的长度</span><br><span class="hljs-string">        :param batch_size:  句子的个数</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># embedded = self.embedding[input].view(1, 1, self.hidden_size)</span><br>        <span class="hljs-comment"># embedded = input.view(len(input), 1, 300)</span><br>        embedded = <span class="hljs-built_in">input</span>.view(seq_len, batch_size, <span class="hljs-number">300</span>)<br>        <span class="hljs-comment"># output, hidden = self.gru(embedded, hidden)</span><br>        output , (hidden, cell)= self.lstm(embedded.<span class="hljs-built_in">float</span>(), (hidden.<span class="hljs-built_in">float</span>(),cell.<span class="hljs-built_in">float</span>()))<br>        <span class="hljs-keyword">return</span> output,hidden,cell<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initHidden</span>(<span class="hljs-params">self</span>):</span><br><br>        <span class="hljs-comment">#各个维度的含义是 (Seguence, minibatch_size, hidden_dim)</span><br>        <span class="hljs-comment"># 1个LSTM层，batch_size=句子的个数, 隐藏层的特征维度300</span><br>        <span class="hljs-keyword">return</span> torch.zeros(<span class="hljs-number">1</span>, self.batch_size, self.hidden_size, device=device,dtype=torch.<span class="hljs-built_in">float</span>)<br><br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SentenceEncoder</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,data_set_sentences,</span></span><br><span class="hljs-function"><span class="hljs-params">                 sentence_to_index,</span></span><br><span class="hljs-function"><span class="hljs-params">                 index_to_sentence</span>)</span><br>        self.data_set_sentences = data_set_sentences  #数据集  将文档切分成了句子 存储格式为 &#123;doc_no :&#123;sen_id:句子1 ,sen_id2 :句子2 &#125;&#125;<br>        <span class="hljs-comment">#有关数据集</span><br>        self.data_set_word = &#123;&#125; <span class="hljs-comment"># 将句子切分成单词  &#123;doc_no:&#123;sen_id:[word1,word2]&#125;&#125;</span><br>        self.sentence_to_index = sentence_to_index  <span class="hljs-comment"># 训练集 将句子切分成单词  然后与下标的映射  句子→索引的字典  &#123;reviewer_id+&quot;#&quot;+sen_id : index&#125;</span><br>        self.index_to_sentence = index_to_sentence  <span class="hljs-comment"># 索引→单词的字典 &#123;index  :  reviewer_id+&quot;#&quot;+sen_id &#125;</span><br>        self.data_set_sentence_word = &#123;&#125; <span class="hljs-comment">#&#123;index : [单词1  单词2  单词3 ]&#125;</span><br>        self.data_set_sentence_count = <span class="hljs-number">0</span>  <span class="hljs-comment"># 句子的数量</span><br>        self.max_sentence_length = <span class="hljs-number">0</span><span class="hljs-comment">#最长句子的长度</span><br>        self.data_set_sentence_word_to_vec = &#123;&#125;<br>        self.data_set_word_to_index = &#123;&#125;<span class="hljs-comment">#单词→索引</span><br>        self.data_set_word_count = &#123;&#125;<span class="hljs-comment">#每个单词的计数</span><br>        self.index_to_data_set_word = &#123;&#125;<span class="hljs-comment">#索引→单词</span><br>        self.word_to_vec = &#123;&#125;  <span class="hljs-comment">#单词与向量的映射</span><br>        <span class="hljs-comment">#定义了一个unknown的词，也就是说没有出现在训练集里的词，我们都叫做unknown，词向量就定义为0</span><br>        self.number_word = <span class="hljs-number">0</span><span class="hljs-comment">#词的数量</span><br>        <span class="hljs-comment">#词向量的维度</span><br>        self.embeddings_size = <span class="hljs-number">300</span><br>        self.data_set_sentence_word_encoder = <span class="hljs-literal">None</span><br><br>        self.glove_input_file = <span class="hljs-string">&#x27;data/glove.6B.300d.txt&#x27;</span><br>        self.word2vec_output_file = <span class="hljs-string">&#x27;data/glove.6B.300d.word2vec.txt&#x27;</span><br>        <span class="hljs-comment"># 加载预训练的glove模型</span><br>        self.glove_model = KeyedVectors.load_word2vec_format(self.word2vec_output_file, binary=<span class="hljs-literal">False</span>)<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deal_data</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment">#索引标记</span><br>        <span class="hljs-keyword">for</span> index, sentence <span class="hljs-keyword">in</span> self.index_to_sentence.items():<br>            doc_no = <span class="hljs-built_in">str</span>(sentence).split(<span class="hljs-string">&quot;#&quot;</span>)[<span class="hljs-number">0</span>]<br>            sen_id = <span class="hljs-built_in">str</span>(sentence).split(<span class="hljs-string">&quot;#&quot;</span>)[<span class="hljs-number">1</span>]<br>            sentence = self.data_set_sentences[doc_no][sen_id]<br>            <span class="hljs-comment"># 句子转化为 单词列表</span><br>            word_list = self.sentence_to_word(sentence)<br>            self.data_set_sentence_word[index] = word_list<br>            self.data_set_sentence_count += <span class="hljs-number">1</span><br>    <span class="hljs-comment"># 将句子转化成词语</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sentence_to_word</span>(<span class="hljs-params">self, sentence</span>):</span><br>        sentence = util.normalize_string(sentence)<br>        word_list= []<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentence.split(<span class="hljs-string">&#x27; &#x27;</span>):<br>            word = util.normalize_string(word)<br>            <span class="hljs-keyword">if</span> word != <span class="hljs-string">&quot;&quot;</span>:<br>                self.addWord(word)<br>                word_list.append(word)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(word_list) &gt;self.max_sentence_length:<br>            self.max_sentence_length = <span class="hljs-built_in">len</span>(word_list)<br>        <span class="hljs-keyword">return</span> word_list<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">addWord</span>(<span class="hljs-params">self, word</span>):</span><br>        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.data_set_word_to_index:<br>            self.data_set_word_to_index[word] = self.number_word<br>            self.data_set_word_count[word] = <span class="hljs-number">1</span><br>            self.index_to_data_set_word[self.number_word] = word<br>            self.number_word += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            self.data_set_word_count[word] += <span class="hljs-number">1</span><br><br><br>    <span class="hljs-comment">#首先需要将这个训练好的模型转换成gensim方便加载的格式(gensim支持word2vec格式的预训练模型格式）</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">glove_to_word2vec</span>(<span class="hljs-params">self</span>):</span><br>        glove2word2vec(self.glove_input_file, self.word2vec_output_file)<br><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    获得词向量</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_word_vec</span>(<span class="hljs-params">self,word</span>):</span><br>        <span class="hljs-keyword">return</span> self.glove_model[word]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">word_to_vector</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">for</span> index , word <span class="hljs-keyword">in</span> self.index_to_data_set_word.items():<br>            <span class="hljs-keyword">try</span>:<br>                word_vec = self.get_word_vec(word)<br>            <span class="hljs-keyword">except</span> KeyError:<br>                word_vec = np.zeros(<span class="hljs-number">300</span>)<br><br>            self.word_to_vec[word] = word_vec<br><br><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    将句子使用rnn进行encoder</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sentence_encoder</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment">#遍历每句话中的每个单词</span><br>        hedden_size = <span class="hljs-number">300</span><br>        encoder = EncoderRNN(<span class="hljs-number">300</span>, hedden_size,self.data_set_sentence_count).to(device)<br>        hidden = encoder.initHidden()<br>        cell = encoder.initHidden()<br>        output = <span class="hljs-literal">None</span><br>        <span class="hljs-comment">#所有句子的 向量列表 三维的</span><br>        sentences_vector_list = []<br>        <span class="hljs-keyword">for</span> index ,words <span class="hljs-keyword">in</span> self.data_set_sentence_word.items():<br>            <span class="hljs-comment">#每句话的单词进行word embeddings之后生成的矩阵</span><br>            sentence_vector_list = []<br>            count = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>            <span class="hljs-comment">#将单词转化为词向量</span><br>                sentence_vector_list.append(self.word_to_vec[word])<br>                count +=<span class="hljs-number">1</span><br>            <span class="hljs-comment">#然后将所有的句子都结合在一起</span><br>            sentence_vector_list = np.array(sentence_vector_list)<br>            <span class="hljs-comment"># 在数组A的边缘填充constant_values指定的数值</span><br>            <span class="hljs-comment"># （3,2）表示在A的第[0]轴填充（二维数组中，0轴表示行），即在0轴前面填充3个宽度的0，比如数组A中的95,96两个元素前面各填充了3个0；在后面填充2个0，比如数组A中的97,98两个元素后面各填充了2个0stant_values表示填充值，且(b</span><br>            <span class="hljs-comment"># （2,3）表示在A的第[1]轴填充（二维数组中，1轴表示列），即在1轴前面填充2个宽度的0，后面填充3个宽度的0</span><br>            sentence_vector_list = np.pad(sentence_vector_list,((<span class="hljs-number">0</span>,self.max_sentence_length-count),(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>)),<span class="hljs-string">&#x27;constant&#x27;</span>,constant_values = (<span class="hljs-number">0</span>,<span class="hljs-number">0</span>))<br>            sentences_vector_list.append(sentence_vector_list)<br>        output,hidden, cell = encoder(self.sentence_to_tensor(sentences_vector_list), hidden, cell,self.max_sentence_length,self.data_set_sentence_count)<br>        print(hidden.shape)<br>        print(output.shape)<br>            <span class="hljs-comment"># self.train_set_sentence_word_encoder[index] = hidden[0].detach().numpy()</span><br>        self.data_set_sentence_word_encoder = output[output.shape[<span class="hljs-number">0</span>]-<span class="hljs-number">1</span>].tolist()<br>        <span class="hljs-comment"># print(self.train_set_sentence_word_encoder)</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    将句子向量转化成tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sentence_to_tensor</span>(<span class="hljs-params">self, vector_list</span>):</span><br>        vector_array = np.array(vector_list)<br>        tensor = torch.tensor(vector_array, dtype=torch.<span class="hljs-built_in">float</span>, device=device).view(self.data_set_sentence_count,self.max_sentence_length,<span class="hljs-number">300</span>)<br>        <span class="hljs-keyword">return</span> tensor<br></code></pre></td></tr></table></figure>
<p>说明：<br />
（1）读入数据<br />
index_to_sentence——表示文档句子的索引与文档句子编号的映射（可以根据index找到属于那篇文档的那个句子），存储方式为{index : doc_no#sen_id}<br />
sentence_to_index——表示文档句子编号与文档句子索引的的映射（可以根据某文档的某个句子找到他存储的index，存储方式为{doc_no#sen_id : index}<br />
data_set_sentences——表示具体文档句子id与文档内容的映射，存储方式为{doc_no#sen_id : 句子内容}</p>
<p>（2）对数据进行处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">self.train_set_word_to_index = &#123;&#125;<span class="hljs-comment">#单词→索引</span><br>self.train_set_word_count = &#123;&#125;<span class="hljs-comment">#每个单词的计数</span><br>self.index_to_train_set_word = &#123;&#125;<span class="hljs-comment">#索引→单词</span><br></code></pre></td></tr></table></figure>
<p>将训练集中所有出现过的单词进行统计，建立单个词语的索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deal_data</span>(<span class="hljs-params">self</span>):</span><br>       <span class="hljs-comment">#索引标记</span><br>       <span class="hljs-keyword">for</span> index, sentence <span class="hljs-keyword">in</span> self.index_to_sentence.items():<br>           doc_no = <span class="hljs-built_in">str</span>(sentence).split(<span class="hljs-string">&quot;#&quot;</span>)[<span class="hljs-number">0</span>]<br>           sen_id = <span class="hljs-built_in">str</span>(sentence).split(<span class="hljs-string">&quot;#&quot;</span>)[<span class="hljs-number">1</span>]<br>           sentence = self.data_set_sentences[doc_no][sen_id]<br>           <span class="hljs-comment"># 句子转化为 单词列表</span><br>           word_list = self.sentence_to_word(sentence)<br>           self.data_set_sentence_word[index] = word_list<br>           self.data_set_sentence_count += <span class="hljs-number">1</span><br>   <span class="hljs-comment"># 将句子转化成词语</span><br>   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sentence_to_word</span>(<span class="hljs-params">self, sentence</span>):</span><br>       sentence = util.normalize_string(sentence)<br>       word_list= []<br>       <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentence.split(<span class="hljs-string">&#x27; &#x27;</span>):<br>           word = util.normalize_string(word)<br>           <span class="hljs-keyword">if</span> word != <span class="hljs-string">&quot;&quot;</span>:<br>               self.addWord(word)<br>               word_list.append(word)<br>       <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(word_list) &gt;self.max_sentence_length:<br>           self.max_sentence_length = <span class="hljs-built_in">len</span>(word_list)<br>       <span class="hljs-keyword">return</span> word_list<br><br>   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">addWord</span>(<span class="hljs-params">self, word</span>):</span><br>       <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.data_set_word_to_index:<br>           self.data_set_word_to_index[word] = self.number_word<br>           self.data_set_word_count[word] = <span class="hljs-number">1</span><br>           self.index_to_data_set_word[self.number_word] = word<br>           self.number_word += <span class="hljs-number">1</span><br>       <span class="hljs-keyword">else</span>:<br>           self.data_set_word_count[word] += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure>
<p>（3）  将句子使用LSTM进行encoder</p>
<p>使用之前处理过的句子的单词列表，求出每一句话中每一个单词的word embedding （1<em>300），然后将这句话中的所有单词的word embedding ，组成一个 （句子中单词个数</em>300）维的矩阵。然后将这个矩阵的行数，补全成了最长句子的单词个数，最终形成的矩阵是 （最长句子的单词个数*300）。</p>
<p>我将所有句子都这样操作，形成了一个三维的张量 （句子个数 * 最长句子的单词个数*300）作为LSTM的输入。</p>
<p>那么在初始化LSTM时 为nn.LSTM(每个单词向量的长度——300, hidden_size)</p>
<p>然后我将hidden 和cell初始化成了 torch.zeros(1, 句子的个数, hidden_size)</p>
<ul>
<li>对于input(seq_len, batch, input_size)  的参数 ：</li>
</ul>
<p>seq_len是序列的个数，对于句子来说，应该是句子的长度，应该是每一句话中单词的个数，这个是需要固定的 ，取了最长的句子的单词数。</p>
<p>batch表示一次性喂给网络多少条句子，初始化成句子的个数</p>
<p>input_size应该是每个具体的输入是多少维的向量，这里应该是300（word embedding的维度数量）</p>
<p>最终的hidden_size =300  最终生成了（句子的个数*300）的矩阵，作为Encoder的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#首先需要将这个训练好的模型转换成gensim方便加载的格式(gensim支持word2vec格式的预训练模型格式）</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">glove_to_word2vec</span>(<span class="hljs-params">self</span>):</span><br>      glove2word2vec(self.glove_input_file, self.word2vec_output_file)<br><br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  获得词向量</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_word_vec</span>(<span class="hljs-params">self,word</span>):</span><br>      <span class="hljs-keyword">return</span> self.glove_model[word]<br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">word_to_vector</span>(<span class="hljs-params">self</span>):</span><br>      <span class="hljs-keyword">for</span> index , word <span class="hljs-keyword">in</span> self.index_to_data_set_word.items():<br>          <span class="hljs-keyword">try</span>:<br>              word_vec = self.get_word_vec(word)<br>          <span class="hljs-keyword">except</span> KeyError:<br>              word_vec = np.zeros(<span class="hljs-number">300</span>)<br><br>          self.word_to_vec[word] = word_vec<br><br><br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  将句子使用rnn进行encoder</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sentence_encoder</span>(<span class="hljs-params">self</span>):</span><br>      <span class="hljs-comment">#遍历每句话中的每个单词</span><br>      hedden_size = <span class="hljs-number">300</span><br>      encoder = EncoderRNN(<span class="hljs-number">300</span>, hedden_size,self.data_set_sentence_count).to(device)<br>      hidden = encoder.initHidden()<br>      cell = encoder.initHidden()<br>      output = <span class="hljs-literal">None</span><br>      <span class="hljs-comment">#所有句子的 向量列表 三维的</span><br>      sentences_vector_list = []<br>      <span class="hljs-keyword">for</span> index ,words <span class="hljs-keyword">in</span> self.data_set_sentence_word.items():<br>          <span class="hljs-comment">#每句话的单词进行word embeddings之后生成的矩阵</span><br>          sentence_vector_list = []<br>          count = <span class="hljs-number">0</span><br>          <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>          <span class="hljs-comment">#将单词转化为词向量</span><br>              sentence_vector_list.append(self.word_to_vec[word])<br>              count +=<span class="hljs-number">1</span><br>          <span class="hljs-comment">#然后将所有的句子都结合在一起</span><br>          sentence_vector_list = np.array(sentence_vector_list)<br>          <span class="hljs-comment"># 在数组A的边缘填充constant_values指定的数值</span><br>          <span class="hljs-comment"># （3,2）表示在A的第[0]轴填充（二维数组中，0轴表示行），即在0轴前面填充3个宽度的0，比如数组A中的95,96两个元素前面各填充了3个0；在后面填充2个0，比如数组A中的97,98两个元素后面各填充了2个0stant_values表示填充值，且(b</span><br>          <span class="hljs-comment"># （2,3）表示在A的第[1]轴填充（二维数组中，1轴表示列），即在1轴前面填充2个宽度的0，后面填充3个宽度的0</span><br>          sentence_vector_list = np.pad(sentence_vector_list,((<span class="hljs-number">0</span>,self.max_sentence_length-count),(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>)),<span class="hljs-string">&#x27;constant&#x27;</span>,constant_values = (<span class="hljs-number">0</span>,<span class="hljs-number">0</span>))<br>          sentences_vector_list.append(sentence_vector_list)<br>      output,hidden, cell = encoder(self.sentence_to_tensor(sentences_vector_list), hidden, cell,self.max_sentence_length,self.data_set_sentence_count)<br>      print(hidden.shape)<br>      print(output.shape)<br>          <span class="hljs-comment"># self.train_set_sentence_word_encoder[index] = hidden[0].detach().numpy()</span><br>      self.data_set_sentence_word_encoder = output[output.shape[<span class="hljs-number">0</span>]-<span class="hljs-number">1</span>].tolist()<br>      <span class="hljs-comment"># print(self.train_set_sentence_word_encoder)</span><br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  将句子向量转化成tensor</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sentence_to_tensor</span>(<span class="hljs-params">self, vector_list</span>):</span><br>      vector_array = np.array(vector_list)<br>      tensor = torch.tensor(vector_array, dtype=torch.<span class="hljs-built_in">float</span>, device=device).view(self.data_set_sentence_count,self.max_sentence_length,<span class="hljs-number">300</span>)<br>      <span class="hljs-keyword">return</span> tensor<br></code></pre></td></tr></table></figure>
<h2 id="四图卷积网络graph-convolutional-network及显著性估计saliency-estimation"><a class="markdownIt-Anchor" href="#四图卷积网络graph-convolutional-network及显著性估计saliency-estimation"></a> 四.图卷积网络（Graph Convolutional Network）及显著性估计（Saliency Estimation）</h2>
<p><strong>- 1.前言</strong><br />
（1）在计算完句子嵌入和句子语义关系图后，使用单层图卷积网络（GCN），以便捕获每个句子的高级隐藏特征，封装句子信息以及图结构。<br />
图卷积网络的邻接矩阵为句子语义关系图加上单位矩阵的结果（A = A + I），特征矩阵为使用LSTM进行句子编码后的结果。<br />
（2）然后使用以下等式，获取句子的隐藏特征<br />
<img src="https://img-blog.csdnimg.cn/20200729202514147.png" alt="在这里插入图片描述" /><br />
Wi是第i个图卷积层的权重矩阵，bi是偏差矢量。使用ELU作为激活函数。<br />
（3）之后使用一个线性层来估计每个句子的显着性分数，然后通过softmax将分数归一化。</p>
<p><strong>- 2.图卷积网络的学习与理解</strong></p>
<blockquote>
<p>参考文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54505069">https://zhuanlan.zhihu.com/p/54505069</a><br />
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89503068">https://zhuanlan.zhihu.com/p/89503068</a><br />
<a target="_blank" rel="noopener" href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html">https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html</a></p>
</blockquote>
<p>（1）图神经网络GNN<br />
<img src="https://img-blog.csdnimg.cn/20200729203547914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
<img src="https://img-blog.csdnimg.cn/20200729203558981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
<img src="https://img-blog.csdnimg.cn/20200729203628762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
（2）图卷积网络</p>
<p><img src="https://img-blog.csdnimg.cn/20200729203718684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
<img src="https://img-blog.csdnimg.cn/20200729203759813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><br />
这个图也正好应证了论文中的公式<br />
<img src="https://img-blog.csdnimg.cn/20200729202514147.png" alt="在这里插入图片描述" /><br />
两层卷积层，每一层之后都有一个激活函数。</p>
<p><strong>- 3.图卷积网络GCN的实现</strong><br />
<strong>（1）卷积层的定义</strong><br />
卷积层输入维度为节点输入特征的维度，还可以设定偏差矢量。<br />
向前传播时，需要将节点的邻接矩阵和特征矩阵进行运算，然后进行相应的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GraphConvolution</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_dim, output_dim, use_bias=<span class="hljs-literal">True</span></span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;图卷积：L*X*\theta</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">        ----------</span><br><span class="hljs-string">            input_dim: int</span><br><span class="hljs-string">                节点输入特征的维度 D</span><br><span class="hljs-string">            output_dim: int</span><br><span class="hljs-string">                输出特征维度 D‘</span><br><span class="hljs-string">            use_bias : bool, optional</span><br><span class="hljs-string">                是否使用偏置</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(GraphConvolution, self).__init__()<br>        self.input_dim = input_dim<br>        self.output_dim = output_dim<br>        self.use_bias = use_bias<br>        <span class="hljs-comment"># 定义GCN层的权重矩阵    input_dim=300  output_dim=300</span><br>        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))<br>        <span class="hljs-keyword">if</span> self.use_bias:<br>            self.bias = nn.Parameter(torch.Tensor(output_dim))<br>        <span class="hljs-keyword">else</span>:<br>            self.register_parameter(<span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-literal">None</span>)<br>        self.reset_parameters()  <span class="hljs-comment"># 使用自定义的参数初始化方式</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset_parameters</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 自定义参数初始化方式</span><br>        <span class="hljs-comment"># 权重参数初始化方式</span><br>        init.kaiming_uniform_(self.weight)<br>        <span class="hljs-keyword">if</span> self.use_bias:  <span class="hljs-comment"># 偏置参数初始化为0</span><br>            init.zeros_(self.bias)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, adjacency, input_feature</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;邻接矩阵是稀疏矩阵，因此在计算时使用稀疏矩阵乘法</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">        -------</span><br><span class="hljs-string">            adjacency:</span><br><span class="hljs-string">                邻接矩阵</span><br><span class="hljs-string">            input_feature: torch.Tensor</span><br><span class="hljs-string">                输入特征</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment">#矩阵相乘</span><br>        <span class="hljs-comment"># h =  ̃ELU(A X W0 +b0 )   A为邻接矩阵  X为特征矩阵  W0为权重矩阵  B0为偏差矢量</span><br>        <span class="hljs-comment"># S = ̃ELU(A h W1 +b1 )</span><br>        support = torch.mm(input_feature, self.weight)  <span class="hljs-comment"># X W (N,D&#x27;);   X (N,D);W (D,D&#x27;)   input_feature 维度为 句子个数*300  weight的维度为 300 * 300  输出为  句子个数 *300</span><br>        output = torch.mm(adjacency, support)  <span class="hljs-comment"># (N,D&#x27;)  #adjacency 为句子个数*句子个数     support 为 句子个数 *300   output为句子个数 *300</span><br>        <span class="hljs-comment">#也可以使用稀疏矩阵的乘法</span><br>        <span class="hljs-comment"># support = torch.mm(input_feature, self.weight) #XW (N,D&#x27;);X (N,D);W (D,D&#x27;)  </span><br>        <span class="hljs-comment">#output = torch.sparse.mm(adjacency, support) #(N,D&#x27;)</span><br><br>        <span class="hljs-keyword">if</span> self.use_bias:<br>            output += self.bias<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>
<p>（2）GCN模型的定义<br />
这个模型包含两个卷积层，并且在最终的输出前还使用了线性层以及softmax计算每一个句子的显著性分数。并且模型还使用了0.2的丢弃率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GcnNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    定义一个包含两层GraphConvolution的模型</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_dim = <span class="hljs-number">300</span> ,output_dim =<span class="hljs-number">300</span>,dropout = <span class="hljs-number">0.2</span></span>):</span><br>        <span class="hljs-built_in">super</span>(GcnNet, self).__init__()<br>        self.dropout = dropout<br>        self.gcn1 = GraphConvolution(input_dim, output_dim)<br>        self.gcn2 = GraphConvolution(input_dim, output_dim)<br>        <span class="hljs-comment">#使用一个简单的线性层来估计每个句子的显着性分数,通过softmax将分数归一化并获得我们的显着性分数</span><br>        self.linear = nn.Linear(input_dim,output_dim)<br><br><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    S =ELU(A ̃ELU(A ̃XW +b )W +b )</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, adjacency, feature</span>):</span><br>        <span class="hljs-comment">#采用elu作为激活函数</span><br>        <span class="hljs-comment"># h =  ̃ELU(A X W0 +b0 )   A为邻接矩阵  X为特征矩阵  W0为权重矩阵  B0为偏差矢量</span><br>        h = F.elu(self.gcn1(adjacency, feature))<br><br>        s = F.dropout(h,p = self.dropout)<br>        <span class="hljs-comment">#S = ̃ELU(A h W1 +b1 )</span><br>        s = self.gcn2(adjacency, h)<br><br>        <span class="hljs-comment">#使用一个简单的线性层来估计每个句子的显着性分数,通过softmax将分数归一化并获得我们的显着性分数</span><br>        output = self.linear(s)<br>        <span class="hljs-comment">#输出为每个维度的得分 0-1之间</span><br>        output = F.softmax(output)<br>        <span class="hljs-keyword">return</span> output<br><br></code></pre></td></tr></table></figure>
<h2 id="五模型的训练及结果的评估"><a class="markdownIt-Anchor" href="#五模型的训练及结果的评估"></a> 五.模型的训练及结果的评估</h2>
<p><strong>- 1.前言</strong><br />
模型SemSentSum以端到端的方式训练(end-to-end端到端指的是输入是原始数据，输出是最后的结果)，并且使每个句子的显着性得分预测和ROUGE-1 F1得分之间的等式2的交叉熵损失最小。<br />
<img src="https://img-blog.csdnimg.cn/20200729210107803.png" alt="在这里插入图片描述" /></p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/">多文档摘要</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/09/27/%20AndrewNg-deeplearning-ai/AndrewNg-deepinglearning-ai-note-catalog/"><img class="prev-cover" src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">吴恩达老师深度学习公开课学习笔记目录</div></div></a></div><div class="next-post pull-right"><a href="/2020/05/28/Undergraduate-Curriculum-Design/CurriculumDesign-jdsnCompare/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">京东苏宁爬虫，商品价格比较</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ccclll777</div><div class="author-info__description">胸怀猛虎 细嗅蔷薇</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ccclll777"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ccclll777" target="_blank" title="fab fa-github"><i class="GitHub"></i></a><a class="social-icon" href="mailto:sdu945860882@gmail.com" target="_blank" title="fa fa-envelope"><i class="E-Mail"></i></a><a class="social-icon" href="https://www.weibo.com/6732062654" target="_blank" title="fab fa-weibo"><i class="Weibo"></i></a><a class="social-icon" href="https://blog.csdn.net/baidu_41871794" target="_blank" title="gratipay"><i class="CSDN"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.</span> <span class="toc-text"> 一.数据加载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E6%9E%84%E5%BB%BA%E5%8F%A5%E5%AD%90%E7%9A%84%E8%AF%AD%E4%B9%89%E5%85%B3%E7%B3%BB%E5%9B%BEsentence-semantic-relation-graph"><span class="toc-number">2.</span> <span class="toc-text"> 二.构建句子的语义关系图（Sentence Semantic Relation Graph）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%8F%A5%E5%AD%90%E7%BC%96%E7%A0%81sentence-encoder"><span class="toc-number">3.</span> <span class="toc-text"> 三.句子编码（Sentence Encoder）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9Cgraph-convolutional-network%E5%8F%8A%E6%98%BE%E8%91%97%E6%80%A7%E4%BC%B0%E8%AE%A1saliency-estimation"><span class="toc-number">4.</span> <span class="toc-text"> 四.图卷积网络（Graph Convolutional Network）及显著性估计（Saliency Estimation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E5%8F%8A%E7%BB%93%E6%9E%9C%E7%9A%84%E8%AF%84%E4%BC%B0"><span class="toc-number">5.</span> <span class="toc-text"> 五.模型的训练及结果的评估</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符"><img src="/2021/10/16/Arithmetic-LeetCode/282/show.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Leetcode 282. 给表达式添加运算符"/></a><div class="content"><a class="title" href="/2021/10/16/Arithmetic-LeetCode/282/" title="Leetcode 282. 给表达式添加运算符">Leetcode 282. 给表达式添加运算符</a><time datetime="2021-10-16T15:35:16.000Z" title="发表于 2021-10-16 23:35:16">2021-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch强化学习算法实现"/></a><div class="content"><a class="title" href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/" title="Pytorch强化学习算法实现">Pytorch强化学习算法实现</a><time datetime="2020-12-12T02:54:37.000Z" title="发表于 2020-12-12 10:54:37">2020-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块"><img src="https://tva1.sinaimg.cn/large/832afe33ly1gbhxplql40j22801e0q3c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyTorch常用工具模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/" title="PyTorch常用工具模块">PyTorch常用工具模块</a><time datetime="2020-12-09T13:32:23.000Z" title="发表于 2020-12-09 21:32:23">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块"><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中神经网络工具箱nn模块"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/" title="Pytorch中神经网络工具箱nn模块">Pytorch中神经网络工具箱nn模块</a><time datetime="2020-12-09T13:25:38.000Z" title="发表于 2020-12-09 21:25:38">2020-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd"><img src="/2020/12/09/Pytorch/Pytorch-and-Autograd/torch.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch中的Autograd"/></a><div class="content"><a class="title" href="/2020/12/09/Pytorch/Pytorch-and-Autograd/" title="Pytorch中的Autograd">Pytorch中的Autograd</a><time datetime="2020-12-09T13:25:19.000Z" title="发表于 2020-12-09 21:25:19">2020-12-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg2.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By ccclll777</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>