<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>多文档摘要 | ccclll777&#39;s blogs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="多文档摘要论文复现">
<meta property="og:type" content="article">
<meta property="og:title" content="多文档摘要">
<meta property="og:url" content="http://yoursite.com/2020/07/23/Paper-SemSentSum/index.html">
<meta property="og:site_name" content="ccclll777&#39;s blogs">
<meta property="og:description" content="多文档摘要论文复现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/202007231606529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020072316103727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200723161135460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200723162557845.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200723163050742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729175411378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200723163451721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729175639695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200723164311128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729202514147.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729203547914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729203558981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729203628762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729203718684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729203759813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729202514147.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200729210107803.png">
<meta property="article:published_time" content="2020-07-23T09:20:08.000Z">
<meta property="article:modified_time" content="2020-09-27T07:47:18.000Z">
<meta property="article:author" content="ccclll777">
<meta property="article:tag" content="多文档摘要">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/202007231606529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70">
  
    <link rel="alternate" href="/atom.xml" title="ccclll777&#39;s blogs" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">ccclll777&#39;s blogs</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Paper-SemSentSum" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/23/Paper-SemSentSum/" class="article-date">
  <time datetime="2020-07-23T09:20:08.000Z" itemprop="datePublished">2020-07-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/">论文复现</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      多文档摘要
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>多文档摘要论文复现<br> <span id="more"></span></p>
<h2 id="一-数据加载"><a href="#一-数据加载" class="headerlink" title="一.数据加载"></a>一.数据加载</h2><p> <strong>- 1.数据集说明</strong><br>  文章采用了Document Understanding Conferences（DUC）上针对最常用的多文档摘要数据集，其中DUC 2001/2002用于训练，DUC 2003用于验证，最后DUC 2004用于测试。</p>
<blockquote>
<p>数据集需要在<a target="_blank" rel="noopener" href="https://www-nlpir.nist.gov/projects/duc/guidelines.html进行申请">https://www-nlpir.nist.gov/projects/duc/guidelines.html进行申请</a></p>
</blockquote>
<p> <strong>- 2.读取数据</strong><br> 首先，从文档中读取数据，然后切分数据集，由于机器的性能，我没有读取全部的数据集，只读取了部分的数据集进行实验。我是用正则表达式，从对应的文档中读取了文档的正文以及文档对应的参考摘要。<br> 格式为{doc_no : 文档内容}和{doc_no : 参考摘要}</p>
<p> <strong>- 3.切分句子</strong><br> 读取完数据之后，需要将文档切分成句子，存储格式为{doc_no :{sen_id:句子1 ，sen_id2 :句子2 }}</p>
<p> <strong>- 4.将切分完的句子建立索引</strong><br> 建立（index-&gt;句子) 的字典，格式为{index1 : doc_no#sen_Id1，index2 : doc_no#sen_Id2}<br>  建立（句子-&gt;index) 的字典，格式为{doc_no#sen_Id1 :index1，doc_no#sen_Id2 : index2}<br>  建立 （文档-&gt;index）的字典，格式为 {doc_no :  [index1,index2…] }<br>  将训练集，验证集，测试集对应的index存储到对应的列表中。</p>
<p> <strong>- 5.代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding = utf-8</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> util</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoadData</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.data_set = &#123;&#125;</span><br><span class="line">        self.data_set_summary = &#123;&#125;</span><br><span class="line">        self.current_path = os.path.abspath(os.path.dirname(__file__))  <span class="comment"># 当前文件路径</span></span><br><span class="line">        self.train_set = &#123;&#125;  <span class="comment"># 训练集</span></span><br><span class="line">        self.validation_set = &#123;&#125;  <span class="comment"># 验证集</span></span><br><span class="line">        self.test_set = &#123;&#125;  <span class="comment"># 测试集</span></span><br><span class="line"></span><br><span class="line">        self.train_set_len = <span class="number">0</span></span><br><span class="line">        self.validation_set_len = <span class="number">0</span></span><br><span class="line">        self.test_set_len = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.train_set_sentences = &#123;&#125;  <span class="comment"># 训练集  将文档切分成了句子 存储格式为 &#123;doc_no :&#123;sen_id:句子1 ,sen_id2 :句子2 &#125;&#125;</span></span><br><span class="line">        self.validation_set_sentences = &#123;&#125;  <span class="comment"># 验证集</span></span><br><span class="line">        self.test_set_sentences = &#123;&#125;  <span class="comment"># 测试集</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#数据集</span></span><br><span class="line">        self.index_to_sentence = &#123;&#125;  <span class="comment"># 下标与 文档句子的映射</span></span><br><span class="line"></span><br><span class="line">        self.sentence_to_index = &#123;&#125;  <span class="comment"># 文档中的句子与下标的映射</span></span><br><span class="line">        <span class="comment"># 训练集</span></span><br><span class="line">        self.train_set_sencence_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.train_set_sentences_list = &#123;&#125;  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line"></span><br><span class="line">        self.train_index = []  <span class="comment"># 训练集的所有下标</span></span><br><span class="line">        <span class="comment"># 验证集</span></span><br><span class="line">        self.validation_set_sencence_count = <span class="number">0</span></span><br><span class="line">        self.validation_set_sentences_list = &#123;&#125;  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line"></span><br><span class="line">        self.validation_index = []  <span class="comment"># 训练集的所有下标</span></span><br><span class="line">        <span class="comment"># 测试集</span></span><br><span class="line">        self.test_set_sencence_count = <span class="number">0</span></span><br><span class="line">        self.test_set_sentences_list = &#123;&#125;  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line"></span><br><span class="line">        self.test_index = []  <span class="comment"># 训练集的所有下标</span></span><br><span class="line"></span><br><span class="line">        self.train_set_summary = &#123;&#125;  <span class="comment"># 训练集对应的 参考摘要</span></span><br><span class="line">        self.validation_set_summary = &#123;&#125;  <span class="comment"># 验证集对应的 参考摘要</span></span><br><span class="line">        self.test_set_summary = &#123;&#125;  <span class="comment"># 测试集对应的 参考摘要</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_data</span>(<span class="params">self</span>):</span></span><br><span class="line">        DUC2001_path = self.current_path + <span class="string">&#x27;/data/DUC/DUC2001_Summarization_Documents/data/training&#x27;</span>  <span class="comment"># 待读取文件的文件夹地址</span></span><br><span class="line">        DUC2001_files = os.listdir(DUC2001_path)  <span class="comment"># 获得文件夹中所有文件的名称列表</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> DUC2001_files:</span><br><span class="line">            <span class="keyword">if</span> os.path.isdir(DUC2001_path + <span class="string">&quot;/&quot;</span> + file):  <span class="comment"># 判断是否是文件夹</span></span><br><span class="line">                doc_path_list = DUC2001_path + <span class="string">&quot;/&quot;</span> + file + <span class="string">&quot;/docs&quot;</span></span><br><span class="line">                summary_path = DUC2001_path + <span class="string">&quot;/&quot;</span> + file + <span class="string">&quot;/&quot;</span> + file + <span class="built_in">str</span>(file)[-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">for</span> doc_path <span class="keyword">in</span> os.listdir(doc_path_list):</span><br><span class="line">                    <span class="keyword">if</span> os.path.isfile(doc_path_list + <span class="string">&quot;/&quot;</span> + doc_path):</span><br><span class="line">                        self.get_data(doc_path_list + <span class="string">&quot;/&quot;</span> + doc_path, summary_path + <span class="string">&quot;/perdocs&quot;</span>)</span><br><span class="line">        DUC2001_path_test = self.current_path + <span class="string">&#x27;/data/DUC/DUC2001_Summarization_Documents/data/test&#x27;</span>  <span class="comment"># 待读取文件的文件夹地址</span></span><br><span class="line">        DUC2001_files_test = os.listdir(DUC2001_path_test + <span class="string">&quot;/docs&quot;</span>)  <span class="comment"># 获得文件夹中所有文件的名称列表</span></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> DUC2001_files_test:</span><br><span class="line">            <span class="keyword">if</span> os.path.isdir(DUC2001_path_test + <span class="string">&quot;/docs/&quot;</span> + file):  <span class="comment"># 判断是否是文件夹</span></span><br><span class="line">                doc_path_list = DUC2001_path_test + <span class="string">&quot;/docs/&quot;</span> + file</span><br><span class="line">                summary_path = DUC2001_path_test + <span class="string">&quot;/original.summaries/&quot;</span> + file + <span class="built_in">str</span>(file)[-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">for</span> doc_path <span class="keyword">in</span> os.listdir(doc_path_list):</span><br><span class="line">                    <span class="keyword">if</span> os.path.isfile(doc_path_list + <span class="string">&quot;/&quot;</span> + doc_path):</span><br><span class="line">                        self.get_data(doc_path_list + <span class="string">&quot;/&quot;</span> + doc_path, summary_path + <span class="string">&quot;/perdocs&quot;</span>)</span><br><span class="line"></span><br><span class="line">        DUC2001_path_testtraining = self.current_path + <span class="string">&#x27;/data/DUC/DUC2001_Summarization_Documents/data/testtraining/duc2002testtraining&#x27;</span>  <span class="comment"># 待读取文件的文件夹地址</span></span><br><span class="line">        DUC2001_files_testtraining = os.listdir(DUC2001_path_testtraining)  <span class="comment"># 获得文件夹中所有文件的名称列表</span></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> DUC2001_files_testtraining:</span><br><span class="line">            <span class="keyword">if</span> os.path.isdir(DUC2001_path_testtraining + <span class="string">&quot;/&quot;</span> + file):  <span class="comment"># 判断是否是文件夹</span></span><br><span class="line">                path_list = os.listdir(DUC2001_path_testtraining + <span class="string">&quot;/&quot;</span> + file)</span><br><span class="line">                <span class="keyword">for</span> path <span class="keyword">in</span> path_list:</span><br><span class="line">                    doc_no = path</span><br><span class="line">                    fr_doc = <span class="built_in">open</span>(DUC2001_path_testtraining + <span class="string">&quot;/&quot;</span> + file + <span class="string">&quot;/&quot;</span> + path + <span class="string">&quot;/&quot;</span> + path + <span class="string">&quot;.body&quot;</span>,</span><br><span class="line">                                  encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">                    content = fr_doc.read()</span><br><span class="line">                    content = content.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">                    self.data_set[doc_no] = content</span><br><span class="line">                    fr_summary = <span class="built_in">open</span>(DUC2001_path_testtraining + <span class="string">&quot;/&quot;</span> + file + <span class="string">&quot;/&quot;</span> + path + <span class="string">&quot;/&quot;</span> + path + <span class="string">&quot;.abs&quot;</span>,</span><br><span class="line">                                      encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">                    summary = fr_summary.read()</span><br><span class="line">                    summary = summary.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">                    self.data_set_summary[doc_no] = summary</span><br><span class="line">        length = <span class="built_in">len</span>(self.data_set)</span><br><span class="line">        self.train_set_len = <span class="built_in">int</span>(length * <span class="number">0.8</span>)</span><br><span class="line">        self.validation_set_len = <span class="built_in">int</span>(length * <span class="number">0.1</span>)</span><br><span class="line">        self.test_set_len = <span class="built_in">int</span>(length * <span class="number">0.1</span>)</span><br><span class="line">        <span class="comment"># 切分读取的数据为训练集 验证集 测试集</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> doc_no,content <span class="keyword">in</span> self.data_set.items():</span><br><span class="line">            <span class="keyword">if</span> index &lt; self.train_set_len:</span><br><span class="line">                self.train_set[doc_no] = content</span><br><span class="line">                self.train_set_summary[doc_no] = self.data_set_summary[doc_no]</span><br><span class="line">            <span class="keyword">elif</span> index &gt; self.train_set_len <span class="keyword">and</span> index&lt;self.train_set_len+self.validation_set_len:</span><br><span class="line">                self.validation_set[doc_no] = content</span><br><span class="line">                self.validation_set_summary[doc_no] = self.data_set_summary[doc_no]</span><br><span class="line">            <span class="keyword">elif</span> index &gt; self.train_set_len+self.validation_set_len <span class="keyword">and</span> index &lt; length:</span><br><span class="line">                self.test_set[doc_no] = content</span><br><span class="line">                self.test_set_summary[doc_no] = self.data_set_summary[doc_no]</span><br><span class="line">            index +=<span class="number">1</span></span><br><span class="line">      <span class="string">&quot;&quot;&quot;利用正则表达式，提取文档内容和文档的摘要&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_data</span>(<span class="params">self, doc_path, summary_path</span>):</span></span><br><span class="line">        fr = <span class="built_in">open</span>(doc_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        content = fr.read()</span><br><span class="line">        content = content.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        doc = re.findall(<span class="string">&quot;&lt;TEXT&gt;(.*?)&lt;/TEXT&gt;&quot;</span>, content)[<span class="number">0</span>]</span><br><span class="line">        doc_no = re.findall(<span class="string">&quot;&lt;DOCNO&gt;(.*?)&lt;/DOCNO&gt;&quot;</span>, content)[<span class="number">0</span>].replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        self.data_set[doc_no] = doc.replace(<span class="string">&quot;&lt;p&gt;&quot;</span>, <span class="string">&quot;&quot;</span>).replace(<span class="string">&quot;&lt;/p&gt;&quot;</span>, <span class="string">&quot;&quot;</span>).replace(<span class="string">&quot;&lt;P&gt;&quot;</span>, <span class="string">&quot;&quot;</span>).replace(</span><br><span class="line">            <span class="string">&quot;&lt;/P&gt;&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        fr = <span class="built_in">open</span>(summary_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        summary_list = fr.read()</span><br><span class="line">        summary_list = summary_list.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        summary = re.findall(<span class="string">&#x27;&lt;SUM.*?DOCREF=&quot;&#123;&#125;.*?&quot;&gt;(.*?)&lt;/SUM&gt;&#x27;</span>.<span class="built_in">format</span>(doc_no), summary_list)[<span class="number">0</span>]</span><br><span class="line">        self.data_set_summary[doc_no] = summary</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文档切分成句子&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cut_doc_to_sentences</span>(<span class="params">self, <span class="built_in">set</span>=<span class="string">&quot;train_set&quot;</span></span>):</span></span><br><span class="line">        sentences_count = <span class="number">0</span>  <span class="comment"># 句子数量</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">set</span> == <span class="string">&quot;train_set&quot;</span>:</span><br><span class="line">            <span class="comment"># 读取数据</span></span><br><span class="line">            <span class="keyword">for</span> doc_no, doc_content <span class="keyword">in</span> self.train_set.items():</span><br><span class="line">                <span class="comment"># 将文档切分成句子</span></span><br><span class="line">                sentence_list = util.cut_doc_to_sentences(doc_content)</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">len</span>(sentence_list)) == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                document_dict = &#123;&#125;</span><br><span class="line">                sentences_count += <span class="built_in">len</span>(sentence_list)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentence_list)):</span><br><span class="line">                    <span class="comment"># 将句子编号 然后存入字典中</span></span><br><span class="line">                    document_dict[<span class="string">&quot;sen_id_&quot;</span> + <span class="built_in">str</span>(i)] = sentence_list[i]</span><br><span class="line">                self.train_set_sentences[doc_no] = document_dict</span><br><span class="line">            self.train_set_sencence_count = sentences_count</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">set</span> == <span class="string">&quot;validation_set&quot;</span>:</span><br><span class="line">            <span class="keyword">for</span> doc_no, doc_content <span class="keyword">in</span> self.validation_set.items():</span><br><span class="line">                sentence_list = util.cut_doc_to_sentences(doc_content)</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">len</span>(sentence_list)) == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                document_dict = &#123;&#125;</span><br><span class="line">                sentences_count += <span class="built_in">len</span>(sentence_list)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentence_list)):</span><br><span class="line">                    document_dict[<span class="string">&quot;sen_id_&quot;</span> + <span class="built_in">str</span>(i)] = sentence_list[i]</span><br><span class="line">                self.validation_set_sentences[doc_no] = document_dict</span><br><span class="line">            self.validation_set_sencence_count = sentences_count</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">set</span> == <span class="string">&quot;test_set&quot;</span>:</span><br><span class="line">            <span class="keyword">for</span> doc_no, doc_content <span class="keyword">in</span> self.test_set.items():</span><br><span class="line">                sentence_list = util.cut_doc_to_sentences(doc_content)</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">len</span>(sentence_list)) == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                document_dict = &#123;&#125;</span><br><span class="line">                sentences_count += <span class="built_in">len</span>(sentence_list)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentence_list)):</span><br><span class="line">                    document_dict[<span class="string">&quot;sen_id_&quot;</span> + <span class="built_in">str</span>(i)] = sentence_list[i]</span><br><span class="line">                self.test_set_sentences[doc_no] = document_dict</span><br><span class="line">            self.test_set_sencence_count = sentences_count</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_index</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 构造句子和矩阵下标的映射</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> doc_no, sentences <span class="keyword">in</span> self.train_set_sentences.items():</span><br><span class="line">            <span class="comment"># 遍历文档中的每个句子</span></span><br><span class="line">            sentence_index_list = []</span><br><span class="line">            <span class="keyword">for</span> sen_id, sentence <span class="keyword">in</span> sentences.items():</span><br><span class="line">                self.index_to_sentence[index] = doc_no + <span class="string">&quot;#&quot;</span> + sen_id</span><br><span class="line">                self.sentence_to_index[doc_no + <span class="string">&quot;#&quot;</span> + sen_id] = index</span><br><span class="line">                sentence_index_list.append(index)</span><br><span class="line">                self.train_index.append(index)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            self.train_set_sentences_list[doc_no] = sentence_index_list</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构造句子和矩阵下标的映射</span></span><br><span class="line">        <span class="keyword">for</span> doc_no, sentences <span class="keyword">in</span> self.validation_set_sentences.items():</span><br><span class="line">            <span class="comment"># 遍历文档中的每个句子</span></span><br><span class="line">            sentence_index_list = []</span><br><span class="line">            <span class="keyword">for</span> sen_id, sentence <span class="keyword">in</span> sentences.items():</span><br><span class="line">                self.index_to_sentence[index] = doc_no + <span class="string">&quot;#&quot;</span> + sen_id</span><br><span class="line">                self.sentence_to_index[doc_no + <span class="string">&quot;#&quot;</span> + sen_id] = index</span><br><span class="line">                sentence_index_list.append(index)</span><br><span class="line">                self.validation_index.append(index)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">            self.validation_set_sentences_list[doc_no] = sentence_index_list</span><br><span class="line">        <span class="comment"># 构造句子和矩阵下标的映射</span></span><br><span class="line">        <span class="keyword">for</span> doc_no, sentences <span class="keyword">in</span> self.test_set_sentences.items():</span><br><span class="line">            <span class="comment"># 遍历文档中的每个句子</span></span><br><span class="line">            sentence_index_list = []</span><br><span class="line">            <span class="keyword">for</span> sen_id, sentence <span class="keyword">in</span> sentences.items():</span><br><span class="line">                self.index_to_sentence[index] = doc_no + <span class="string">&quot;#&quot;</span> + sen_id</span><br><span class="line">                self.sentence_to_index[doc_no + <span class="string">&quot;#&quot;</span> + sen_id] = index</span><br><span class="line">                sentence_index_list.append(index)</span><br><span class="line">                self.test_index.append(index)</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">            self.test_set_sentences_list[doc_no] = sentence_index_list</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.read_data()</span><br><span class="line">        self.cut_doc_to_sentences(<span class="built_in">set</span>=<span class="string">&quot;train_set&quot;</span>)</span><br><span class="line">        self.cut_doc_to_sentences(<span class="built_in">set</span>=<span class="string">&quot;validation_set&quot;</span>)</span><br><span class="line">        self.cut_doc_to_sentences(<span class="built_in">set</span>=<span class="string">&quot;test_set&quot;</span>)</span><br><span class="line">        self.create_index()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="二-构建句子的语义关系图（Sentence-Semantic-Relation-Graph）"><a href="#二-构建句子的语义关系图（Sentence-Semantic-Relation-Graph）" class="headerlink" title="二.构建句子的语义关系图（Sentence Semantic Relation Graph）"></a>二.构建句子的语义关系图（Sentence Semantic Relation Graph）</h2><ul>
<li><strong>1.解释：</strong></li>
</ul>
<p>（1）用图对句子建模，图的顶点为文档i中的句子j（Si，j），边为两个句子之间的相似程度。需要使用英语Wikipedia语料库上训练的的模型进行句子嵌入（sentence embeddings），产生句子的向量，然后根据向量计算句子之间的余弦相似度，然后构建矩阵。<br>（2）引入一个阈值t，去除相似度小于阈值t的边，以强调较高的句子相似度，避免模型无法显著地利用句子之间的语义结构</p>
<ul>
<li><strong>2.sentence embeddings环境安装工作</strong></li>
</ul>
<p>我找到了（Pagliardini et al. (2018)）的论文中描述的模型的开源实现，他可以在fasttext库的基础上，进行句子嵌入。</p>
<blockquote>
<p>sent2vec模型地址：<a target="_blank" rel="noopener" href="https://github.com/epfml/sent2vec">https://github.com/epfml/sent2vec</a></p>
</blockquote>
<p>（1）由于他基于fasttext，所以先下载并编译了fasttext，具体方法他在github中写的非常清楚，在他们目录文件中写好了Makefile，直接用本地的gcc环境进行编译</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/fastText">https://github.com/facebookresearch/fastText</a></p>
</blockquote>
<p>（2）下载论文中描述的的wiki百科600维的预训练unigram模型，在fasttext中进行使用</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://drive.google.com/uc?id=0B6VhzidiLvjSa19uYWlLUEkzX3c&amp;export=download">https://drive.google.com/uc?id=0B6VhzidiLvjSa19uYWlLUEkzX3c&amp;export=download</a></p>
</blockquote>
<p>（3）下载了斯坦福解析器，配合nltk库进行Tokenizer</p>
<blockquote>
<p>解析器下载：<a target="_blank" rel="noopener" href="https://nlp.stanford.edu/software/lex-parser.shtml#Download">https://nlp.stanford.edu/software/lex-parser.shtml#Download</a><br>使用教程：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36652619/article/details/75091327">https://blog.csdn.net/qq_36652619/article/details/75091327</a></p>
</blockquote>
<ul>
<li><strong>3.使用nltk库和斯坦福解析器，进行Tokenizer，然后使用wiki百科600维的预训练unigram模型进行sentence embeddings</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> subprocess <span class="keyword">import</span> call</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize.stanford <span class="keyword">import</span> StanfordTokenizer</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentencesEmbeddings</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#fasttext执行</span></span><br><span class="line">        self.FASTTEXT_EXEC_PATH = os.path.abspath(<span class="string">&quot;./sent2vec-master/fasttext&quot;</span>)</span><br><span class="line">        <span class="comment">#斯坦福分词器的路径</span></span><br><span class="line">        self.BASE_SNLP_PATH = <span class="string">&quot;sent2vec-master/stanford-postagger-full/&quot;</span></span><br><span class="line">        self.SNLP_TAGGER_JAR = os.path.join(self.BASE_SNLP_PATH, <span class="string">&quot;stanford-postagger.jar&quot;</span>)</span><br><span class="line">        <span class="comment">#wiki百科预训练模型的路径</span></span><br><span class="line">        self.MODEL_WIKI_UNIGRAMS = os.path.abspath(<span class="string">&quot;sent2vec-master/wiki_unigrams.bin&quot;</span>)</span><br><span class="line">        self.tknzr = StanfordTokenizer(self.SNLP_TAGGER_JAR, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        print(<span class="string">&quot;SentencesEmbeddings初始化完成&quot;</span>)</span><br><span class="line">    <span class="comment">#将句子去除符号，大小写转换后，进行分词</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">self, sentence, to_lower=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Arguments:</span></span><br><span class="line"><span class="string">            - tknzr: a tokenizer implementing the NLTK tokenizer interface</span></span><br><span class="line"><span class="string">            - sentence: a string to be tokenized</span></span><br><span class="line"><span class="string">            - to_lower: lowercasing or not</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sentence = sentence.strip()</span><br><span class="line">        sentence = <span class="string">&#x27; &#x27;</span>.join([self.format_token(x) <span class="keyword">for</span> x <span class="keyword">in</span>  self.tknzr.tokenize(sentence)])</span><br><span class="line">        <span class="keyword">if</span> to_lower:</span><br><span class="line">            sentence = sentence.lower()</span><br><span class="line">        sentence = re.sub(<span class="string">&#x27;((www\.[^\s]+)|(https?://[^\s]+)|(http?://[^\s]+))&#x27;</span>,<span class="string">&#x27;&lt;url&gt;&#x27;</span>,sentence) <span class="comment">#replace urls by &lt;url&gt;</span></span><br><span class="line">        sentence = re.sub(<span class="string">&#x27;(\@[^\s]+)&#x27;</span>,<span class="string">&#x27;&lt;user&gt;&#x27;</span>,sentence) <span class="comment">#replace @user268 by &lt;user&gt;</span></span><br><span class="line">        <span class="built_in">filter</span>(<span class="keyword">lambda</span> word: <span class="string">&#x27; &#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> word, sentence)</span><br><span class="line">        <span class="keyword">return</span> sentence</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">format_token</span>(<span class="params">self,token</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> token == <span class="string">&#x27;-LRB-&#x27;</span>:</span><br><span class="line">            token = <span class="string">&#x27;(&#x27;</span></span><br><span class="line">        <span class="keyword">elif</span> token == <span class="string">&#x27;-RRB-&#x27;</span>:</span><br><span class="line">            token = <span class="string">&#x27;)&#x27;</span></span><br><span class="line">        <span class="keyword">elif</span> token == <span class="string">&#x27;-RSB-&#x27;</span>:</span><br><span class="line">            token = <span class="string">&#x27;]&#x27;</span></span><br><span class="line">        <span class="keyword">elif</span> token == <span class="string">&#x27;-LSB-&#x27;</span>:</span><br><span class="line">            token = <span class="string">&#x27;[&#x27;</span></span><br><span class="line">        <span class="keyword">elif</span> token == <span class="string">&#x27;-LCB-&#x27;</span>:</span><br><span class="line">            token = <span class="string">&#x27;&#123;&#x27;</span></span><br><span class="line">        <span class="keyword">elif</span> token == <span class="string">&#x27;-RCB-&#x27;</span>:</span><br><span class="line">            token = <span class="string">&#x27;&#125;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> token</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_sentences</span>(<span class="params">self, sentences, to_lower=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Arguments:</span></span><br><span class="line"><span class="string">            - tknzr: 斯坦福解析器</span></span><br><span class="line"><span class="string">            - sentences:句子列表</span></span><br><span class="line"><span class="string">            - to_lower: 是否转化成消协</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#返回token化的结果</span></span><br><span class="line">        <span class="keyword">return</span> [self.tokenize( s, to_lower) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_embeddings_for_preprocessed_sentences</span>(<span class="params">self,sentences, model_path, fasttext_exec_path</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Arguments:</span></span><br><span class="line"><span class="string">            - sentences:分词后的结果</span></span><br><span class="line"><span class="string">            - model_path: wiki百科模型文件的路径</span></span><br><span class="line"><span class="string">            - fasttext_exec_path: fasttext的路径</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        timestamp = <span class="built_in">str</span>(time.time())</span><br><span class="line">        test_path = os.path.abspath(<span class="string">&#x27;./&#x27;</span>+timestamp+<span class="string">&#x27;_fasttext.test.txt&#x27;</span>)</span><br><span class="line">        embeddings_path = os.path.abspath(<span class="string">&#x27;./&#x27;</span>+timestamp+<span class="string">&#x27;_fasttext.embeddings.txt&#x27;</span>)</span><br><span class="line">        self.dump_text_to_disk(test_path, sentences)</span><br><span class="line">        call(fasttext_exec_path+</span><br><span class="line">              <span class="string">&#x27; print-sentence-vectors &#x27;</span>+</span><br><span class="line">              model_path + <span class="string">&#x27; &lt; &#x27;</span>+</span><br><span class="line">              test_path + <span class="string">&#x27; &gt; &#x27;</span> +</span><br><span class="line">              embeddings_path, shell=<span class="literal">True</span>)</span><br><span class="line">        embeddings = self.read_embeddings(embeddings_path)</span><br><span class="line">        os.remove(test_path)</span><br><span class="line">        os.remove(embeddings_path)</span><br><span class="line">        <span class="keyword">assert</span>(<span class="built_in">len</span>(sentences) == <span class="built_in">len</span>(embeddings))</span><br><span class="line">        <span class="keyword">return</span> np.array(embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_embeddings</span>(<span class="params">self,embeddings_path</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Arguments:</span></span><br><span class="line"><span class="string">            - embeddings_path: path to the embeddings</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(embeddings_path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> in_stream:</span><br><span class="line">            embeddings = []</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> in_stream:</span><br><span class="line">                line = <span class="string">&#x27;[&#x27;</span>+line.replace(<span class="string">&#x27; &#x27;</span>,<span class="string">&#x27;,&#x27;</span>)+<span class="string">&#x27;]&#x27;</span></span><br><span class="line">                embeddings.append(<span class="built_in">eval</span>(line))</span><br><span class="line">            <span class="keyword">return</span> embeddings</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dump_text_to_disk</span>(<span class="params">self,file_path, X, Y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Arguments:</span></span><br><span class="line"><span class="string">            - file_path: where to dump the data</span></span><br><span class="line"><span class="string">            - X: list of sentences to dump</span></span><br><span class="line"><span class="string">            - Y: labels, if any</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> out_stream:</span><br><span class="line">            <span class="keyword">if</span> Y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y):</span><br><span class="line">                    out_stream.write(<span class="string">&#x27;__label__&#x27;</span>+<span class="built_in">str</span>(y)+<span class="string">&#x27; &#x27;</span>+x+<span class="string">&#x27; \n&#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">                    out_stream.write(x+<span class="string">&#x27; \n&#x27;</span>)</span><br><span class="line">    <span class="comment">#输入为一个句子的列表 返回embeeding之后的数据 600纬</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_sentence_embeddings</span>(<span class="params">self,sentences</span>):</span></span><br><span class="line">        wiki_embeddings = <span class="literal">None</span></span><br><span class="line">        <span class="comment">#加载斯坦福的分词器</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#进行分词操作</span></span><br><span class="line">        s = <span class="string">&#x27; &lt;delimiter&gt; &#x27;</span>.join(sentences)</span><br><span class="line">        tokenized_sentences_SNLP = self.tokenize_sentences([s])</span><br><span class="line">        tokenized_sentences_SNLP = tokenized_sentences_SNLP[<span class="number">0</span>].split(<span class="string">&#x27; &lt;delimiter&gt; &#x27;</span>)</span><br><span class="line">        <span class="keyword">assert</span>(<span class="built_in">len</span>(tokenized_sentences_SNLP) == <span class="built_in">len</span>(sentences))</span><br><span class="line">        <span class="comment">#使用wiki百科预训练的模型进行embeddings</span></span><br><span class="line">        wiki_embeddings = self.get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \</span><br><span class="line">                                         self.MODEL_WIKI_UNIGRAMS, self.FASTTEXT_EXEC_PATH)</span><br><span class="line">        <span class="keyword">return</span> wiki_embeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">embeddings</span>(<span class="params">self,sentences</span>):</span></span><br><span class="line"></span><br><span class="line">        my_embeddings = self.get_sentence_embeddings(sentences)</span><br><span class="line">        <span class="keyword">return</span> my_embeddings</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>4.句子相似度的计算</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env Python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">构造语义关系图  以及进行sentence encoder</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> SentencesEmbeddings</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> util</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentenceSemanticRelationGraph</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, train_set_sentences,</span></span></span><br><span class="line"><span class="function"><span class="params">                 validation_set_sentences,</span></span></span><br><span class="line"><span class="function"><span class="params">                 test_set_sentences,</span></span></span><br><span class="line"><span class="function"><span class="params">                 train_set_sencence_count,</span></span></span><br><span class="line"><span class="function"><span class="params">                 validation_set_sencence_count,</span></span></span><br><span class="line"><span class="function"><span class="params">                 test_set_sencence_count,</span></span></span><br><span class="line"><span class="function"><span class="params">                 sentence_to_index,</span></span></span><br><span class="line"><span class="function"><span class="params">                 index_to_sentence,</span></span></span><br><span class="line"><span class="function"><span class="params">                 train_set_sentences_list,</span></span></span><br><span class="line"><span class="function"><span class="params">                 validation_set_sentences_list,</span></span></span><br><span class="line"><span class="function"><span class="params">                 test_set_sentences_list,</span></span></span><br><span class="line"><span class="function"><span class="params">                 train_index,</span></span></span><br><span class="line"><span class="function"><span class="params">                 validation_index,</span></span></span><br><span class="line"><span class="function"><span class="params">                 test_index</span>):</span></span><br><span class="line">        self.sentences_embeddings = SentencesEmbeddings.SentencesEmbeddings()</span><br><span class="line">        self.train_set_sentences = train_set_sentences  <span class="comment"># 训练集  将文档切分成了句子 存储格式为 &#123;doc_no :&#123;sen_id:句子1 ,sen_id2 :句子2 &#125;&#125;</span></span><br><span class="line">        self.validation_set_sentences = validation_set_sentences  <span class="comment"># 验证集</span></span><br><span class="line">        self.test_set_sentences = test_set_sentences  <span class="comment"># 测试集</span></span><br><span class="line">        self.train_set_sencence_count = train_set_sencence_count <span class="comment">#训练集句子数量</span></span><br><span class="line">        self.validation_set_sencence_count = validation_set_sencence_count  <span class="comment"># 验证集句子数量</span></span><br><span class="line">        self.test_set_sencence_count = test_set_sencence_count  <span class="comment"># 测试集句子数量</span></span><br><span class="line">        self.sentence_count = train_set_sencence_count+validation_set_sencence_count+test_set_sencence_count</span><br><span class="line">        self.data_set_sentences_embeddings = &#123;&#125; <span class="comment">#存储句子嵌入的集合</span></span><br><span class="line"></span><br><span class="line">        self.data_set_sentences = &#123;&#125;</span><br><span class="line">        self.tgsim = <span class="number">0.35</span>  <span class="comment">#句子之间余弦相似度的阈值  大于阈值的边才会被保留</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#数据集相关</span></span><br><span class="line">        self.index_to_sentence = index_to_sentence  <span class="comment"># 余弦相似度矩阵的下标与 文档句子的映射</span></span><br><span class="line"></span><br><span class="line">        self.sentence_to_index = sentence_to_index  <span class="comment"># 文档中的句子与余弦相似度矩阵下标的映射</span></span><br><span class="line">        self.data_set_cosine_similarity_matrix = <span class="literal">None</span>  <span class="comment"># 存储余弦相似度的矩阵</span></span><br><span class="line">        self.data_set_sentences_list = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#训练集相关</span></span><br><span class="line">        self.train_index = train_index <span class="comment">#训练集的下标</span></span><br><span class="line">        self.train_set_sentences_list = train_set_sentences_list  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line">        <span class="comment">#验证集相关</span></span><br><span class="line">        self.validation_index = validation_index  <span class="comment"># 验证集的下标</span></span><br><span class="line"></span><br><span class="line">        self.validation_set_sentences_list = validation_set_sentences_list  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line">        <span class="comment">#测试集相关</span></span><br><span class="line">        self.test_index = test_index  <span class="comment"># 测试集的下标</span></span><br><span class="line"></span><br><span class="line">        self.test_set_sentences_list = test_set_sentences_list  <span class="comment"># 文档 与index的映射    &#123;doc_no :  [index1,index2...]&#125;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    处理已经读入的训练集 测试集  验证集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deal_data</span>(<span class="params">self</span>):</span></span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.train_set_sentences_list.items():</span><br><span class="line">           self.data_set_sentences_list[doc_no] = sentences_list</span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.validation_set_sentences_list.items():</span><br><span class="line">           self.data_set_sentences_list[doc_no] = sentences_list</span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.test_set_sentences_list.items():</span><br><span class="line">           self.data_set_sentences_list[doc_no] = sentences_list</span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.train_set_sentences.items():</span><br><span class="line">           self.data_set_sentences[doc_no] = sentences_list</span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.validation_set_sentences.items():</span><br><span class="line">           self.data_set_sentences[doc_no] = sentences_list</span><br><span class="line">       <span class="keyword">for</span> doc_no, sentences_list <span class="keyword">in</span> self.test_set_sentences.items():</span><br><span class="line">           self.data_set_sentences[doc_no] = sentences_list</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    调用方法计算句子嵌入 </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_sentences_embeddings</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 以文档为单位计算句子嵌入  遍历每个文档的每一句话的index</span></span><br><span class="line">        <span class="keyword">for</span> doc_no, sentence_index_list <span class="keyword">in</span> self.data_set_sentences_list.items():</span><br><span class="line">            index_list = []</span><br><span class="line">            sentence_list = []</span><br><span class="line">            <span class="comment"># 根据index找到对应的句子</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> sentence_index_list:</span><br><span class="line">                <span class="comment"># sen_id = str(self.train_set_index_to_sentence[index]).split(&quot;#&quot;)[1]</span></span><br><span class="line">                sen_id = <span class="built_in">str</span>(self.index_to_sentence[index]).split(<span class="string">&quot;#&quot;</span>)[<span class="number">1</span>]</span><br><span class="line">                sentence = self.data_set_sentences[doc_no][sen_id]</span><br><span class="line">                sentence_list.append(sentence)</span><br><span class="line">                index_list.append(index)</span><br><span class="line">            <span class="comment"># 将文档中所有的句子加入列表，统一计算句子嵌入</span></span><br><span class="line">            sentence_embedding_list = self.sentences_embeddings.embeddings(sentence_list)</span><br><span class="line">            <span class="comment"># 然后加入句子嵌入的列表</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(index_list)):</span><br><span class="line">                self.data_set_sentences_embeddings[index_list[i]] = sentence_embedding_list[i]</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算句子间的余弦相似度 构造语义关系图</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_cosine_similarity_matrix</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 初始化余弦相似度的矩阵</span></span><br><span class="line">        self.data_set_cosine_similarity_matrix = np.zeros(</span><br><span class="line">            (self.sentence_count, self.sentence_count))</span><br><span class="line">        <span class="comment"># 遍历映射 进行余弦相似度的计算</span></span><br><span class="line">        <span class="keyword">for</span> index, <span class="built_in">id</span> <span class="keyword">in</span> self.index_to_sentence.items():</span><br><span class="line">            <span class="keyword">for</span> index2, id2 <span class="keyword">in</span> self.index_to_sentence.items():</span><br><span class="line">                <span class="keyword">if</span> index == index2 :</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> self.data_set_cosine_similarity_matrix[index][index2] == <span class="number">0</span>:</span><br><span class="line">                    <span class="comment"># 句子1 的嵌入</span></span><br><span class="line">                    embeddings1 = self.data_set_sentences_embeddings[index]</span><br><span class="line">                    <span class="comment"># 句子2的嵌入</span></span><br><span class="line">                    embeddings2 = self.data_set_sentences_embeddings[index2]</span><br><span class="line">                    cosine_similarity = self.calculate_cosine_similarity(embeddings1, embeddings2)</span><br><span class="line">                    <span class="comment"># 如果大于阈值 在进行存储</span></span><br><span class="line">                    <span class="keyword">if</span> cosine_similarity &gt; self.tgsim:</span><br><span class="line">                        self.data_set_cosine_similarity_matrix[index][index2] = cosine_similarity</span><br><span class="line">                        self.data_set_cosine_similarity_matrix[index2][index] = cosine_similarity</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算两个numpy向量之间的余弦相似度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_cosine_similarity</span>(<span class="params">self,vector1,vector2</span>):</span></span><br><span class="line">        cosine_similarity = np.dot(vector1,vector2)/(np.linalg.norm(vector1)*(np.linalg.norm(vector2)))</span><br><span class="line">        <span class="keyword">return</span> cosine_similarity</span><br></pre></td></tr></table></figure>
<p>解释：</p>
<ol>
<li><p>获得读取到的数据，将其构成几个集合：<br>（1）data_set_sentences_list——表示文档的句子的索引,存储方式为{doc_no : [index1,index2]}，表示一篇文档句子的存储位置<br>（2）index_to_sentence——表示文档句子的索引与文档句子编号的映射（可以根据index找到属于那篇文档的那个句子），存储方式为{index : doc_no#sen_id}<br>（3）sentence_to_index——表示文档句子编号与文档句子索引的的映射（可以根据某文档的某个句子找到他存储的index，存储方式为{doc_no#sen_id : index}<br>（4）data_set_sentences——表示具体文档句子id与文档内容的映射，存储方式为{doc_no#sen_id : 句子内容}</p>
</li>
<li><p>然后调用方法计算每一篇文档的sentence embeddings，存储格式为{reviewer_id1：{sen_id1：embeddings1，sen_id2：embeddings2} ，reviewer_id2：{sen_id1：embeddings1，sen_id2：embeddings2} }。</p>
</li>
<li>最后借助sentence embeddings，来计算句子语义关系图data_set_cosine_similarity_matrix</li>
</ol>
<p>至此，第一部分的工作完成。</p>
<h2 id="三-句子编码（Sentence-Encoder）"><a href="#三-句子编码（Sentence-Encoder）" class="headerlink" title="三.句子编码（Sentence Encoder）"></a>三.句子编码（Sentence Encoder）</h2><p> <strong>- 1.解释</strong><br> 在构建完语义关系图之后，还需要对训练集中的所有单词，使用300维的预先训练的GloVe嵌入（Pen- nington et al., 2014）进行单词嵌入（word embeddings），然后将单词嵌入输入句子编码器以计算句子嵌入，句子编码器使用了单层正向循环神经网络（ a single-layer forward recurrent neural network）的变体，长短时记忆神经网络LSTM，最后从隐藏层中提取句子嵌入，然后，将所有句子嵌入连接到一个矩阵X中，该矩阵X将构成图卷积网络将使用的输入节点特征。</p>
<p> <strong>- 2.循环神经网络RNN的学习</strong></p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/50915723">https://zhuanlan.zhihu.com/p/50915723</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/30844905">https://zhuanlan.zhihu.com/p/30844905</a></p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/202007231606529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。对于文本数据，句子前后是有语义关系的，所以RNN正好可以处理句子前后关系的影响。但是RNN也有序列过长的情况下梯度消失或者梯度爆炸的问题。</p>
<p> <strong>- 3.LSTM：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/2020072316103727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200723161135460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>用pytorch实现lstm：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/master/generated/torch.nn.LSTM.html">https://pytorch.org/docs/master/generated/torch.nn.LSTM.html</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/79064602">https://zhuanlan.zhihu.com/p/79064602</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size,batch_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size <span class="comment">#</span></span><br><span class="line">        self.input_size = input_size <span class="comment">#每个单词向量的长度</span></span><br><span class="line">        self.batch_size = batch_size <span class="comment">#句子的个数</span></span><br><span class="line">        <span class="comment"># requires_grad指定是否在训练过程中对词向量的权重进行微调</span></span><br><span class="line">        <span class="comment"># self.embedding.weight.requires_grad = True</span></span><br><span class="line">        self.lstm = nn.LSTM(self.input_size, hidden_size)</span><br><span class="line">        <span class="comment"># self.gru = nn.GRU(hidden_size, hidden_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden,cell,seq_len,batch_size</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param input:</span></span><br><span class="line"><span class="string">        :param hidden:</span></span><br><span class="line"><span class="string">        :param cell:</span></span><br><span class="line"><span class="string">        :param seq_len:     句子的长度</span></span><br><span class="line"><span class="string">        :param batch_size:  句子的个数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># embedded = self.embedding[input].view(1, 1, self.hidden_size)</span></span><br><span class="line">        <span class="comment"># embedded = input.view(len(input), 1, 300)</span></span><br><span class="line">        embedded = <span class="built_in">input</span>.view(seq_len, batch_size, <span class="number">300</span>)</span><br><span class="line">        <span class="comment"># output, hidden = self.gru(embedded, hidden)</span></span><br><span class="line">        output , (hidden, cell)= self.lstm(embedded.<span class="built_in">float</span>(), (hidden.<span class="built_in">float</span>(),cell.<span class="built_in">float</span>()))</span><br><span class="line">        <span class="keyword">return</span> output,hidden,cell</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200723162557845.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200723163050742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>（1）torch.nn.LSTM(*args, kwargs)<br>参数列表：<br>– input_size ：输入特征维数，如我们输入的是300维的预先训练的GloVe嵌入产生的词向量<br>– hidden_size：隐藏层状态的维数，即隐藏层节点的个数，这里我选择输入的也是300维<br>– num_layers： LSTM 堆叠的层数，默认值是1层，我们的神经网络也是单层的<br>– bias： 隐层状态是否带bias，默认为true。bias是偏置值，或者偏移值。没有偏置值就是以0为中轴，或以0为起点。<br>– batch_first：输入输出的第一维是否为 batch_size，默认值 False。因为 Torch 中，人们习惯使用Torch中带有的dataset，dataloader向神经网络模型连续输入数据，这里面就有一个 batch_size 的参数，表示一次输入多少个数据。 在 LSTM 模型中，输入数据必须是一批数据，为了区分LSTM中的批量数据和dataloader中的批量数据是否相同意义，LSTM 模型就通过这个参数的设定来区分。 如果是相同意义的，就设置为True，如果不同意义的，设置为False。 torch.LSTM 中 batch_size 维度默认是放在第二维度，故此参数设置可以将 batch_size 放在第一维度。如：input 默认是(4,1,5)，中间的 1 是 batch_size，指定batch_first=True后就是(1,4,5)。所以，如果你的输入数据是二维数据的话，就应该将 batch_first 设置为True;</p>
<p>– dropout： 默认值0。是否在除最后一个 RNN 层外的其他 RNN 层后面加 dropout 层。输入值是 0-1 之间的小数，表示概率。0表示0概率dripout，即不dropout<br>– bidirectional： 是否是双向 RNN，默认为：false，若为 true，则：num_directions=2，否则为1。<br>（2）向前传播时的输入<br><img src="https://img-blog.csdnimg.cn/20200729175411378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20200723163451721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200729175639695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>输入的张量的每一个维度都有固定的含义，不能弄错，需要在理解之后，才能进行修改。</p>
<ul>
<li><p><strong>4.word embeddings（使用预训练的glove进行词嵌入）</strong></p>
<p>（1）下载预训练的模型</p>
</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></p>
</blockquote>
<p>（2）glove模型的使用<br>使用python的gensim工具包。首先需要将这个训练好的模型转换成gensim方便加载的格式(gensim支持word2vec格式的预训练模型格式）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.scripts.glove2word2vec <span class="keyword">import</span> glove2word2vec</span><br><span class="line">glove_input_file = <span class="string">&#x27;data/glove.6B.300d.txt&#x27;</span></span><br><span class="line">word2vec_output_file = <span class="string">&#x27;data/glove.6B.300d.word2vec.txt&#x27;</span></span><br><span class="line">glove2word2vec(self.glove_input_file, self.word2vec_output_file)</span><br></pre></td></tr></table></figure>
<p>转换过模型格式后，就可以使用里面的词向量了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 获得单词cat的词向量</span></span><br><span class="line">cat_vec = glove_model[<span class="string">&#x27;cat&#x27;</span>]</span><br><span class="line">print(cat_vec)</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20200723164311128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li><strong>5.具体实现</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env Python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> gensim.scripts.glove2word2vec <span class="keyword">import</span> glove2word2vec</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> util</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderRNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size,batch_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderRNN, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size <span class="comment">#</span></span><br><span class="line">        self.input_size = input_size <span class="comment">#每个单词向量的长度</span></span><br><span class="line">        self.batch_size = batch_size <span class="comment">#句子的个数</span></span><br><span class="line">        <span class="comment"># requires_grad指定是否在训练过程中对词向量的权重进行微调</span></span><br><span class="line">        <span class="comment"># self.embedding.weight.requires_grad = True</span></span><br><span class="line">        self.lstm = nn.LSTM(self.input_size, hidden_size)</span><br><span class="line">        <span class="comment"># self.gru = nn.GRU(hidden_size, hidden_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden,cell,seq_len,batch_size</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param input:</span></span><br><span class="line"><span class="string">        :param hidden:</span></span><br><span class="line"><span class="string">        :param cell:</span></span><br><span class="line"><span class="string">        :param seq_len:     句子的长度</span></span><br><span class="line"><span class="string">        :param batch_size:  句子的个数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># embedded = self.embedding[input].view(1, 1, self.hidden_size)</span></span><br><span class="line">        <span class="comment"># embedded = input.view(len(input), 1, 300)</span></span><br><span class="line">        embedded = <span class="built_in">input</span>.view(seq_len, batch_size, <span class="number">300</span>)</span><br><span class="line">        <span class="comment"># output, hidden = self.gru(embedded, hidden)</span></span><br><span class="line">        output , (hidden, cell)= self.lstm(embedded.<span class="built_in">float</span>(), (hidden.<span class="built_in">float</span>(),cell.<span class="built_in">float</span>()))</span><br><span class="line">        <span class="keyword">return</span> output,hidden,cell</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initHidden</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#各个维度的含义是 (Seguence, minibatch_size, hidden_dim)</span></span><br><span class="line">        <span class="comment"># 1个LSTM层，batch_size=句子的个数, 隐藏层的特征维度300</span></span><br><span class="line">        <span class="keyword">return</span> torch.zeros(<span class="number">1</span>, self.batch_size, self.hidden_size, device=device,dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentenceEncoder</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,data_set_sentences,</span></span></span><br><span class="line"><span class="function"><span class="params">                 sentence_to_index,</span></span></span><br><span class="line"><span class="function"><span class="params">                 index_to_sentence</span>)</span></span><br><span class="line">        self.data_set_sentences = data_set_sentences  #数据集  将文档切分成了句子 存储格式为 &#123;doc_no :&#123;sen_id:句子1 ,sen_id2 :句子2 &#125;&#125;</span><br><span class="line">        <span class="comment">#有关数据集</span></span><br><span class="line">        self.data_set_word = &#123;&#125; <span class="comment"># 将句子切分成单词  &#123;doc_no:&#123;sen_id:[word1,word2]&#125;&#125;</span></span><br><span class="line">        self.sentence_to_index = sentence_to_index  <span class="comment"># 训练集 将句子切分成单词  然后与下标的映射  句子→索引的字典  &#123;reviewer_id+&quot;#&quot;+sen_id : index&#125;</span></span><br><span class="line">        self.index_to_sentence = index_to_sentence  <span class="comment"># 索引→单词的字典 &#123;index  :  reviewer_id+&quot;#&quot;+sen_id &#125;</span></span><br><span class="line">        self.data_set_sentence_word = &#123;&#125; <span class="comment">#&#123;index : [单词1  单词2  单词3 ]&#125;</span></span><br><span class="line">        self.data_set_sentence_count = <span class="number">0</span>  <span class="comment"># 句子的数量</span></span><br><span class="line">        self.max_sentence_length = <span class="number">0</span><span class="comment">#最长句子的长度</span></span><br><span class="line">        self.data_set_sentence_word_to_vec = &#123;&#125;</span><br><span class="line">        self.data_set_word_to_index = &#123;&#125;<span class="comment">#单词→索引</span></span><br><span class="line">        self.data_set_word_count = &#123;&#125;<span class="comment">#每个单词的计数</span></span><br><span class="line">        self.index_to_data_set_word = &#123;&#125;<span class="comment">#索引→单词</span></span><br><span class="line">        self.word_to_vec = &#123;&#125;  <span class="comment">#单词与向量的映射</span></span><br><span class="line">        <span class="comment">#定义了一个unknown的词，也就是说没有出现在训练集里的词，我们都叫做unknown，词向量就定义为0</span></span><br><span class="line">        self.number_word = <span class="number">0</span><span class="comment">#词的数量</span></span><br><span class="line">        <span class="comment">#词向量的维度</span></span><br><span class="line">        self.embeddings_size = <span class="number">300</span></span><br><span class="line">        self.data_set_sentence_word_encoder = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.glove_input_file = <span class="string">&#x27;data/glove.6B.300d.txt&#x27;</span></span><br><span class="line">        self.word2vec_output_file = <span class="string">&#x27;data/glove.6B.300d.word2vec.txt&#x27;</span></span><br><span class="line">        <span class="comment"># 加载预训练的glove模型</span></span><br><span class="line">        self.glove_model = KeyedVectors.load_word2vec_format(self.word2vec_output_file, binary=<span class="literal">False</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deal_data</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment">#索引标记</span></span><br><span class="line">        <span class="keyword">for</span> index, sentence <span class="keyword">in</span> self.index_to_sentence.items():</span><br><span class="line">            doc_no = <span class="built_in">str</span>(sentence).split(<span class="string">&quot;#&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">            sen_id = <span class="built_in">str</span>(sentence).split(<span class="string">&quot;#&quot;</span>)[<span class="number">1</span>]</span><br><span class="line">            sentence = self.data_set_sentences[doc_no][sen_id]</span><br><span class="line">            <span class="comment"># 句子转化为 单词列表</span></span><br><span class="line">            word_list = self.sentence_to_word(sentence)</span><br><span class="line">            self.data_set_sentence_word[index] = word_list</span><br><span class="line">            self.data_set_sentence_count += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 将句子转化成词语</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sentence_to_word</span>(<span class="params">self, sentence</span>):</span></span><br><span class="line">        sentence = util.normalize_string(sentence)</span><br><span class="line">        word_list= []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">&#x27; &#x27;</span>):</span><br><span class="line">            word = util.normalize_string(word)</span><br><span class="line">            <span class="keyword">if</span> word != <span class="string">&quot;&quot;</span>:</span><br><span class="line">                self.addWord(word)</span><br><span class="line">                word_list.append(word)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(word_list) &gt;self.max_sentence_length:</span><br><span class="line">            self.max_sentence_length = <span class="built_in">len</span>(word_list)</span><br><span class="line">        <span class="keyword">return</span> word_list</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addWord</span>(<span class="params">self, word</span>):</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.data_set_word_to_index:</span><br><span class="line">            self.data_set_word_to_index[word] = self.number_word</span><br><span class="line">            self.data_set_word_count[word] = <span class="number">1</span></span><br><span class="line">            self.index_to_data_set_word[self.number_word] = word</span><br><span class="line">            self.number_word += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data_set_word_count[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#首先需要将这个训练好的模型转换成gensim方便加载的格式(gensim支持word2vec格式的预训练模型格式）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">glove_to_word2vec</span>(<span class="params">self</span>):</span></span><br><span class="line">        glove2word2vec(self.glove_input_file, self.word2vec_output_file)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    获得词向量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_word_vec</span>(<span class="params">self,word</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.glove_model[word]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word_to_vector</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> index , word <span class="keyword">in</span> self.index_to_data_set_word.items():</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                word_vec = self.get_word_vec(word)</span><br><span class="line">            <span class="keyword">except</span> KeyError:</span><br><span class="line">                word_vec = np.zeros(<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">            self.word_to_vec[word] = word_vec</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将句子使用rnn进行encoder</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sentence_encoder</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment">#遍历每句话中的每个单词</span></span><br><span class="line">        hedden_size = <span class="number">300</span></span><br><span class="line">        encoder = EncoderRNN(<span class="number">300</span>, hedden_size,self.data_set_sentence_count).to(device)</span><br><span class="line">        hidden = encoder.initHidden()</span><br><span class="line">        cell = encoder.initHidden()</span><br><span class="line">        output = <span class="literal">None</span></span><br><span class="line">        <span class="comment">#所有句子的 向量列表 三维的</span></span><br><span class="line">        sentences_vector_list = []</span><br><span class="line">        <span class="keyword">for</span> index ,words <span class="keyword">in</span> self.data_set_sentence_word.items():</span><br><span class="line">            <span class="comment">#每句话的单词进行word embeddings之后生成的矩阵</span></span><br><span class="line">            sentence_vector_list = []</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">            <span class="comment">#将单词转化为词向量</span></span><br><span class="line">                sentence_vector_list.append(self.word_to_vec[word])</span><br><span class="line">                count +=<span class="number">1</span></span><br><span class="line">            <span class="comment">#然后将所有的句子都结合在一起</span></span><br><span class="line">            sentence_vector_list = np.array(sentence_vector_list)</span><br><span class="line">            <span class="comment"># 在数组A的边缘填充constant_values指定的数值</span></span><br><span class="line">            <span class="comment"># （3,2）表示在A的第[0]轴填充（二维数组中，0轴表示行），即在0轴前面填充3个宽度的0，比如数组A中的95,96两个元素前面各填充了3个0；在后面填充2个0，比如数组A中的97,98两个元素后面各填充了2个0stant_values表示填充值，且(b</span></span><br><span class="line">            <span class="comment"># （2,3）表示在A的第[1]轴填充（二维数组中，1轴表示列），即在1轴前面填充2个宽度的0，后面填充3个宽度的0</span></span><br><span class="line">            sentence_vector_list = np.pad(sentence_vector_list,((<span class="number">0</span>,self.max_sentence_length-count),(<span class="number">0</span>,<span class="number">0</span>)),<span class="string">&#x27;constant&#x27;</span>,constant_values = (<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">            sentences_vector_list.append(sentence_vector_list)</span><br><span class="line">        output,hidden, cell = encoder(self.sentence_to_tensor(sentences_vector_list), hidden, cell,self.max_sentence_length,self.data_set_sentence_count)</span><br><span class="line">        print(hidden.shape)</span><br><span class="line">        print(output.shape)</span><br><span class="line">            <span class="comment"># self.train_set_sentence_word_encoder[index] = hidden[0].detach().numpy()</span></span><br><span class="line">        self.data_set_sentence_word_encoder = output[output.shape[<span class="number">0</span>]-<span class="number">1</span>].tolist()</span><br><span class="line">        <span class="comment"># print(self.train_set_sentence_word_encoder)</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将句子向量转化成tensor</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sentence_to_tensor</span>(<span class="params">self, vector_list</span>):</span></span><br><span class="line">        vector_array = np.array(vector_list)</span><br><span class="line">        tensor = torch.tensor(vector_array, dtype=torch.<span class="built_in">float</span>, device=device).view(self.data_set_sentence_count,self.max_sentence_length,<span class="number">300</span>)</span><br><span class="line">        <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure>
<p> 说明：<br> （1）读入数据<br>index_to_sentence——表示文档句子的索引与文档句子编号的映射（可以根据index找到属于那篇文档的那个句子），存储方式为{index : doc_no#sen_id}<br>sentence_to_index——表示文档句子编号与文档句子索引的的映射（可以根据某文档的某个句子找到他存储的index，存储方式为{doc_no#sen_id : index}<br>data_set_sentences——表示具体文档句子id与文档内容的映射，存储方式为{doc_no#sen_id : 句子内容}</p>
<p>（2）对数据进行处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.train_set_word_to_index = &#123;&#125;<span class="comment">#单词→索引</span></span><br><span class="line">self.train_set_word_count = &#123;&#125;<span class="comment">#每个单词的计数</span></span><br><span class="line">self.index_to_train_set_word = &#123;&#125;<span class="comment">#索引→单词</span></span><br></pre></td></tr></table></figure>
<p>将训练集中所有出现过的单词进行统计，建立单个词语的索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_data</span>(<span class="params">self</span>):</span></span><br><span class="line">       <span class="comment">#索引标记</span></span><br><span class="line">       <span class="keyword">for</span> index, sentence <span class="keyword">in</span> self.index_to_sentence.items():</span><br><span class="line">           doc_no = <span class="built_in">str</span>(sentence).split(<span class="string">&quot;#&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">           sen_id = <span class="built_in">str</span>(sentence).split(<span class="string">&quot;#&quot;</span>)[<span class="number">1</span>]</span><br><span class="line">           sentence = self.data_set_sentences[doc_no][sen_id]</span><br><span class="line">           <span class="comment"># 句子转化为 单词列表</span></span><br><span class="line">           word_list = self.sentence_to_word(sentence)</span><br><span class="line">           self.data_set_sentence_word[index] = word_list</span><br><span class="line">           self.data_set_sentence_count += <span class="number">1</span></span><br><span class="line">   <span class="comment"># 将句子转化成词语</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">sentence_to_word</span>(<span class="params">self, sentence</span>):</span></span><br><span class="line">       sentence = util.normalize_string(sentence)</span><br><span class="line">       word_list= []</span><br><span class="line">       <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split(<span class="string">&#x27; &#x27;</span>):</span><br><span class="line">           word = util.normalize_string(word)</span><br><span class="line">           <span class="keyword">if</span> word != <span class="string">&quot;&quot;</span>:</span><br><span class="line">               self.addWord(word)</span><br><span class="line">               word_list.append(word)</span><br><span class="line">       <span class="keyword">if</span> <span class="built_in">len</span>(word_list) &gt;self.max_sentence_length:</span><br><span class="line">           self.max_sentence_length = <span class="built_in">len</span>(word_list)</span><br><span class="line">       <span class="keyword">return</span> word_list</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">addWord</span>(<span class="params">self, word</span>):</span></span><br><span class="line">       <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.data_set_word_to_index:</span><br><span class="line">           self.data_set_word_to_index[word] = self.number_word</span><br><span class="line">           self.data_set_word_count[word] = <span class="number">1</span></span><br><span class="line">           self.index_to_data_set_word[self.number_word] = word</span><br><span class="line">           self.number_word += <span class="number">1</span></span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           self.data_set_word_count[word] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>（3）  将句子使用LSTM进行encoder</p>
<p>使用之前处理过的句子的单词列表，求出每一句话中每一个单词的word embedding （1<em>300），然后将这句话中的所有单词的word embedding ，组成一个 （句子中单词个数</em>300）维的矩阵。然后将这个矩阵的行数，补全成了最长句子的单词个数，最终形成的矩阵是 （最长句子的单词个数*300）。</p>
<p>我将所有句子都这样操作，形成了一个三维的张量 （句子个数 <em> 最长句子的单词个数</em>300）作为LSTM的输入。</p>
<p>那么在初始化LSTM时 为nn.LSTM(每个单词向量的长度——300, hidden_size)</p>
<p> 然后我将hidden 和cell初始化成了 torch.zeros(1, 句子的个数, hidden_size)</p>
<ul>
<li>对于input(seq_len, batch, input_size)  的参数 ：</li>
</ul>
<p>seq_len是序列的个数，对于句子来说，应该是句子的长度，应该是每一句话中单词的个数，这个是需要固定的 ，取了最长的句子的单词数。</p>
<p>batch表示一次性喂给网络多少条句子，初始化成句子的个数</p>
<p>input_size应该是每个具体的输入是多少维的向量，这里应该是300（word embedding的维度数量）</p>
<p>最终的hidden_size =300  最终生成了（句子的个数*300）的矩阵，作为Encoder的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先需要将这个训练好的模型转换成gensim方便加载的格式(gensim支持word2vec格式的预训练模型格式）</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">glove_to_word2vec</span>(<span class="params">self</span>):</span></span><br><span class="line">      glove2word2vec(self.glove_input_file, self.word2vec_output_file)</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  获得词向量</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_word_vec</span>(<span class="params">self,word</span>):</span></span><br><span class="line">      <span class="keyword">return</span> self.glove_model[word]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">word_to_vector</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="keyword">for</span> index , word <span class="keyword">in</span> self.index_to_data_set_word.items():</span><br><span class="line">          <span class="keyword">try</span>:</span><br><span class="line">              word_vec = self.get_word_vec(word)</span><br><span class="line">          <span class="keyword">except</span> KeyError:</span><br><span class="line">              word_vec = np.zeros(<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">          self.word_to_vec[word] = word_vec</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  将句子使用rnn进行encoder</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sentence_encoder</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="comment">#遍历每句话中的每个单词</span></span><br><span class="line">      hedden_size = <span class="number">300</span></span><br><span class="line">      encoder = EncoderRNN(<span class="number">300</span>, hedden_size,self.data_set_sentence_count).to(device)</span><br><span class="line">      hidden = encoder.initHidden()</span><br><span class="line">      cell = encoder.initHidden()</span><br><span class="line">      output = <span class="literal">None</span></span><br><span class="line">      <span class="comment">#所有句子的 向量列表 三维的</span></span><br><span class="line">      sentences_vector_list = []</span><br><span class="line">      <span class="keyword">for</span> index ,words <span class="keyword">in</span> self.data_set_sentence_word.items():</span><br><span class="line">          <span class="comment">#每句话的单词进行word embeddings之后生成的矩阵</span></span><br><span class="line">          sentence_vector_list = []</span><br><span class="line">          count = <span class="number">0</span></span><br><span class="line">          <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">          <span class="comment">#将单词转化为词向量</span></span><br><span class="line">              sentence_vector_list.append(self.word_to_vec[word])</span><br><span class="line">              count +=<span class="number">1</span></span><br><span class="line">          <span class="comment">#然后将所有的句子都结合在一起</span></span><br><span class="line">          sentence_vector_list = np.array(sentence_vector_list)</span><br><span class="line">          <span class="comment"># 在数组A的边缘填充constant_values指定的数值</span></span><br><span class="line">          <span class="comment"># （3,2）表示在A的第[0]轴填充（二维数组中，0轴表示行），即在0轴前面填充3个宽度的0，比如数组A中的95,96两个元素前面各填充了3个0；在后面填充2个0，比如数组A中的97,98两个元素后面各填充了2个0stant_values表示填充值，且(b</span></span><br><span class="line">          <span class="comment"># （2,3）表示在A的第[1]轴填充（二维数组中，1轴表示列），即在1轴前面填充2个宽度的0，后面填充3个宽度的0</span></span><br><span class="line">          sentence_vector_list = np.pad(sentence_vector_list,((<span class="number">0</span>,self.max_sentence_length-count),(<span class="number">0</span>,<span class="number">0</span>)),<span class="string">&#x27;constant&#x27;</span>,constant_values = (<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line">          sentences_vector_list.append(sentence_vector_list)</span><br><span class="line">      output,hidden, cell = encoder(self.sentence_to_tensor(sentences_vector_list), hidden, cell,self.max_sentence_length,self.data_set_sentence_count)</span><br><span class="line">      print(hidden.shape)</span><br><span class="line">      print(output.shape)</span><br><span class="line">          <span class="comment"># self.train_set_sentence_word_encoder[index] = hidden[0].detach().numpy()</span></span><br><span class="line">      self.data_set_sentence_word_encoder = output[output.shape[<span class="number">0</span>]-<span class="number">1</span>].tolist()</span><br><span class="line">      <span class="comment"># print(self.train_set_sentence_word_encoder)</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  将句子向量转化成tensor</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sentence_to_tensor</span>(<span class="params">self, vector_list</span>):</span></span><br><span class="line">      vector_array = np.array(vector_list)</span><br><span class="line">      tensor = torch.tensor(vector_array, dtype=torch.<span class="built_in">float</span>, device=device).view(self.data_set_sentence_count,self.max_sentence_length,<span class="number">300</span>)</span><br><span class="line">      <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure>
<h2 id="四-图卷积网络（Graph-Convolutional-Network）及显著性估计（Saliency-Estimation）"><a href="#四-图卷积网络（Graph-Convolutional-Network）及显著性估计（Saliency-Estimation）" class="headerlink" title="四.图卷积网络（Graph Convolutional Network）及显著性估计（Saliency Estimation）"></a>四.图卷积网络（Graph Convolutional Network）及显著性估计（Saliency Estimation）</h2><p> <strong>- 1.前言</strong><br>（1）在计算完句子嵌入和句子语义关系图后，使用单层图卷积网络（GCN），以便捕获每个句子的高级隐藏特征，封装句子信息以及图结构。<br>图卷积网络的邻接矩阵为句子语义关系图加上单位矩阵的结果（A = A + I），特征矩阵为使用LSTM进行句子编码后的结果。<br>（2）然后使用以下等式，获取句子的隐藏特征<br><img src="https://img-blog.csdnimg.cn/20200729202514147.png" alt="在这里插入图片描述"><br>Wi是第i个图卷积层的权重矩阵，bi是偏差矢量。使用ELU作为激活函数。<br>（3）之后使用一个线性层来估计每个句子的显着性分数，然后通过softmax将分数归一化。</p>
<p> <strong>- 2.图卷积网络的学习与理解</strong></p>
<blockquote>
<p>参考文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54505069">https://zhuanlan.zhihu.com/p/54505069</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89503068">https://zhuanlan.zhihu.com/p/89503068</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html">https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html</a></p>
</blockquote>
<p>（1）图神经网络GNN<br><img src="https://img-blog.csdnimg.cn/20200729203547914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200729203558981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200729203628762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>（2）图卷积网络</p>
<p><img src="https://img-blog.csdnimg.cn/20200729203718684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200729203759813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JhaWR1XzQxODcxNzk0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这个图也正好应证了论文中的公式<br><img src="https://img-blog.csdnimg.cn/20200729202514147.png" alt="在这里插入图片描述"><br>两层卷积层，每一层之后都有一个激活函数。</p>
<p> <strong>- 3.图卷积网络GCN的实现</strong><br> <strong>（1）卷积层的定义</strong><br> 卷积层输入维度为节点输入特征的维度，还可以设定偏差矢量。<br> 向前传播时，需要将节点的邻接矩阵和特征矩阵进行运算，然后进行相应的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraphConvolution</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_dim, output_dim, use_bias=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;图卷积：L*X*\theta</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">            input_dim: int</span></span><br><span class="line"><span class="string">                节点输入特征的维度 D</span></span><br><span class="line"><span class="string">            output_dim: int</span></span><br><span class="line"><span class="string">                输出特征维度 D‘</span></span><br><span class="line"><span class="string">            use_bias : bool, optional</span></span><br><span class="line"><span class="string">                是否使用偏置</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(GraphConvolution, self).__init__()</span><br><span class="line">        self.input_dim = input_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.use_bias = use_bias</span><br><span class="line">        <span class="comment"># 定义GCN层的权重矩阵    input_dim=300  output_dim=300</span></span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))</span><br><span class="line">        <span class="keyword">if</span> self.use_bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(output_dim))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">&#x27;bias&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        self.reset_parameters()  <span class="comment"># 使用自定义的参数初始化方式</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 自定义参数初始化方式</span></span><br><span class="line">        <span class="comment"># 权重参数初始化方式</span></span><br><span class="line">        init.kaiming_uniform_(self.weight)</span><br><span class="line">        <span class="keyword">if</span> self.use_bias:  <span class="comment"># 偏置参数初始化为0</span></span><br><span class="line">            init.zeros_(self.bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, adjacency, input_feature</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;邻接矩阵是稀疏矩阵，因此在计算时使用稀疏矩阵乘法</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">            adjacency:</span></span><br><span class="line"><span class="string">                邻接矩阵</span></span><br><span class="line"><span class="string">            input_feature: torch.Tensor</span></span><br><span class="line"><span class="string">                输入特征</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#矩阵相乘</span></span><br><span class="line">        <span class="comment"># h =  ̃ELU(A X W0 +b0 )   A为邻接矩阵  X为特征矩阵  W0为权重矩阵  B0为偏差矢量</span></span><br><span class="line">        <span class="comment"># S = ̃ELU(A h W1 +b1 )</span></span><br><span class="line">        support = torch.mm(input_feature, self.weight)  <span class="comment"># X W (N,D&#x27;);   X (N,D);W (D,D&#x27;)   input_feature 维度为 句子个数*300  weight的维度为 300 * 300  输出为  句子个数 *300</span></span><br><span class="line">        output = torch.mm(adjacency, support)  <span class="comment"># (N,D&#x27;)  #adjacency 为句子个数*句子个数     support 为 句子个数 *300   output为句子个数 *300</span></span><br><span class="line">        <span class="comment">#也可以使用稀疏矩阵的乘法</span></span><br><span class="line">        <span class="comment"># support = torch.mm(input_feature, self.weight) #XW (N,D&#x27;);X (N,D);W (D,D&#x27;)  </span></span><br><span class="line">        <span class="comment">#output = torch.sparse.mm(adjacency, support) #(N,D&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_bias:</span><br><span class="line">            output += self.bias</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>（2）GCN模型的定义<br>这个模型包含两个卷积层，并且在最终的输出前还使用了线性层以及softmax计算每一个句子的显著性分数。并且模型还使用了0.2的丢弃率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GcnNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义一个包含两层GraphConvolution的模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_dim = <span class="number">300</span> ,output_dim =<span class="number">300</span>,dropout = <span class="number">0.2</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GcnNet, self).__init__()</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.gcn1 = GraphConvolution(input_dim, output_dim)</span><br><span class="line">        self.gcn2 = GraphConvolution(input_dim, output_dim)</span><br><span class="line">        <span class="comment">#使用一个简单的线性层来估计每个句子的显着性分数,通过softmax将分数归一化并获得我们的显着性分数</span></span><br><span class="line">        self.linear = nn.Linear(input_dim,output_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    S =ELU(A ̃ELU(A ̃XW +b )W +b )</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, adjacency, feature</span>):</span></span><br><span class="line">        <span class="comment">#采用elu作为激活函数</span></span><br><span class="line">        <span class="comment"># h =  ̃ELU(A X W0 +b0 )   A为邻接矩阵  X为特征矩阵  W0为权重矩阵  B0为偏差矢量</span></span><br><span class="line">        h = F.elu(self.gcn1(adjacency, feature))</span><br><span class="line"></span><br><span class="line">        s = F.dropout(h,p = self.dropout)</span><br><span class="line">        <span class="comment">#S = ̃ELU(A h W1 +b1 )</span></span><br><span class="line">        s = self.gcn2(adjacency, h)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#使用一个简单的线性层来估计每个句子的显着性分数,通过softmax将分数归一化并获得我们的显着性分数</span></span><br><span class="line">        output = self.linear(s)</span><br><span class="line">        <span class="comment">#输出为每个维度的得分 0-1之间</span></span><br><span class="line">        output = F.softmax(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="五-模型的训练及结果的评估"><a href="#五-模型的训练及结果的评估" class="headerlink" title="五.模型的训练及结果的评估"></a>五.模型的训练及结果的评估</h2><p> <strong>- 1.前言</strong><br> 模型SemSentSum以端到端的方式训练(end-to-end端到端指的是输入是原始数据，输出是最后的结果)，并且使每个句子的显着性得分预测和ROUGE-1 F1得分之间的等式2的交叉熵损失最小。<br> <img src="https://img-blog.csdnimg.cn/20200729210107803.png" alt="在这里插入图片描述"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/23/Paper-SemSentSum/" data-id="ckutyt6pi0076av0a63axce4a" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/" rel="tag">多文档摘要</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/09/27/%20AndrewNg-deeplearning-ai/AndrewNg-deepinglearning-ai-note-catalog/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          吴恩达老师深度学习公开课学习笔记目录
        
      </div>
    </a>
  
  
    <a href="/2020/05/28/Undergraduate-Curriculum-Design/CurriculumDesign-jdsnCompare/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">京东苏宁爬虫，商品价格比较</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/PTA%E7%94%B2%E7%BA%A7%E5%88%B7%E9%A2%98/">PTA甲级刷题</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode%E5%88%B7%E9%A2%98/">leetcode刷题</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/">爬虫学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/">论文复现</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/">课程设计</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/c/" rel="tag">c++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/django/" rel="tag">django</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pyquery/" rel="tag">pyquery</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/springboot/" rel="tag">springboot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vue/" rel="tag">vue</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" rel="tag">动态规划</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E6%BA%AF/" rel="tag">回溯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE/" rel="tag">图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/" rel="tag">多文档摘要</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" rel="tag">字符串</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" rel="tag">操作系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%91/" rel="tag">树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E6%8B%9F/" rel="tag">模拟</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82/" rel="tag">模拟网络请求</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="tag">深度学习框架</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90/" rel="tag">网页解析</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/c/" style="font-size: 17.78px;">c++</a> <a href="/tags/django/" style="font-size: 10px;">django</a> <a href="/tags/java/" style="font-size: 14.44px;">java</a> <a href="/tags/pyquery/" style="font-size: 10px;">pyquery</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/pytorch/" style="font-size: 15.56px;">pytorch</a> <a href="/tags/springboot/" style="font-size: 10px;">springboot</a> <a href="/tags/tensorflow/" style="font-size: 16.67px;">tensorflow</a> <a href="/tags/vue/" style="font-size: 11.11px;">vue</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 10px;">动态规划</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size: 10px;">回溯</a> <a href="/tags/%E5%9B%BE/" style="font-size: 10px;">图</a> <a href="/tags/%E5%A4%9A%E6%96%87%E6%A1%A3%E6%91%98%E8%A6%81/" style="font-size: 10px;">多文档摘要</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 10px;">字符串</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">强化学习</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 13.33px;">操作系统</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 11.11px;">数据库</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%A0%91/" style="font-size: 12.22px;">树</a> <a href="/tags/%E6%A8%A1%E6%8B%9F/" style="font-size: 10px;">模拟</a> <a href="/tags/%E6%A8%A1%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82/" style="font-size: 11.11px;">模拟网络请求</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 14.44px;">深度学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" style="font-size: 18.89px;">深度学习框架</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 15.56px;">爬虫</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 17.78px;">算法</a> <a href="/tags/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90/" style="font-size: 10px;">网页解析</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/10/16/Arithmetic-LeetCode/282/">282</a>
          </li>
        
          <li>
            <a href="/2020/12/12/Pytorch/Pytorch-Reinforcement-Learning-Algorithm/">Pytorch强化学习算法实现</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/PyTorch-Commonly-Used-Tool-Modules/">PyTorch常用工具模块</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/Pytorch-and-torch-nn/">Pytorch中神经网络工具箱nn模块</a>
          </li>
        
          <li>
            <a href="/2020/12/09/Pytorch/Pytorch-and-Autograd/">Pytorch中的Autograd</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 ccclll777<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>